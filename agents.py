"""
agents.py - Multi-Agent System cho Media Tracker Bot
S·ª≠ d·ª•ng Agno framework ƒë·ªÉ coordinate gi·ªØa c√°c agents
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.models.groq import Groq
from agno.tools.crawl4ai import Crawl4aiTools
from agno.tools.googlesearch import GoogleSearchTools
from agno.team import Team

from models import (
    MediaSource,
    Article,
    IndustrySummary,
    OverallSummary,
    CompetitorReport,
    CrawlResult,
    CrawlConfig,
    BotStatus,
    IndustryType,
    ContentCluster,
)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CrawlerAgent:
    """Agent chuy√™n crawler web s·ª≠ d·ª•ng Crawl4AI"""

    def __init__(self, model_provider: str = "openai", model_id: str = "gpt-4o-mini"):
        """
        Initialize CrawlerAgent

        Args:
            model_provider: "openai" ho·∫∑c "groq"
            model_id: Model ID ƒë·ªÉ s·ª≠ d·ª•ng
        """
        # Ch·ªçn model provider
        if model_provider == "groq":
            model = Groq(id=model_id)
        else:
            model = OpenAIChat(id=model_id)

        self.agent = Agent(
            name="MediaCrawler",
            role="Web Crawler v√† Content Extractor",
            model=model,
            tools=[Crawl4aiTools(max_length=2000), GoogleSearchTools()],
            instructions=[
                "B·∫°n l√† chuy√™n gia crawl web cho tracking truy·ªÅn th√¥ng Vi·ªát Nam.",
                "Nhi·ªám v·ª•: Crawl c√°c website b√°o/t·∫°p ch√≠/media ƒë·ªÉ t√¨m tin t·ª©c v·ªÅ competitors.",
                "Ch·ªâ tr√≠ch xu·∫•t nh·ªØng b√†i vi·∫øt c√≥ li√™n quan ƒë·∫øn keywords ƒë∆∞·ª£c cung c·∫•p.",
                "Lu√¥n cung c·∫•p link g·ªëc v√† t√≥m t·∫Øt n·ªôi dung ch√≠nh x√°c.",
                "∆Øu ti√™n tin t·ª©c m·ªõi nh·∫•t trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.",
                "Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng text c√≥ c·∫•u tr√∫c r√µ r√†ng.",
                "B·ªè qua c√°c b√†i vi·∫øt kh√¥ng li√™n quan ho·∫∑c spam.",
                "X·ª≠ l√Ω gracefully c√°c l·ªói crawling v√† b√°o c√°o chi ti·∫øt.",
                "Focus on direct website crawling instead of search engines.",
            ],
            show_tool_calls=True,
            markdown=True,
            add_datetime_to_instructions=True,
        )

    async def crawl_media_source(
        self,
        media_source: MediaSource,
        keywords: List[str],
        date_range_days: int = 30,
        start_date: str = None,
        end_date: str = None,
    ) -> CrawlResult:
        """
        Crawl m·ªôt media source ƒë·ªÉ t√¨m articles li√™n quan

        Args:
            media_source: MediaSource object
            keywords: List keywords ƒë·ªÉ t√¨m ki·∫øm
            date_range_days: S·ªë ng√†y t√¨m v·ªÅ tr∆∞·ªõc (n·∫øu kh√¥ng c√≥ start_date/end_date)
            start_date: Ng√†y b·∫Øt ƒë·∫ßu (format: YYYY-MM-DD)
            end_date: Ng√†y k·∫øt th√∫c (format: YYYY-MM-DD)

        Returns:
            CrawlResult v·ªõi articles t√¨m ƒë∆∞·ª£c
        """
        start_time = datetime.now()

        try:
            # Determine date range
            if start_date and end_date:
                date_filter = f"t·ª´ {start_date} ƒë·∫øn {end_date}"
            else:
                end_date_obj = datetime.now()
                start_date_obj = end_date_obj - timedelta(days=date_range_days)
                date_filter = f"t·ª´ {start_date_obj.strftime('%Y-%m-%d')} ƒë·∫øn {end_date_obj.strftime('%Y-%m-%d')}"

            # Build domain URL for direct crawling
            domain_url = media_source.domain
            if domain_url and not domain_url.startswith("http"):
                domain_url = f"https://{domain_url}"

            # X√¢y d·ª±ng crawl query - shorter and more focused
            keywords_str = ", ".join(
                keywords[:5]
            )  # Limit to 5 keywords to avoid long URLs

            crawl_query = f"""
            Crawl website: {domain_url or media_source.name}
            
            T√¨m c√°c b√†i b√°o c√≥ ch·ª©a t·ª´ kh√≥a: {keywords_str}
            Th·ªùi gian: {date_filter}
            
            Y√™u c·∫ßu:
            - Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ, t√≥m t·∫Øt, ng√†y ƒëƒÉng, link g·ªëc
            - Ch·ªâ l·∫•y b√†i vi·∫øt li√™n quan ƒë·∫øn keywords
            - Format: Ti√™u ƒë·ªÅ | Ng√†y | T√≥m t·∫Øt | Link
            
            Media: {media_source.name}
            """

            # G·ªçi agent ƒë·ªÉ crawl
            session_id = (
                f"crawler-{media_source.name}-{datetime.now().strftime('%Y%m%d%H%M%S')}"
            )
            response = await self.agent.arun(crawl_query, session_id=session_id)

            # Parse response th√†nh CrawlResult
            duration = (datetime.now() - start_time).total_seconds()

            if response and response.content:
                return CrawlResult(
                    source_name=media_source.name,
                    source_type=media_source.type,
                    url=media_source.domain,
                    articles_found=self._parse_articles_from_response(
                        response.content, media_source
                    ),
                    crawl_status="success",
                    crawl_duration=duration,
                )
            else:
                return CrawlResult(
                    source_name=media_source.name,
                    source_type=media_source.type,
                    url=media_source.domain,
                    articles_found=[],
                    crawl_status="failed",
                    error_message="No response from crawler",
                    crawl_duration=duration,
                )

        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            logger.error(f"Crawl failed for {media_source.name}: {str(e)}")

            return CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=media_source.domain,
                articles_found=[],
                crawl_status="failed",
                error_message=str(e),
                crawl_duration=duration,
            )

    def _parse_articles_from_response(
        self, content: str, media_source: MediaSource
    ) -> List[Article]:
        """Parse n·ªôi dung response th√†nh list Article objects"""
        articles = []

        try:
            # Th·ª≠ parse JSON n·∫øu c√≥
            if content.strip().startswith("{") or content.strip().startswith("["):
                data = json.loads(content)
                if isinstance(data, list):
                    for item in data:
                        article = self._create_article_from_dict(item, media_source)
                        if article:
                            articles.append(article)
            else:
                # Parse text format
                articles = self._parse_text_articles(content, media_source)

        except Exception as e:
            logger.warning(f"Failed to parse articles from response: {str(e)}")

        return articles

    def _create_article_from_dict(
        self, data: Dict, media_source: MediaSource
    ) -> Optional[Article]:
        """T·∫°o Article object t·ª´ dictionary"""
        try:
            link = data.get("link_bai_bao", "").strip()

            if not link.startswith("http"):
                return None

            return Article(
                stt=data.get("stt", 1),
                ngay_phat_hanh=data.get("ngay_phat_hanh", datetime.now().date()),
                dau_bao=media_source.name,
                cum_noi_dung=data.get("cum_noi_dung", ContentCluster.OTHER),
                tom_tat_noi_dung=data.get("tom_tat_noi_dung", ""),
                link_bai_bao=link,
                nganh_hang=data.get("nganh_hang", IndustryType.DAU_AN),
                nhan_hang=data.get("nhan_hang", []),
                keywords_found=data.get("keywords_found", []),
            )
        except Exception as e:
            logger.warning(f"Failed to create article from dict: {str(e)}")
            return None

    def _parse_text_articles(
        self, content: str, media_source: MediaSource
    ) -> List[Article]:
        """Parse articles t·ª´ text content"""
        articles = []

        # Basic parsing logic - c√≥ th·ªÉ c·∫£i thi·ªán th√™m
        lines = content.split("\n")
        current_article = {}

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Detect article boundaries v√† extract info
            if "ti√™u ƒë·ªÅ:" in line.lower() or "title:" in line.lower():
                current_article["title"] = line.split(":", 1)[1].strip()
            elif "link:" in line.lower() or "url:" in line.lower():
                current_article["url"] = line.split(":", 1)[1].strip()
            elif "ng√†y:" in line.lower() or "date:" in line.lower():
                current_article["date"] = line.split(":", 1)[1].strip()
            elif "t√≥m t·∫Øt:" in line.lower() or "summary:" in line.lower():
                current_article["summary"] = line.split(":", 1)[1].strip()

                # Khi c√≥ ƒë·ªß th√¥ng tin, t·∫°o Article
                if len(current_article) >= 3:
                    try:
                        article = Article(
                            stt=len(articles) + 1,
                            ngay_phat_hanh=datetime.now().date(),
                            dau_bao=media_source.name,
                            cum_noi_dung=ContentCluster.OTHER,
                            tom_tat_noi_dung=current_article.get(
                                "summary", current_article.get("title", "")
                            ),
                            link_bai_bao=current_article.get(
                                "url", "https://example.com"
                            ),
                            nganh_hang=IndustryType.DAU_AN,  # Default, s·∫Ω ƒë∆∞·ª£c ProcessorAgent ph√¢n lo·∫°i l·∫°i
                            nhan_hang=[],
                            keywords_found=[],
                        )
                        articles.append(article)
                    except Exception as e:
                        logger.warning(f"Failed to create article: {str(e)}")

                    current_article = {}

        return articles


class ProcessorAgent:
    """Agent chuy√™n x·ª≠ l√Ω v√† ph√¢n t√≠ch n·ªôi dung"""

    def __init__(self, model_provider: str = "openai", model_id: str = "gpt-4o"):
        """Initialize ProcessorAgent v·ªõi model m·∫°nh h∆°n cho analysis"""

        if model_provider == "groq":
            model = Groq(id=model_id)
        else:
            model = OpenAIChat(id=model_id)

        self.agent = Agent(
            name="ContentProcessor",
            role="Content Analyst v√† Classifier",
            model=model,
            instructions=[
                "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch n·ªôi dung truy·ªÅn th√¥ng cho ng√†nh FMCG Vi·ªát Nam.",
                "Nhi·ªám v·ª•: Ph√¢n t√≠ch v√† ph√¢n lo·∫°i n·ªôi dung b√†i b√°o theo ng√†nh h√†ng v√† nh√£n hi·ªáu.",
                "X√°c ƒë·ªãnh ch√≠nh x√°c: Ng√†nh h√†ng, Nh√£n h√†ng, C·ª•m n·ªôi dung ch√≠nh.",
                "C√°c ng√†nh h√†ng ch√≠nh: D·∫ßu ƒÉn, Gia v·ªã, G·∫°o & Ng≈© c·ªëc, S·ªØa (UHT), Baby Food, Home Care.",
                "C√°c c·ª•m n·ªôi dung: Ho·∫°t ƒë·ªông doanh nghi·ªáp, CSR, Marketing, Ra m·∫Øt s·∫£n ph·∫©m, H·ª£p t√°c, B√°o c√°o t√†i ch√≠nh.",
                "Tr√≠ch xu·∫•t keywords v√† nh√£n hi·ªáu competitors m·ªôt c√°ch ch√≠nh x√°c.",
                "ƒê√°nh gi√° t·∫ßm quan tr·ªçng v√† impact c·ªßa t·ª´ng b√†i vi·∫øt.",
                "Lo·∫°i b·ªè c√°c b√†i vi·∫øt kh√¥ng li√™n quan ho·∫∑c ch·∫•t l∆∞·ª£ng th·∫•p.",
                "Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng text c√≥ c·∫•u tr√∫c r√µ r√†ng.",
            ],
            show_tool_calls=False,
            markdown=True,
            add_datetime_to_instructions=True,
        )

    def _filter_by_industry_keywords(
        article: Article, keywords_config: Dict[str, List[str]]
    ) -> bool:
        """Ki·ªÉm tra xem b√†i vi·∫øt c√≥ th·∫≠t s·ª± li√™n quan ƒë·∫øn ng√†nh"""
        for industry, kw_list in keywords_config.items():
            if any(kw.lower() in article.tom_tat_noi_dung.lower() for kw in kw_list):
                return True
        return False

    def _extract_possible_brands(keywords_config: Dict[str, List[str]]) -> List[str]:
        brand_candidates = []
        for industry, keywords in keywords_config.items():
            for kw in keywords:
                # Brand th∆∞·ªùng ƒë∆∞·ª£c vi·∫øt hoa ch·ªØ ƒë·∫ßu
                if kw.istitle():
                    brand_candidates.append(kw)
        return list(set(brand_candidates))  # Lo·∫°i tr√πng l·∫∑p

    async def process_articles(
        self, raw_articles: List[Article], keywords_config: Dict[str, List[str]]
    ) -> List[Article]:
        """
        X·ª≠ l√Ω v√† ph√¢n lo·∫°i c√°c articles

        Args:
            raw_articles: List articles th√¥ t·ª´ crawler
            keywords_config: Config keywords theo ng√†nh h√†ng

        Returns:
            List articles ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† ph√¢n lo·∫°i
        """

        brand_list = self._extract_possible_brands(keywords_config)

        if not raw_articles:
            return []

        try:
            # T·∫°o prompt cho vi·ªác ph√¢n t√≠ch
            analysis_prompt = f"""

            Danh s√°ch c√°c nh√£n h√†ng ƒë·ªëi th·ªß c·∫ßn x√°c ƒë·ªãnh:
            {json.dumps(brand_list, ensure_ascii=False, indent=2)}

            Ph√¢n t√≠ch v√† ph√¢n lo·∫°i {len(raw_articles)} b√†i b√°o sau ƒë√¢y:
            
            Keywords config:
            {json.dumps(keywords_config, ensure_ascii=False, indent=2)}
            
            Raw articles:
            {json.dumps([article.model_dump() for article in raw_articles], ensure_ascii=False, indent=2)}
            
            M·ª•c ti√™u: Lo·∫°i b·ªè c√°c b√†i kh√¥ng thu·ªôc ng√†nh h√†ng, ƒë·∫∑c bi·ªát l√† tr√°nh nh·∫ßm c√°c b√†i ch·ª©a t√™n th∆∞∆°ng hi·ªáu nh∆∞ng kh√¥ng li√™n quan ƒë·∫øn s·∫£n ph·∫©m ng√†nh ƒë√≥.

            Y√™u c·∫ßu:
            1. Ph√¢n lo·∫°i ch√≠nh x√°c ng√†nh h√†ng cho t·ª´ng b√†i
            2. V·ªõi m·ªói b√†i vi·∫øt, x√°c ƒë·ªãnh c√°c nh√£n h√†ng competitors n√†o ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong n·ªôi dung t·ª´ danh s√°ch c√°c nh√£n h√†ng (kh√¥ng th√™m n·∫øu kh√¥ng th·∫•y r√µ n·ªôi dung).
            3. Ph√¢n lo·∫°i c·ª•m n·ªôi dung (Ho·∫°t ƒë·ªông doanh nghi·ªáp, ch∆∞∆°ng tr√¨nh CSR, chi·∫øn d·ªãch Marketing, ra m·∫Øt s·∫£n ph·∫©m, th√¥ng tin s·∫£n ph·∫©m, h·ª£p t√°c, b√°o c√°o t√†i ch√≠nh, etc.):
                3.1 Ho·∫°t ƒë·ªông doanh nghi·ªáp: b√†i vi·∫øt v·ªÅ ho·∫°t ƒë·ªông s·∫£n xu·∫•t, v·∫≠n h√†nh, m·ªü r·ªông nh√† m√°y, tuy·ªÉn d·ª•ng, t·ªï ch·ª©c n·ªôi b·ªô,...
                3.2 Ch∆∞∆°ng tr√¨nh CSR: b√†i vi·∫øt n√≥i v·ªÅ ho·∫°t ƒë·ªông v√¨ c·ªông ƒë·ªìng, t√†i tr·ª£ h·ªçc b·ªïng, t·ª´ thi·ªán, b·∫£o v·ªá m√¥i tr∆∞·ªùng,...
                3.3 Chi·∫øn d·ªãch Marketing: b√†i vi·∫øt v·ªÅ chi·∫øn d·ªãch qu·∫£ng b√°, truy·ªÅn th√¥ng, KOLs, s·ª± ki·ªán, khuy·∫øn m√£i, qu·∫£ng c√°o,...
                3.4 Ra m·∫Øt s·∫£n ph·∫©m ho·∫∑c th√¥ng tin s·∫£n ph·∫©m: b√†i vi·∫øt gi·ªõi thi·ªáu s·∫£n ph·∫©m m·ªõi, c·∫£i ti·∫øn s·∫£n ph·∫©m, ƒë√≥ng g√≥i m·ªõi,...
                3.5 H·ª£p t√°c: b√†i vi·∫øt n√≥i v·ªÅ c√°c h·ª£p ƒë·ªìng h·ª£p t√°c, MOU, li√™n doanh, li√™n k·∫øt,...
                3.6 B√°o c√°o t√†i ch√≠nh: b√†i vi·∫øt n√™u k·∫øt qu·∫£ kinh doanh, l·ª£i nhu·∫≠n, chi ph√≠, tƒÉng tr∆∞·ªüng,...
                N·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c, m·ªõi ch·ªçn l√†: Kh√°c.
                L∆∞u √Ω: Kh√¥ng ƒë∆∞·ª£c g√°n nh·∫ßm t·∫•t c·∫£ v√†o ‚ÄúKh√°c‚Äù. Ch·ªâ s·ª≠ d·ª•ng ‚ÄúKh√°c‚Äù khi b√†i vi·∫øt th·ª±c s·ª± kh√¥ng thu·ªôc b·∫•t k·ª≥ lo·∫°i n√†o ·ªü tr√™n.
            4. Tr√≠ch xu·∫•t keywords t√¨m th·∫•y
            5. C·∫£i thi·ªán t√≥m t·∫Øt n·ªôi dung
            6. Lo·∫°i b·ªè c√°c b√†i kh√¥ng li√™n quan
            7. V·ªõi m·ªói b√†i b√°o, x√°c ƒë·ªãnh ng√†nh h√†ng th·ª±c s·ª± li√™n quan d·ª±a tr√™n ng·ªØ c·∫£nh ‚Äì kh√¥ng ch·ªâ s·ª± xu·∫•t hi·ªán t·ª´ kh√≥a.

            Ghi nh·ªõ:
            1. N·∫øu ch·ªâ ƒë·ªÅ c·∫≠p th∆∞∆°ng hi·ªáu m√† kh√¥ng n√≥i v·ªÅ s·∫£n ph·∫©m ng√†nh li√™n quan th√¨ lo·∫°i b·ªè.
            
            Tr·∫£ v·ªÅ list Article objects ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß.
            """
            session_id = f"processor-{datetime.now().strftime('%Y%m%d%H%M%S')}"
            response = await self.agent.arun(analysis_prompt, session_id=session_id)

            if response and response.content:
                # Parse response th√†nh list Article
                processed_articles = self._parse_processed_articles(
                    response.content, raw_articles
                )

                processed_articles = [
                    a
                    for a in processed_articles
                    if self._filter_by_industry_keywords(a, keywords_config)
                ]

                logger.info(
                    f"Processed {len(processed_articles)} articles successfully"
                )
                return processed_articles
            else:
                logger.warning("No response from processor agent")
                return raw_articles

        except Exception as e:
            logger.error(f"Processing failed: {str(e)}")
            return raw_articles

    def _parse_processed_articles(
        self, content: str, original_articles: List[Article]
    ) -> List[Article]:
        """Parse processed articles t·ª´ agent response"""
        try:
            # Th·ª≠ parse JSON response
            if content.strip().startswith("["):
                data = json.loads(content)
                processed_articles = []

                for item in data:
                    try:
                        article = Article(**item)
                        processed_articles.append(article)
                    except Exception as e:
                        logger.warning(f"Failed to parse article: {str(e)}")
                        continue

                return processed_articles
            else:
                # Fallback: return original articles
                logger.warning(
                    "Could not parse processed articles, returning originals"
                )
                return original_articles

        except Exception as e:
            logger.error(f"Failed to parse processed articles: {str(e)}")
            return original_articles


class ReportAgent:
    """Agent chuy√™n t·∫°o reports"""

    def __init__(self, model_provider: str = "openai", model_id: str = "gpt-4o"):
        """Initialize ReportAgent"""

        if model_provider == "groq":
            model = Groq(id=model_id)
        else:
            model = OpenAIChat(id=model_id)

        self.agent = Agent(
            name="ReportGenerator",
            role="Report Generator v√† Data Analyst",
            model=model,
            instructions=[
                "B·∫°n l√† chuy√™n gia t·∫°o b√°o c√°o truy·ªÅn th√¥ng cho ng√†nh FMCG.",
                "Nhi·ªám v·ª•: T·∫°o b√°o c√°o competitor analysis t·ª´ d·ªØ li·ªáu b√†i b√°o ƒë√£ crawl.",
                "Format b√°o c√°o theo template: CALOFIC COMPETITOR REPORT - PR & SOCIAL MONTHLY REPORT.",
                "T·∫°o t√≥m t·∫Øt t·ªïng quan v√† chi ti·∫øt theo t·ª´ng ng√†nh h√†ng.",
                "Th·ªëng k√™ ch√≠nh x√°c: s·ªë l∆∞·ª£ng b√†i, ph√¢n b·ªë theo ng√†nh, top media sources.",
                "Insights v√† recommendations d·ª±a tr√™n d·ªØ li·ªáu.",
                "ƒê·∫£m b·∫£o format professional v√† easy-to-read.",
                "Include charts v√† visualizations n·∫øu c√≥ th·ªÉ.",
                "Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng text c√≥ c·∫•u tr√∫c r√µ r√†ng.",
            ],
            show_tool_calls=False,
            markdown=True,
            add_datetime_to_instructions=True,
        )

    async def generate_report(
        self, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        """
        T·∫°o b√°o c√°o t·ª´ list articles

        Args:
            articles: List articles ƒë√£ processed
            date_range: Kho·∫£ng th·ªùi gian b√°o c√°o

        Returns:
            CompetitorReport object
        """
        try:
            # T·∫°o prompt ƒë·ªÉ generate report
            report_prompt = f"""
            T·∫°o b√°o c√°o COMPETITOR REPORT t·ª´ {len(articles)} b√†i b√°o sau:
            
            Th·ªùi gian: {date_range}
            
            Articles data:
            {json.dumps([article.dict() for article in articles], ensure_ascii=False, indent=2)}
            
            Y√™u c·∫ßu b√°o c√°o:
            1. Overall Summary v·ªõi th·ªëng k√™ t·ªïng quan
            2. Industry Summaries theo t·ª´ng ng√†nh h√†ng
            3. Top competitors v√† market insights
            4. Ph√¢n t√≠ch trend v√† patterns
            5. Recommendations
            
            Format theo CompetitorReport model ƒë√£ ƒë·ªãnh nghƒ©a.
            """

            response = await self.agent.arun(report_prompt)

            if response and response.content:
                # Parse response th√†nh CompetitorReport
                report = self._parse_report_response(
                    response.content, articles, date_range
                )
                logger.info(f"Generated report with {len(articles)} articles")
                return report
            else:
                # Fallback: t·∫°o basic report
                return self._create_basic_report(articles, date_range)

        except Exception as e:
            logger.error(f"Report generation failed: {str(e)}")
            return self._create_basic_report(articles, date_range)

    def _parse_report_response(
        self, content: str, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        """Parse report t·ª´ agent response"""
        try:
            if content.strip().startswith("{"):
                data = json.loads(content)
                return CompetitorReport(**data)
            else:
                return self._create_basic_report(articles, date_range)
        except Exception as e:
            logger.warning(f"Failed to parse report response: {str(e)}")
            return self._create_basic_report(articles, date_range)

    def _create_basic_report(
        self, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        """T·∫°o basic report khi parse th·∫•t b·∫°i"""
        # Group articles by industry
        industry_groups = {}
        for article in articles:
            industry = article.nganh_hang
            if industry not in industry_groups:
                industry_groups[industry] = []
            industry_groups[industry].append(article)

        # T·∫°o industry summaries
        industry_summaries = []
        for industry, industry_articles in industry_groups.items():
            # Collect unique brands
            brands = set()
            content_clusters = set()
            media_sources = set()

            for article in industry_articles:
                brands.update(article.nhan_hang)
                content_clusters.add(article.cum_noi_dung)
                media_sources.add(article.dau_bao)

            summary = IndustrySummary(
                nganh_hang=industry,
                nhan_hang=list(brands),
                cum_noi_dung=list(content_clusters),
                so_luong_bai=len(industry_articles),
                cac_dau_bao=list(media_sources),
            )
            industry_summaries.append(summary)

        # T·∫°o overall summary
        overall_summary = OverallSummary(
            thoi_gian_trich_xuat=date_range,
            industries=industry_summaries,
            tong_so_bai=len(articles),
        )

        return CompetitorReport(
            overall_summary=overall_summary,
            industry_summaries=industry_summaries,
            articles=articles,
            total_articles=len(articles),
            date_range=date_range,
        )


class MediaTrackerTeam:
    """Main coordinator team cho to√†n b·ªô workflow"""

    def __init__(
        self,
        config: CrawlConfig,
        model_provider: str = "openai",
        stop_event: Optional[asyncio.Event] = None,
    ):
        """
        Initialize MediaTrackerTeam

        Args:
            config: CrawlConfig v·ªõi media sources v√† keywords
            model_provider: "openai" ho·∫∑c "groq"
        """
        self.config = config
        self.model_provider = model_provider
        self.stop_event = stop_event

        # Initialize individual agents
        self.crawler = CrawlerAgent(model_provider=model_provider)
        self.processor = ProcessorAgent(model_provider=model_provider)
        self.reporter = ReportAgent(model_provider=model_provider)

        # Status tracking
        self.status = BotStatus()

        # Setup team coordinator
        self._setup_coordinator()

    def _setup_coordinator(self):
        """Setup team coordinator agent"""
        if self.model_provider == "groq":
            model = Groq(id="llama-3.1-70b-versatile")
        else:
            model = OpenAIChat(id="gpt-4o")

        self.coordinator = Team(
            name="MediaTrackerCoordinator",
            mode="coordinate",
            model=model,
            members=[self.crawler.agent, self.processor.agent, self.reporter.agent],
            description="Coordinator team cho media tracking workflow",
            instructions=[
                "ƒêi·ªÅu ph·ªëi workflow: Crawling -> Processing -> Reporting",
                "Monitor progress v√† handle errors gracefully",
                "Optimize performance v√† resource usage",
                "Provide detailed status updates",
                "Ensure data quality v√† accuracy",
            ],
            enable_agentic_context=True,
            share_member_interactions=True,
            show_members_responses=False,
            markdown=True,
        )

    async def run_full_pipeline(self) -> CompetitorReport:
        self.status.is_running = True
        self.status.current_task = "Initializing pipeline"
        self.status.total_sources = len(self.config.media_sources)
        self.status.completed_sources = 0
        self.status.failed_sources = 0

        try:
            logger.info("Starting media tracking pipeline...")

            # Phase 1: Crawling
            self.status.current_task = "Crawling media sources"
            all_articles = []

            for i, media_source in enumerate(self.config.media_sources):
                if self.stop_event and self.stop_event.is_set():
                    logger.warning("üõë Pipeline stopped during crawling.")
                    raise Exception("Pipeline was stopped by user.")

                try:
                    relevant_keywords = self._get_relevant_keywords(media_source)

                    result = await self.crawler.crawl_media_source(
                        media_source=media_source,
                        keywords=relevant_keywords,
                        date_range_days=self.config.date_range_days,
                    )

                    if result.crawl_status == "success":
                        all_articles.extend(result.articles_found)
                        self.status.completed_sources += 1
                    else:
                        self.status.failed_sources += 1
                        logger.warning(
                            f"Failed to crawl {media_source.name}: {result.error_message}"
                        )

                    self.status.progress = (i + 1) / self.status.total_sources * 30

                except Exception as e:
                    logger.error(f"Error crawling {media_source.name}: {str(e)}")
                    self.status.failed_sources += 1

            logger.info(f"Crawling completed. Found {len(all_articles)} articles")

            # Stop check tr∆∞·ªõc processing
            if self.stop_event and self.stop_event.is_set():
                logger.warning("üõë Pipeline stopped before processing.")
                raise Exception("Pipeline was stopped by user.")

            # Phase 2: Processing
            self.status.current_task = "Processing and analyzing content"
            self.status.progress = 40

            processed_articles = await self.processor.process_articles(
                raw_articles=all_articles, keywords_config=self.config.keywords
            )

            self.status.progress = 70
            logger.info(
                f"Processing completed. {len(processed_articles)} articles processed"
            )

            # Stop check tr∆∞·ªõc report
            if self.stop_event and self.stop_event.is_set():
                logger.warning("üõë Pipeline stopped before report generation.")
                raise Exception("Pipeline was stopped by user.")

            # Phase 3: Report Generation
            self.status.current_task = "Generating report"
            self.status.progress = 80

            date_range = self._generate_date_range()
            report = await self.reporter.generate_report(
                articles=processed_articles, date_range=date_range
            )

            self.status.progress = 100
            self.status.current_task = "Completed"
            self.status.is_running = False
            self.status.last_run = datetime.now()

            logger.info("Pipeline completed successfully!")
            return report

        except Exception as e:
            self.status.is_running = False
            self.status.current_task = f"Failed: {str(e)}"
            logger.error(f"Pipeline failed: {str(e)}")
            raise

    def _get_relevant_keywords(self, media_source: MediaSource) -> List[str]:
        """Get relevant keywords cho media source"""
        # Combine all keywords from config
        all_keywords = []
        for industry_keywords in self.config.keywords.values():
            all_keywords.extend(industry_keywords)

        return all_keywords

    def _generate_date_range(self) -> str:
        """Generate date range string"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=self.config.date_range_days)

        return f"T·ª´ ng√†y {start_date.strftime('%d/%m/%Y')} ƒë·∫øn ng√†y {end_date.strftime('%d/%m/%Y')}"

    def get_status(self) -> BotStatus:
        """Get current status c·ªßa bot"""
        return self.status


# Utility functions
async def create_sample_team() -> MediaTrackerTeam:
    """T·∫°o sample team ƒë·ªÉ test"""
    from models import MediaSource, MediaType

    # Sample config
    sample_config = CrawlConfig(
        keywords={
            "D·∫ßu ƒÉn": ["T∆∞·ªùng An", "Coba", "Nortalic", "d·∫ßu ƒÉn"],
            "Gia v·ªã": ["gia v·ªã", "seasoning", "ch·∫•m"],
        },
        media_sources=[
            MediaSource(
                stt=1, name="VietnamBiz", type=MediaType.WEBSITE, domain="vietnambiz.vn"
            ),
            MediaSource(
                stt=2, name="VnExpress", type=MediaType.WEBSITE, domain="vnexpress.net"
            ),
        ],
        date_range_days=30,
    )

    return MediaTrackerTeam(config=sample_config)


if __name__ == "__main__":
    # Test agents
    async def test_agents():
        team = await create_sample_team()
        print("‚úÖ Agents created successfully!")
        print(f"Team coordinator: {team.coordinator.name}")
        print(f"Total media sources: {team.status.total_sources}")

        # Test run (comment out for actual run)
        # report = await team.run_full_pipeline()
        # print(f"Report generated: {report.total_articles} articles")

    asyncio.run(test_agents())
