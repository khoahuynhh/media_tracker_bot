# src/agents.py
"""
Multi-Agent System for the Media Tracker Bot.
Phi√™n b·∫£n n√†y s·∫Ω c·∫£i thi·ªán l·∫°i c√°c prompt chi ti·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng ph√¢n t√≠ch,
ƒë·ªìng th·ªùi gi·ªØ l·∫°i y√™u c·∫ßu output d·∫°ng JSON ƒë·ªÉ h·ªá th·ªëng ho·∫°t ƒë·ªông b·ªÅn b·ªâ.
"""

import asyncio
import json
import logging
import gc
import httpx
import re
import ast
import time
import random
import os
import inspect
import requests
import unicodedata
import contextlib

from datetime import datetime, date
from typing import List, Dict, Optional, Any, Tuple
from playwright.sync_api import sync_playwright
from playwright.async_api import async_playwright
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from urllib.parse import (
    urlparse,
    urlunparse,
    parse_qsl,
    urlencode,
    unquote,
    urljoin,
    parse_qs,
)
from bs4 import BeautifulSoup
from asyncio import Semaphore

# Import Agno
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.models.groq import Groq
from agno.models.google import Gemini
from agno.tools.crawl4ai import Crawl4aiTools
from agno.tools.googlesearch import GoogleSearchTools
from ddgs import DDGS

# Import modules
from openai import APITimeoutError
from .parsing import ArticleParser
from .event import event_bus, decision_bus, fallback_lock_by_session
from .models import (
    MediaSource,
    Article,
    IndustrySummary,
    OverallSummary,
    CompetitorReport,
    CrawlResult,
    CrawlConfig,
    BotStatus,
    ContentCluster,
    KeywordManager,
)
from .configs import CONFIG_DIR, settings
from .cache_manager import SafeCacheManager
from .task_state import task_manager
from .proxy import PROXIES

logger = logging.getLogger(__name__)

PROVIDER_MODEL_MAP = {
    "openai": {"default": "gpt-4o-mini", "report": "gpt-4o-mini"},
    "groq": {"default": "llama-3.1-70b-versatile", "report": "llama-3.1-70b-versatile"},
    "gemini": {"default": "gemini-2.0-flash", "report": "gemini-2.0-flash"},
}

http_client = httpx.AsyncClient(
    http2=True,
    timeout=50.0,
    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50),
    transport=httpx.AsyncHTTPTransport(retries=2),
)


def get_llm_model(provider: Optional[str], model_id: Optional[str]) -> Any:
    status = settings.get_api_key_status()
    chosen = (provider or status["default_provider"] or "openai").lower()

    # NEW: ∆∞u ti√™n model_id truy·ªÅn v√†o; n·∫øu kh√¥ng c√≥ th√¨ l·∫•y t·ª´ ENV (DEFAULT_MODEL_ID)
    mid = (
        model_id
        or os.getenv("DEFAULT_MODEL_ID")
        or PROVIDER_MODEL_MAP.get(chosen, PROVIDER_MODEL_MAP["openai"])["default"]
    )

    if chosen == "groq":
        return Groq(id=mid)
    elif chosen == "gemini":
        return Gemini(id=mid)
    else:
        return OpenAIChat(id=mid, http_client=http_client)


# API Errors
def _configured_providers_in_order(settings, preferred: str | None = None) -> list[str]:
    st = settings.get_api_key_status()
    # h√†m b·∫°n ƒë√£ c√≥
    enabled_ok = {
        "openai": bool(st.get("openai_configured"))
        and getattr(settings, "openai_enabled", True),
        "groq": bool(st.get("groq_configured"))
        and getattr(settings, "groq_enabled", True),
        "gemini": bool(st.get("google_configured"))
        and getattr(settings, "google_enabled", True),
    }
    if preferred:
        p = preferred.lower()
        return [p] if enabled_ok.get(p) else []
    return [p for p, ok in enabled_ok.items() if ok]


def _map_llm_error(err: Exception) -> tuple[str, str]:
    sc = getattr(getattr(err, "response", None), "status_code", None)
    code = None
    try:
        body = getattr(err, "response", None)
        if body is not None and hasattr(body, "json"):
            j = body.json()
            code = (j.get("error") or {}).get("code")
    except Exception:
        pass

    if sc in (401, 403):
        return "PROVIDER_AUTH", "API key invalid/expired"

    if sc == 429:
        if (code or "").lower() == "insufficient_quota":
            return "PROVIDER_NO_QUOTA", "Insufficient quota"
        return "PROVIDER_RATE_LIMIT", "Rate limit"

    if isinstance(err, (asyncio.TimeoutError, httpx.ReadTimeout, httpx.ConnectTimeout)):
        return "PROVIDER_TIMEOUT", "Timeout"

    return "PROVIDER_ERROR", str(err)

# ---- LLM concurrency guard ----
_LLM_SEMAPHORES: dict[str, tuple[asyncio.Semaphore, int]] = {}

def _sem_for(provider: str) -> asyncio.Semaphore:
    cap = int(os.getenv("LLM_MAX_CONCURRENCY", "2"))
    entry = _LLM_SEMAPHORES.get(provider)
    if entry is None or entry[1] != cap:
        sem = asyncio.Semaphore(cap)
        _LLM_SEMAPHORES[provider] = (sem, cap)
        return sem
    return entry[0]

# Timeout c·∫•u h√¨nh cho m·ªói call LLM
_LLM_REQ_TIMEOUT = float(os.getenv("LLM_REQUEST_TIMEOUT", "25"))


def ddgs_search_text(query: str, max_results=15):
    """
    Search tool chung (kh√¥ng r√†ng bu·ªôc domain).
    D√πng random UA + proxy ƒë·ªÉ tr√°nh cache/block.
    """
    ua = _random_ua()
    proxy = random.choice(PROXIES) if PROXIES else None
    # regions = ["vi-vn", "us-en", "sg-en"]
    regions = ["us-en"]
    region = random.choice(regions)

    backends = [
        "bing",
        # "brave",
        # "duckduckgo",
        # "html",
        # "mojeek",
        # "mullvad_brave",
        # "mullvad_google",
        # "yandex",
        # "yahoo",
        # "wikipedia",
    ]
    random.shuffle(backends)
    for be in backends:
        try:
            with DDGS(timeout=60) as ddg:
                rows = list(
                    ddg.text(
                        query,
                        max_results=max_results,
                        region=region,
                        safesearch="off",
                        backend="auto",
                        timelimit="y",
                    )
                )
            if rows:
                logger.info(
                    "[DDGS] backend=%s rows=%d q=%r ua=%s proxy=%s region=%s",
                    be,
                    len(rows),
                    query,
                    ua["User-Agent"],
                    proxy,
                    region,
                )
                return rows
            time.sleep(0.8 + random.random() * 0.8)
        except Exception as e:
            logger.warning("‚ö†Ô∏è DDGS backend=%s error=%s proxy=%s", be, e, proxy)
            time.sleep(0.8 + random.random() * 0.8)

    return []


class GoogleSearchWithDelay(GoogleSearchTools):
    def run(self, query: str):
        time.sleep(random.uniform(2.5, 5.0))  # Delay t·ª± nhi√™n
        return super().run(query)


def google_cse_search(domain, industry_name, keywords):
    API_KEY = os.getenv("GOOGLE_API_KEY_SEARCH")
    if not API_KEY:
        logger.error("‚ùå GOOGLE_API_KEY_SEARCH ch∆∞a ƒë∆∞·ª£c set.")
        return None

    # Chu·∫©n h√≥a domain cho site: query
    norm_domain = re.sub(r"^https?://", "", domain).split("/")[0].lower()
    norm_domain = norm_domain.lstrip("www.")

    # TƒÉng kh·∫£ nƒÉng b·∫Øt hub: th√™m OR cho tag/tags/chu-de
    q_parts = (
        [f"site:{norm_domain}", industry_name]
        + list(keywords)
        + ["(tags OR tag OR chu-de)"]
    )
    query = " ".join(q_parts)

    url = "https://www.googleapis.com/customsearch/v1"
    params = {"key": API_KEY, "cx": "16c863775f52f42cd", "q": query, "num": 20}

    try:
        resp = requests.get(url, params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()
    except Exception as e:
        logger.warning("‚ùå Google CSE l·ªói: %s", e)
        return None

    items = data.get("items", [])
    if not items:
        logger.warning("‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£: %r", data)
        return None

    # --- helpers ---
    NUM_RE = re.compile(r"(\d{2,})")  # l·∫•y s·ªë >= 2 ch·ªØ s·ªë ƒë·ªÉ tr√°nh noise

    def extract_numeric_id(u: str) -> int:
        try:
            path = urlparse(u).path
            nums = NUM_RE.findall(path)
            return int(nums[-1]) if nums else -1  # l·∫•y s·ªë cu·ªëi trong path
        except Exception:
            return -1

    def kw_hits(u: str) -> int:
        lu = u.lower()
        return sum(1 for kw in keywords if kw.lower() in lu)

    def is_same_domain(u: str) -> bool:
        try:
            host = urlparse(u).netloc.lower()
        except Exception:
            return False
        host = host.lstrip("www.").lstrip("m.")
        return host.endswith(norm_domain)

    # L·ªçc candidates: ƒë√∫ng domain + l√† hub + c√≥ keyword trong URL
    candidates: list[str] = []
    for it in items:
        link = it.get("link", "")
        if not link:
            continue
        if not is_same_domain(link):
            continue
        lu = link.lower()
        if is_tag_hub_url(lu) and any(kw.lower() in lu for kw in keywords):
            candidates.append(link)
            logger.debug("CSE candidate: %s", link)

    if not candidates:
        logger.warning("‚ùå Kh√¥ng c√≥ hub h·ª£p l·ªá trong k·∫øt qu·∫£ CSE.")
        return None

    # Ch·ªçn theo: c√≥ s·ªë? ‚Üí ID l·ªõn ‚Üí s·ªë keyword kh·ªõp ‚Üí prefer /tags/ ‚Üí URL ng·∫Øn
    def hub_sort_key(u: str):
        lu = u.lower()
        uid = extract_numeric_id(lu)
        has_num = 1 if uid >= 0 else 0
        prefer_tags = 1 if "/tags/" in lu else 0
        return (has_num, uid, kw_hits(lu), prefer_tags, -len(u))

    pick = sorted(set(candidates), key=hub_sort_key, reverse=True)[0]
    logger.info("‚úÖ Ch·ªçn hub t·ª´ CSE: %s", pick)
    return pick


# DDGS Helpers
def _norm_host(h):
    h = h.lower()
    return h[4:] if h.startswith("www.") else h


def clean_duck_href(href: str) -> str | None:
    if not href:
        return None

    if "zhihu.com/tardis/" in href or "zhihu.com/question/" in href:
        return None

    # 1) Relative -> absolute
    href = urljoin("https://duckduckgo.com", href)

    p = urlparse(href)
    # 2) DuckDuckGo redirect: l·∫•y URL th·∫≠t t·ª´ query
    if p.netloc.endswith("duckduckgo.com") and p.path.startswith(("/l/", "/r/")):
        q = parse_qs(p.query)
        for key in ("uddg", "u", "rut"):
            if q.get(key):
                href = unquote(q[key][0])
                p = urlparse(href)
                break

    # 3) Google Translate proxy -> l·∫•y param 'u'
    if p.netloc.endswith("googleusercontent.com") and p.path.startswith("/translate"):
        q = parse_qs(p.query)
        if q.get("u"):
            href = unquote(q["u"][0])
            p = urlparse(href)

    # 4) G·ªçn AMP/mobile
    path = p.path.replace("/amp/", "/").replace("/amp", "/")
    host = p.netloc.lower()
    if host.startswith("www."):
        host = host[4:]

    # 5) L·∫Øp l·∫°i URL ƒë√£ chu·∫©n ho√°
    return urlunparse((p.scheme or "https", host, path, "", p.query, ""))


# --- Anti-block constants & helpers ---
BLOCK_PATTERNS = (
    "access denied",
    "forbidden",
    "blocked",
    "captcha",
    "cloudflare",
    "attention required",
    "unusual traffic",
    "verify you are a human",
    "challenge",
    "bot detection",
)


def _rand_headers() -> dict:
    # ‚Äúbrowsery‚Äù headers: Accept/Language + Sec-Fetch
    return {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
    }


def _looks_blocked(text: str | None) -> bool:
    if not text:
        return True
    low = text.lower()
    return any(k in low for k in BLOCK_PATTERNS)


def _random_ua() -> dict:
    """Sinh User-Agent ng·∫´u nhi√™n cho m·ªói query"""
    return {
        "User-Agent": (
            f"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            f"Chrome/{random.randint(118, 124)}.0.{random.randint(1000,9999)}.100 Safari/537.36"
        )
    }


class DomainPolicy:
    def __init__(self, max_concurrent: int = 2, min_gap_sec: float = 0.8):
        self.sem = asyncio.Semaphore(max_concurrent)
        self.min_gap = float(min_gap_sec)
        self.last_t = 0.0


class DomainLimiter:
    def __init__(self):
        self._policies: dict[str, DomainPolicy] = {}

    def policy(self, domain: str) -> DomainPolicy:
        if domain not in self._policies:
            # m·∫∑c ƒë·ªãnh: 2 lu·ªìng/host, gap ~0.8s gi·ªØa c√°c request
            self._policies[domain] = DomainPolicy(2, 0.8)
        return self._policies[domain]

    async def enter(self, url: str):
        d = urlparse(url).netloc
        pol = self.policy(d)
        await pol.sem.acquire()
        # enforce gap
        now = time.monotonic()
        wait = pol.min_gap - (now - pol.last_t)
        if wait > 0:
            await asyncio.sleep(wait + 0.1 * wait)  # th√™m t√≠ jitter
        pol.last_t = time.monotonic()
        return d  # tr·∫£ domain ƒë·ªÉ caller release sau

    def release(self, domain: str):
        self._policies[domain].sem.release()


DOMAIN_LIMITER = DomainLimiter()
# ---------------------------------------


def search_domain_duckduckgo(
    domain: str,
    industry_name: str,
    keywords: list[str],
    max_results,
    query_type=None,
):
    """
    T√¨m ki·∫øm hub/article link trong domain c·ª• th·ªÉ.
    Tr·∫£ v·ªÅ (rows, backend, query).
    """

    # Query c√≥ th√™m salt ƒë·ªÉ tr√°nh cache
    salt = random.randint(1000, 9999)
    suffixes = ["tag", "tags", "tin t·ª©c m·ªõi nh·∫•t"]  # C·∫≠p nh·∫≠t danh s√°ch suffixes

    # Ch·ªçn suffix d·ª±a tr√™n query_type n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if query_type == "tag":
        suffix = "tag"
    elif query_type == "tags":
        suffix = "tags"
    elif query_type == "news":
        suffix = "tin t·ª©c m·ªõi nh·∫•t"
    else:
        suffix = random.choice(suffixes)

    regions = ["us-en"]  # ho·∫∑c m·ªü r·ªông: ["vi-vn", "us-en", "sg-en"]
    region = random.choice(regions)
    query = f"site:{domain} {industry_name} {' '.join(keywords)} {suffix}"

    ua = _random_ua()
    proxy = random.choice(PROXIES) if PROXIES else None

    # T·∫≠p backend ƒëa d·∫°ng, sau ƒë√≥ shuffle ƒë·ªÉ random
    backends = [
        # "bing",
        # "brave",
        "duckduckgo",
        "google",
        # "mojeek",
        # "mullvad_brave",
        # "mullvad_google",
        # "yandex",
        # "yahoo",
        # "wikipedia",
    ]
    random.shuffle(backends)

    for be in backends:
        try:
            with DDGS(timeout=60) as ddg:
                rows = list(
                    ddg.text(
                        query,
                        region=region,
                        max_results=max_results,
                        safesearch="off",
                        backend="auto",
                        timelimit="y",
                    )
                )

            if rows:
                logger.info(
                    "[DDG] domain=%s backend=%s rows=%d query=%r ua=%s proxy=%s region=%s",
                    domain,
                    be,
                    len(rows),
                    query,
                    ua["User-Agent"],
                    proxy,
                    region,
                )
                return rows, be, query

            # N·∫øu r·ªóng th√¨ jitter delay r·ªìi th·ª≠ backend kh√°c
            time.sleep(0.8 + random.random() * 0.8)

        except Exception as e:
            logger.warning("‚ö†Ô∏è DDG backend=%s error=%s proxy=%s", be, e, proxy)
            time.sleep(0.8 + random.random() * 0.8)

    return [], None, query


def get_first_search_link(
    domain: str, keywords: list[str], industry_name: str
) -> str | None:
    wanted_host = _norm_host(domain)
    found_domain_but_no_hub = False

    def pick_from_rows(rows: list[dict]) -> tuple[str | None, bool]:
        """Tr·∫£ v·ªÅ (link, has_same_domain)"""
        same_domain: list[str] = []
        hubs: list[str] = []

        NUM_RE = re.compile(r"(\d{2,})")  # b·ªè qua s·ªë 1 ch·ªØ s·ªë (noise)

        def extract_numeric_id(u: str) -> int:
            try:
                path = urlparse(u).path
            except Exception:
                return -1
            nums = NUM_RE.findall(path)
            return int(nums[-1]) if nums else -1  # l·∫•y s·ªë cu·ªëi trong path

        for r in rows:
            raw = r.get("href")
            href = clean_duck_href(raw)
            if not href:
                continue
            host = _norm_host(urlparse(href).netloc)
            if host != wanted_host:
                continue

            same_domain.append(href)

            # Hub h·ª£p l·ªá = c√≥ /tag|/tags|/chu-de + c√≥ keyword trong URL
            if is_tag_hub_url(href) and any(
                kw.lower() in href.lower() for kw in keywords
            ):
                hubs.append(href)

        has_same_domain = bool(same_domain)

        # ∆Øu ti√™n hub h·ª£p l·ªá
        if hubs:

            def hub_sort_key(u: str):
                lu = u.lower()
                uid = extract_numeric_id(lu)
                has_num = 1 if uid >= 0 else 0
                kw_hits = sum(1 for kw in keywords if kw.lower() in lu)
                prefer_tags = 1 if "/tags/" in lu else 0
                return (has_num, uid, kw_hits, prefer_tags, -len(u))

            # unique + sort gi·∫£m d·∫ßn theo key
            hubs = sorted(set(hubs), key=hub_sort_key, reverse=True)
            return hubs[0], has_same_domain

        # N·∫øu kh√¥ng c√≥ hub th√¨ tr·∫£ None
        return None, has_same_domain

    # Danh s√°ch c√°c query type ƒë·ªÉ th·ª≠ lu√¢n phi√™n
    query_types = ["news", "tag", "tags"]

    # --- Retry loop ---
    max_retry = 3
    for attempt in range(max_retry):
        # Thay query
        query_type = query_types[attempt % len(query_types)]
        rows, be, used_q = search_domain_duckduckgo(
            domain,
            industry_name,
            list(keywords),
            max_results=15,
            query_type=query_type,
        )

        logger.info(
            "[get_first_search_link] try #%d backend=%s query=%r rows=%d",
            attempt + 1,
            be,
            used_q,
            len(rows) if rows else 0,
        )

        if rows:
            for i, r in enumerate(rows, 1):
                raw = r.get("href")
                href = clean_duck_href(raw)
                logger.info("  [%d] raw=%s | cleaned=%s", i, raw, href)

            link, has_same_domain = pick_from_rows(rows)
            if link:
                return link

            # N·∫øu c√≥ link c√πng domain nh∆∞ng kh√¥ng ph·∫£i hub ‚Üí d·ª´ng
            if has_same_domain:
                found_domain_but_no_hub = True

        # N·∫øu ch∆∞a t√¨m th·∫•y link n√†o ƒë√∫ng domain ‚Üí retry
        delay = 2 * (2**attempt) + random.random()
        if found_domain_but_no_hub:
            logger.warning(
                "‚ö†Ô∏è ƒê√£ th·∫•y domain nh∆∞ng ch∆∞a t√¨m th·∫•y hub, ch·ªù %.1fs r·ªìi th·ª≠ l·∫°i (attempt %d/%d)...",
                delay,
                attempt + 1,
                max_retry,
            )
        else:
            logger.warning(
                "‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y link n√†o c√πng domain, ch·ªù %.1fs r·ªìi th·ª≠ l·∫°i...", delay
            )

        time.sleep(delay)

    # --- Sau 3 l·∫ßn retry ---
    if found_domain_but_no_hub:
        logger.warning(
            "‚ö†Ô∏è ƒê√£ t√¨m th·∫•y domain %s sau %d l·∫ßn nh∆∞ng kh√¥ng c√≥ hub h·ª£p l·ªá ‚Üí b·ªè qua",
            domain,
            max_retry,
        )
    else:
        logger.warning(
            "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y hub h·ª£p l·ªá trong %d l·∫ßn th·ª≠, fallback Google CSE: %s",
            max_retry,
            domain,
        )
        return google_cse_search(domain, industry_name, keywords)


# Semaphore ƒë·ªÉ limit concurrent playwright instances
_playwright_sem = Semaphore(2)


class PlaywrightPool:
    _instance = None

    @classmethod
    def instance(cls):
        if not cls._instance:
            cls._instance = cls()
        return cls._instance

    def __init__(self):
        self._started = False
        self._pw = None
        self._browser = None
        self._sem = asyncio.Semaphore(3)  # t·ªïng s·ªë page song song
        self._storage_state: dict[str, dict] = {}  # per-domain storage_state

    async def start(self):
        if self._started:
            return
        self._pw = await async_playwright().start()
        self._browser = await self._pw.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--disable-blink-features=AutomationControlled",
                "--disable-background-timer-throttling",
                "--disable-renderer-backgrounding",
            ],
        )
        self._started = True

    async def close(self):
        if self._browser:
            await self._browser.close()
        if self._pw:
            await self._pw.stop()
        self._started = False
        self._browser = None
        self._pw = None

    async def fetch(
        self, url: str, timeout_ms: int = 25000, referer: str | None = None
    ) -> str:
        await self.start()

        domain = urlparse(url).netloc
        # per-domain policy: limit & pacing
        await DOMAIN_LIMITER.enter(url)
        try:
            async with self._sem:
                # d√πng storage_state ƒë·ªÉ t√°i s·ª≠ d·ª•ng cookie per-domain (nh√¨n ‚Äúng∆∞·ªùi‚Äù h∆°n)
                storage_state = self._storage_state.get(domain)
                ua = _random_ua()

                context = await self._browser.new_context(
                    viewport={"width": 1366, "height": 768},
                    user_agent=ua["User-Agent"],
                    storage_state=storage_state,  # c√≥ th·ªÉ None
                    extra_http_headers={
                        **_rand_headers(),
                        **({"Referer": referer} if referer else {}),
                    },
                    locale="vi-VN",
                )
                # stealth: che webdriver, languages, platform, permissions...
                await context.add_init_script(
                    """
                    Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                    window.chrome = { runtime: {} };
                    Object.defineProperty(navigator, 'languages', {get: () => ['vi-VN', 'vi', 'en-US', 'en']});
                    Object.defineProperty(navigator, 'platform', {get: () => 'Win32'});
                    const originalQuery = window.navigator.permissions.query;
                    window.navigator.permissions.query = (parameters) => (
                        parameters.name === 'notifications' ?
                            Promise.resolve({ state: Notification.permission }) :
                            originalQuery(parameters)
                    );
                    // WebGL vendor spoof nh·∫π
                    const getParameter = WebGLRenderingContext.prototype.getParameter;
                    WebGLRenderingContext.prototype.getParameter = function(par){
                        if (par === 37445) return 'Intel Inc.';       // UNMASKED_VENDOR_WEBGL
                        if (par === 37446) return 'Intel(R) UHD';     // UNMASKED_RENDERER_WEBGL
                        return getParameter.call(this, par);
                    };
                """
                )

                page = await context.new_page()
                blocked = False
                html = ""

                try:
                    await page.goto(
                        url, timeout=timeout_ms, wait_until="domcontentloaded"
                    )
                    await page.wait_for_timeout(450 + int(200 * random.random()))
                    # Cloudflare/challenge check
                    if (
                        await page.locator(
                            "div#challenge-form, div#challenge-container, iframe[title*=captcha]"
                        ).count()
                        > 0
                    ):
                        blocked = True
                    html = await page.content()
                    if len(html) < 1500:
                        body_text = await page.text_content("body") or ""
                        if _looks_blocked(body_text):
                            blocked = True
                finally:
                    # l∆∞u cookie/storage_state cho domain n·∫øu kh√¥ng b·ªã ch·∫∑n
                    try:
                        if not blocked:
                            self._storage_state[domain] = await context.storage_state()
                    except Exception:
                        pass
                    await context.close()

                if blocked:
                    # Cooldown domain + h·∫° concurrency c·ªßa domain t·∫°m th·ªùi
                    DOMAIN_LIMITER.policy(domain).min_gap = min(
                        2.5, DOMAIN_LIMITER.policy(domain).min_gap * 1.5
                    )
                    raise RuntimeError("Blocked/challenge detected")

                return html
        finally:
            DOMAIN_LIMITER.release(domain)


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=6))
async def crawl_with_playwright(url: str) -> str:
    # Gi·ªØ l·∫°i retry decorator c·ªßa b·∫°n
    try:
        html = await PlaywrightPool.instance().fetch(url, timeout_ms=25000)
        if not html or len(html) < 1000:
            return ""
        return html
    except Exception as e:
        logger.warning(f"Playwright error for {url}: {e}")
        return ""


# Optional: Fallback function cho c√°c site kh√≥
async def robust_crawl(url: str) -> str:
    """Crawl v·ªõi fallback mechanism"""
    try:
        return await crawl_with_playwright(url)
    except Exception as e:
        print(f"üéØ Playwright failed, trying fallback for {url}: {e}")
        # Th√™m fallback logic ·ªü ƒë√¢y (requests + cloudscraper, etc.)
        raise  # Ho·∫∑c implement fallback


# --- START: Helpers function ---
async def _maybe_await(fn):
    if inspect.iscoroutinefunction(fn):
        return await fn()
    res = fn()
    if inspect.isawaitable(res):
        return await res
    return res


async def _get_response_text(resp):
    if hasattr(resp, "content"):
        return resp.content
    if hasattr(resp, "text"):
        return resp.text
    # stream
    if hasattr(resp, "__aiter__"):
        chunks = []
        async for part in resp:
            chunks.append(getattr(part, "content", str(part)))
        return "".join(chunks)
    return str(resp)


# B·∫Øt dd/mm/yyyy (c√≥ th·ªÉ k√®m gi·ªù)
_VN_DATE_RE = re.compile(
    r"(?P<d>\d{1,2})[/-](?P<m>\d{1,2})[/-](?P<y>\d{4})(?:\s+(?P<h>\d{1,2}):(?P<min>\d{2}))?"
)


def _norm_iso(s: str | None) -> str | None:
    if not s:
        return None
    s = s.strip()
    # ISO trong meta/json-ld
    try:
        from datetime import datetime

        s2 = s.replace("Z", "+00:00")
        return datetime.fromisoformat(s2).date().isoformat()
    except Exception:
        pass
    # dd/mm/yyyy (VN)
    m = _VN_DATE_RE.search(s)
    if m:
        d, mo, y = int(m.group("d")), int(m.group("m")), int(m.group("y"))
        if 1 <= d <= 31 and 1 <= mo <= 12:
            return f"{y:04d}-{mo:02d}-{d:02d}"
    return None


def extract_dates_rule_based(html: str, url: str):
    """
    Tr·∫£ v·ªÅ dict:
    {
      'published_iso': 'YYYY-MM-DD' | None,
      'modified_iso': 'YYYY-MM-DD' | None,
      'source_published_text': 'chu·ªói g·ªëc' | None
    }
    ∆Øu ti√™n: JSON-LD -> meta article:published_time -> <time datetime> -> text kh·ªëi meta g·∫ßn ti√™u ƒë·ªÅ.
    """
    soup = BeautifulSoup(html, "html.parser")
    head = soup.find("head") or soup

    # 1) JSON-LD (NewsArticle/Article)
    for s in head.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string or "")
            items = data if isinstance(data, list) else [data]
            for it in items:
                if isinstance(it, dict) and it.get("@type") in (
                    "NewsArticle",
                    "Article",
                    "Report",
                ):
                    dp = it.get("datePublished") or it.get("dateCreated")
                    dm = it.get("dateModified")
                    pub_iso = _norm_iso(dp)
                    mod_iso = _norm_iso(dm)
                    if pub_iso:
                        return {
                            "published_iso": pub_iso,
                            "modified_iso": mod_iso,
                            "source_published_text": dp,
                        }
        except Exception:
            pass

    # 2) OpenGraph/Article meta
    meta_pub = head.find("meta", {"property": "article:published_time"}) or head.find(
        "meta", {"name": "pubdate"}
    )
    if meta_pub and meta_pub.get("content"):
        pub_iso = _norm_iso(meta_pub["content"])
        meta_mod = head.find("meta", {"property": "article:modified_time"})
        mod_iso = (
            _norm_iso(meta_mod["content"])
            if meta_mod and meta_mod.get("content")
            else None
        )
        if pub_iso:
            return {
                "published_iso": pub_iso,
                "modified_iso": mod_iso,
                "source_published_text": meta_pub.get("content"),
            }

    # 3) <time datetime="...">
    t = soup.find("time", datetime=True)
    if t:
        pub_iso = _norm_iso(t.get("datetime"))
        if pub_iso:
            return {
                "published_iso": pub_iso,
                "modified_iso": None,
                "source_published_text": t.get_text(" ", strip=True)
                or t.get("datetime"),
            }

    # 4) Text g·∫ßn ti√™u ƒë·ªÅ (ph√π h·ª£p Lao ƒê·ªông / Thanh Ni√™n)
    # Gom c√°c kh·ªëi meta ph·ªï bi·∫øn quanh ti√™u ƒë·ªÅ
    meta_blocks = []
    for sel in [
        ".article__meta",
        ".meta",
        ".date",
        ".ldo-meta",
        "span.time",
        ".details__meta",
        ".details__time",
    ]:
        meta_blocks += [el.get_text(" ", strip=True) for el in soup.select(sel)]
    joined = " | ".join([b for b in meta_blocks if b])

    m = _VN_DATE_RE.search(joined)
    if m:
        d, mo, y = int(m.group("d")), int(m.group("m")), int(m.group("y"))
        pub_iso = f"{y:04d}-{mo:02d}-{d:02d}"
        return {
            "published_iso": pub_iso,
            "modified_iso": None,
            "source_published_text": joined[m.start() : m.end()],
        }

    return {"published_iso": None, "modified_iso": None, "source_published_text": None}


def extract_title_rule_based(html: str) -> str | None:
    soup = BeautifulSoup(html, "html.parser")
    # ∆Øu ti√™n th·∫ª <h1>, r·ªìi meta og:title
    h1 = soup.find("h1")
    if h1 and h1.get_text(strip=True):
        return h1.get_text(strip=True)
    og = soup.find("meta", {"property": "og:title"})
    if og and og.get("content"):
        return og["content"].strip()
    if soup.title and soup.title.get_text(strip=True):
        return soup.title.get_text(strip=True)
    return None


async def validate_and_normalize_link(
    raw_url: str, allowed_domain: str, timeout=10
) -> str | None:
    """Tr·∫£ v·ªÅ URL ƒë√£ chu·∫©n ho√° n·∫øu h·ª£p l·ªá, ng∆∞·ª£c l·∫°i None."""
    try:
        r = await http_client.get(
            raw_url, follow_redirects=True, timeout=timeout, headers=_random_ua()
        )
        if r.status_code != 200:
            return None
        ctype = r.headers.get("Content-Type", "")
        if "text/html" not in ctype:
            return None

        final_url = str(r.url)

        u = urlparse(final_url)
        # B·ªè tracking params (m·ªü r·ªông)
        drop_keys = {"fbclid", "gclid", "yclid", "mc_cid", "mc_eid", "ref", "ref_src"}
        q = [
            (k, v)
            for k, v in parse_qsl(u.query, keep_blank_values=True)
            if not (k.lower().startswith("utm_") or k.lower() in drop_keys)
        ]
        final_url = urlunparse(u._replace(query=urlencode(q, doseq=True)))

        # Canonical n·∫øu c√≥
        soup = BeautifulSoup(r.text, "html.parser")
        can = soup.find("link", rel=lambda x: x and "canonical" in x.lower())
        if can and can.get("href"):
            final_url = can["href"].strip()

        # Check domain cu·ªëi c√πng
        host = urlparse(final_url).netloc.lower().lstrip("www.")
        allowed = allowed_domain.lower().lstrip("www.")
        if not (host == allowed or host.endswith("." + allowed)):
            return None

        # (tu·ª≥ ch·ªçn) basic sanity: ph·∫£i c√≥ <h1> ho·∫∑c og:title
        if not (soup.find("h1") or soup.find("meta", {"property": "og:title"})):
            return None

        return final_url
    except Exception:
        return None


# Check hub link helpers
_TAG_HUB_RE = re.compile(
    r"(?:/(?:tags?|chu-de|tu-khoa|tag)/)|(?:/tu-khoa/[^/?#]+-tag\d+(?:\.tpo)?(?:/|$))"
)


def is_tag_hub_url(u: str) -> bool:
    try:
        path = urlparse(u).path.lower()
    except Exception:
        path = (u or "").lower()
    return bool(_TAG_HUB_RE.search(path))


def _quick_summary_from_html(html: str, max_chars: int = 360) -> str:
    soup = BeautifulSoup(html or "", "html.parser")
    # l·∫•y c√°c ƒëo·∫°n p c√≥ ƒë·ªô d√†i > 100 k√Ω t·ª±, ∆∞u ti√™n ph·∫ßn ƒë·∫ßu
    paras = [
        p.get_text(" ", strip=True)
        for p in soup.select("article p, .article p, .detail p, p")
    ]
    paras = [t for t in paras if t and len(t) > 80]
    text = " ".join(paras[:3]) if paras else ""
    return (text[:max_chars] + "‚Ä¶") if len(text) > max_chars else text


# --- END: helpers function ---
class AgentManager:
    _instance: Optional["AgentManager"] = None

    @classmethod
    def get_instance(cls) -> "AgentManager":
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self):
        # Map (session_id, provider) -> asyncio.Task
        self._tasks: Dict[Tuple[str, str], asyncio.Task] = {}
        # Map (session_id, provider) -> cancel token/c·ªù t·ª± nguy·ªán (n·∫øu worker c·∫ßn)
        self._tokens: Dict[Tuple[str, str], asyncio.Event] = {}

    def get_token(self, session_id: str, provider: str) -> asyncio.Event:
        key = (session_id, provider)
        tok = self._tokens.get(key)
        if tok is None:
            tok = asyncio.Event()
            self._tokens[key] = tok
        return tok

    # G·ªçi khi b·∫°n kh·ªüi ch·∫°y m·ªôt provider
    def register_provider_task(
        self, session_id: str, provider: str, task: asyncio.Task
    ):
        key = (session_id, provider)
        self._tasks[key] = task
        # t·∫°o token cho cooperative cancel (n·∫øu worker check token)
        self._tokens.setdefault(key, asyncio.Event())
        logger.info("Registered provider task: %s %s", session_id, provider)

        def _cleanup(_):
            # Task k·∫øt th√∫c th√¨ d·ªçn registry
            self._tasks.pop(key, None)
            self._tokens.pop(key, None)
            logger.info("Cleaned up provider task: %s %s", session_id, provider)

        task.add_done_callback(_cleanup)

    def get_cancel_event(self, session_id: str, provider: str) -> asyncio.Event:
        # Worker c√≥ th·ªÉ g·ªçi h√†m n√†y ƒë·ªÉ l·∫•y event ki·ªÉm tra trong v√≤ng l·∫∑p
        return self._tokens.setdefault((session_id, provider), asyncio.Event())

    def cancel_provider(self, session_id: str, provider: str) -> bool:
        """
        H·ªßy provider ƒëang ch·∫°y cho session. H·ªó tr·ª£ c·∫£ hard-cancel (task.cancel)
        v√† soft-cancel (ƒë·∫∑t event ƒë·ªÉ worker t·ª± tho√°t).
        """
        key = (session_id, provider)
        task = self._tasks.get(key)
        if not task:
            return False

        # ƒê·∫∑t soft-cancel flag tr∆∞·ªõc (cooperative)
        token = self._tokens.get(key)
        if token and not token.is_set():
            token.set()
            logger.info(
                "Set cancel flag for provider %s (session %s)", provider, session_id
            )

        # Hard-cancel n·∫øu task v·∫´n ch∆∞a k·∫øt th√∫c
        if not task.done() and not task.cancelled():
            task.cancel()
            logger.info(
                "Cancelled asyncio task for provider %s (session %s)",
                provider,
                session_id,
            )
        return True

    # Tu·ª≥ b·∫°n: h·ªßy t·∫•t c·∫£ provider trong m·ªôt session
    def cancel_all_providers(self, session_id: str):
        keys = [k for k in self._tasks.keys() if k[0] == session_id]
        for _, provider in keys:
            self.cancel_provider(session_id, provider)

    def is_running(self, session_id: str, provider: str) -> bool:
        """Tr·∫£ v·ªÅ True n·∫øu provider c√≤n task ƒëang ch·∫°y."""
        return (session_id, provider) in self._tasks and not self._tasks[
            (session_id, provider)
        ].done()


class LLMUserFallbackMixin:
    async def _arun_with_user_fallback(
        self,
        prompt: str,
        *,
        session_id: str,
        preferred_provider: str | None = None,
        preferred_model: str | None = None,
    ):
        providers = _configured_providers_in_order(settings, preferred_provider)
        asked = set()  # tr√°nh h·ªèi tr√πng 1 provider/model trong c√πng l∆∞·ª£t
        _timeout_retry_counter = {}
        if not providers:
            raise RuntimeError("No LLM provider configured")

        while True:
            for i, prov in enumerate(list(providers)):
                model_id = (
                    preferred_model
                    if prov == (preferred_provider or "").lower() and preferred_model
                    else PROVIDER_MODEL_MAP.get(prov, PROVIDER_MODEL_MAP["openai"])[
                        "default"
                    ]
                )
                try:
                    token = AgentManager.get_instance().get_token(session_id, prov)

                    # N·∫øu ƒë√£ b·ªã h·ªßy tr∆∞·ªõc khi g·ªçi
                    if token.is_set():
                        raise asyncio.CancelledError()
                    # y√™u c·∫ßu class c√≥ self._create_agent() v√† self.agent
                    self.model = get_llm_model(prov, model_id)
                    self._create_agent()
                    # T·∫°o task + ƒëƒÉng k√Ω ƒë·ªÉ cancel c·ª©ng ƒë∆∞·ª£c qua AgentManager
                    async with _sem_for(prov):
                        task = asyncio.create_task(self.agent.arun(prompt, session_id=session_id))
                        AgentManager.get_instance().register_provider_task(session_id, prov, task)
                        try:
                            return await asyncio.wait_for(task, timeout=_LLM_REQ_TIMEOUT)
                        except asyncio.TimeoutError:
                            task.cancel()
                            with contextlib.suppress(asyncio.CancelledError):
                                await task
                            raise

                except Exception as e:
                    code, msg = _map_llm_error(e) or ("PROVIDER_ERROR", str(e))
                    remaining = providers[i + 1 :]

                    # ‚úÖ TIMEOUT / RATE LIMIT: gi·ªØ nguy√™n provider, retry im l·∫∑ng (kh√¥ng popup)
                    if code in ("PROVIDER_TIMEOUT", "PROVIDER_RATE_LIMIT"):
                        key = (session_id, prov, model_id)
                        tries = _timeout_retry_counter.get(key, 0)
                        if tries < 3:  # tu·ª≥ ch·ªânh
                            _timeout_retry_counter[key] = tries + 1
                            backoff = min(2**tries, 8)  # 1s, 2s, 4s, cap 8s
                            logger.info(
                                f"[LLM retry] {prov}({model_id}) {code} ‚Üí retry {tries+1}/3 in {backoff}s"
                            )
                            await asyncio.sleep(backoff)
                            # x·∫øp l·∫°i list ƒë·ªÉ l·∫ßn k·∫ø v·∫´n ch·∫°y provider hi·ªán t·∫°i (kh√¥ng switch)
                            providers = (
                                providers[: i + 1]
                                + [prov]
                                + providers[i + 1 :]
                            )
                            continue
                        else:
                            # H·∫øt s·ªë l·∫ßn retry ‚Üí b·ªè qua provider n√†y, chuy·ªÉn t·ª± ƒë·ªông sang provider k·∫ø TI·∫æP (kh√¥ng popup)
                            logger.warning(
                                f"[LLM retry] {prov}({model_id}) exhausted retries ‚Üí try next provider silently"
                            )
                            continue

                    # ‚úÖ H·∫æT QUOTA: m·ªõi popup h·ªèi chuy·ªÉn
                    if code == "PROVIDER_NO_QUOTA":
                        async with fallback_lock_by_session[session_id]:
                            if (prov, model_id) in asked:
                                continue
                            asked.add((prov, model_id))

                            if not remaining:
                                await event_bus.publish(
                                    session_id,
                                    {
                                        "type": "provider_error",
                                        "provider": prov,
                                        "model": model_id,
                                        "code": code,
                                        "message": msg,
                                        "final": True,
                                        "next_options": [],
                                    },
                                )
                                break

                            next_choice = remaining[0]
                            await event_bus.publish(
                                session_id,
                                {
                                    "type": "propose_switch",
                                    "from_provider": prov,
                                    "from_model": model_id,
                                    "message": msg,
                                    "code": code,
                                    "next_options": remaining,
                                    "suggested": next_choice,
                                },
                            )

                            decision = await decision_bus.wait(session_id, timeout=15.0)

                        if not decision:
                            # kh√¥ng ph·∫£n h·ªìi ‚Üí auto th·ª≠ provider k·∫ø
                            continue
                        act = (decision.get("action") or "").lower()
                        if act == "abort":
                            raise RuntimeError("User aborted run")
                        if act == "switch":
                            picked = (decision.get("provider") or next_choice).lower()
                            if picked in remaining:
                                providers = (
                                    providers[: i + 1]
                                    + [picked]
                                    + [x for x in remaining if x != picked]
                                )
                            continue

                    # üîê C√°c l·ªói kh√°c (AUTH/ERROR chung): kh√¥ng popup; m·∫∑c ƒë·ªãnh chuy·ªÉn provider k·∫ø (ho·∫∑c b·∫°n mu·ªën gi·ªØ nguy√™n th√¨ ƒë·ªïi l·∫°i)
                    logger.warning(
                        f"[LLM] {prov}({model_id}) error={code}: {msg} ‚Üí try next provider"
                    )
                    continue

            logger.info("üîÅ Ho√†n th√†nh 1 v√≤ng provider, ngh·ªâ 5s r·ªìi th·ª≠ l·∫°i...")
            await asyncio.sleep(5)


class HubCrawlTool(LLMUserFallbackMixin):
    def __init__(
        self,
        model,
        config,
        session_id,
        parser: ArticleParser,
        check_pause_or_cancel: Optional[callable] = None,
    ):
        self.model = model
        self.session_id = session_id
        self.config = config
        self.parser = parser
        self.check_pause_or_cancel = check_pause_or_cancel or (lambda: None)
        self._create_agent()

    def _create_agent(self):
        self.agent = Agent(
            name="HubCrawler",
            role="Web Crawler v√† Content Extractor",
            tools=[Crawl4aiTools(max_length=10000)],
            model=self.model,
            instructions="""
            B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch n·ªôi dung b√°o ch√≠ tr·ª±c tuy·∫øn, ƒë·∫∑c bi·ªát th√†nh th·∫°o trong vi·ªác nh·∫≠n di·ªán v√† x·ª≠ l√Ω c√°c ƒë·ªãnh d·∫°ng ng√†y th√°ng ƒëa d·∫°ng trong vƒÉn b·∫£n b√°o ch√≠.

            Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc v√† hi·ªÉu n·ªôi dung b√†i b√°o trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh, sau ƒë√≥ tr√≠ch xu·∫•t ch√≠nh x√°c c√°c th√¥ng tin sau:
            - Ti√™u ƒë·ªÅ b√†i b√°o  
            - Ng√†y ph√°t h√†nh b√†i b√°o
            - T√≥m t·∫Øt n·ªôi dung ch√≠nh  
            - ƒê∆∞·ªùng d·∫´n g·ªëc c·ªßa b√†i vi·∫øt

            Ch·ªâ cung c·∫•p d·ªØ li·ªáu ·ªü ƒë·ªãnh d·∫°ng JSON theo y√™u c·∫ßu. Kh√¥ng gi·∫£i th√≠ch th√™m b·∫•t k·ª≥ ƒëi·ªÅu g√¨ kh√°c.
            """,
            show_tool_calls=True,
            markdown=True,
        )

    async def run(
        self,
        media_source: MediaSource,
        keywords: List[str],
        start_date: datetime,
        end_date: datetime,
        industry_name: Optional[str] = None,
    ) -> CrawlResult:
        try:
            hub_url = get_first_search_link(
                media_source.domain, keywords, industry_name
            )
            if not hub_url:
                logger.warning(
                    f"[{media_source.name}] ‚ùå Kh√¥ng t√¨m th·∫•y hub ph√π h·ª£p cho t·ª´ kh√≥a: {keywords}"
                )

            logger.info(f"[{media_source.name}] üîó Hub URL: {hub_url}")
            await _maybe_await(self.check_pause_or_cancel)

            html = ""
            if hub_url and hub_url.startswith(("http://", "https://")):
                html = await crawl_with_playwright(hub_url)
            else:
                logger.error(
                    f"[{media_source.name}] ‚ùå Hub URL tr·ªëng ho·∫∑c kh√¥ng h·ª£p l·ªá: {hub_url}"
                )
            soup = BeautifulSoup(html, "html.parser")

            article_links = []
            base = f"https://{media_source.domain}/"
            for a in soup.select("a"):
                href = a.get("href", "")
                if not href:
                    continue

                if href.startswith(
                    ("#", "javascript:", "vbscript:", "mailto:", "tel:")
                ):
                    continue

                full_url = urljoin(base, href)
                p = urlparse(full_url)

                # Ch·ªâ nh·∫≠n http(s) h·ª£p l·ªá + c√≥ netloc
                if p.scheme not in ("http", "https") or not p.netloc:
                    continue

                # Lo·∫°i link trang ch·ªß/ƒë∆∞·ªùng d·∫´n r·ªóng, trang tag/video
                if p.path in ("", "/") or p.path.startswith(("/tags/", "/video/")):
                    continue

                _norm = lambda s: "".join(
                    c
                    for c in unicodedata.normalize("NFD", s or "")
                    if unicodedata.category(c) != "Mn"
                ).lower()
                # industry_name ƒë√£ c√≥; _norm ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a
                ind_norm = set()
                if industry_name:
                    base = _norm(industry_name)          # v√≠ d·ª•: "sua (uht)" -> "sua (uht)" (b·ªè d·∫•u, lower)
                    if base:
                        ind_norm.add(base)

                        # B·∫£n ƒë·ªì bi·∫øn th·ªÉ theo ng√†nh (c√≥ c·∫£ d·∫°ng kh√¥ng d·∫•u, c√≥/kh√¥ng d·∫•u g·∫°ch, v√† 1 s·ªë t·ª´ ti·∫øng Anh hay g·∫∑p)
                        IND_SYNONYMS = {
                            "sua": {
                                "s·ªØa", "sua", "uht", "sua tiet trung", "s·ªØa ti·ªát tr√πng"
                            },
                            "dau an": {
                                "dau an", "dau-an", "dauan", "cooking oil", "edible oil"
                            },
                            "gia vi": {
                                "gia vi", "gia-vi", "seasoning", "spices", "condiment"
                            },
                            "gao": {
                                "gao", "gao-ngu-coc", "gao va ngu coc", "gao-ngu-coc", "rice"
                            },
                            "ngu coc": {
                                "ngu coc", "ngu-coc", "cereal", "cereals", "grain", "grains"
                            },
                            "homecare": {
                                "homecare", "home-care", "cham soc nha cua", "cham-soc-nha-cua",
                                "ve sinh nha cua", "ve-sinh-nha-cua"
                            },
                        }

                        # N·∫øu t√™n ng√†nh normalized ch·ª©a kho√° n√†o, n·∫°p c√°c bi·∫øn th·ªÉ t∆∞∆°ng ·ª©ng
                        for key, variants in IND_SYNONYMS.items():
                            if key in base:
                                ind_norm |= {_norm(v) for v in variants}

                has_kw = (
                    any(kw.lower() in p.path.lower() for kw in keywords)
                    if p.path
                    else False
                )
                has_ind = (
                    any(v in p.path.lower() for v in ind_norm) if ind_norm else False
                )
                if not (has_kw or has_ind):
                    continue

                article_links.append(full_url)

            valid_links = list(
                dict.fromkeys(
                    link
                    for link in article_links
                    if media_source.domain in link
                    and "admicro" not in link
                    and "adn" not in link
                )
            )

            logger.info(
                f"[{media_source.name}] üîó T√¨m ƒë∆∞·ª£c {len(valid_links)} b√†i vi·∫øt t·ª´ hub"
            )

            sem = Semaphore(4)  # tu·ª≥ quota
            start_t = time.monotonic()
            all_articles = []

            async def _retry_process_link(
                process_link, link: str, max_retries: int = 2
            ) -> list:
                """
                G·ªçi process_link(link) v·ªõi retry + exponential backoff.
                - Th√†nh c√¥ng khi tr·∫£ v·ªÅ list kh√¥ng r·ªóng.
                - N·∫øu parse tr·∫£ [] ho·∫∑c n√©m exception ‚Üí retry (t·ªëi ƒëa max_retries).
                - H·∫øt retry ‚Üí tr·∫£ [] ƒë·ªÉ pipeline ti·∫øp t·ª•c.
                """
                for attempt in range(max_retries + 1):  # 0..max_retries
                    try:
                        parsed = await process_link(link)
                        if parsed:  # list c√≥ ph·∫ßn t·ª≠
                            return parsed
                        # treat empty as failure ƒë·ªÉ th·ª≠ l·∫°i
                        raise RuntimeError("Parser returned empty list")
                    except Exception as e:
                        # L·∫ßn cu·ªëi ‚Üí d·ª´ng
                        if attempt >= max_retries:
                            # log ng·∫Øn g·ªçn; c√≥ th·ªÉ th√™m traceback n·∫øu c·∫ßn
                            logger.warning(
                                f"Parse fail {link}: {e} (exhausted retries)"
                            )
                            return []

                        # Backoff + jitter nh·∫π
                        delay = min(2**attempt, 8) + random.random() * 0.5
                        logger.info(
                            f"[retry] parse {link} attempt {attempt + 1}/{max_retries} in {delay:.1f}s: {e}"
                        )
                        await asyncio.sleep(delay)
                        # loop ti·∫øp

            async def process_link(link):
                await _maybe_await(self.check_pause_or_cancel)

                # 1) HTTP tr∆∞·ªõc
                try:
                    dom = urlparse(link).netloc
                    await DOMAIN_LIMITER.enter(link)
                    r = await http_client.get(
                        link,
                        timeout=15,
                        follow_redirects=True,
                        headers={
                            **_random_ua(),
                            **_rand_headers(),
                            "Referer": f"https://{dom}/",
                        },
                    )
                finally:
                    DOMAIN_LIMITER.release(dom)

                content_type = r.headers.get("content-type", "")
                art_html = (
                    r.text
                    if (r.status_code == 200 and "text/html" in content_type)
                    else ""
                )

                # 2) N·∫øu nghi b·ªã block/HTML qu√° ng·∫Øn ‚Üí Playwright
                if (not art_html) or (len(art_html) < 1500) or _looks_blocked(art_html):
                    art_html = await crawl_with_playwright(link)

                # 2. Extract ng√†y + ti√™u ƒë·ªÅ
                meta = extract_dates_rule_based(art_html, link)
                title_rb = extract_title_rule_based(art_html)

                parsed = None

                def _is_valid_article(obj) -> bool:
                    """ƒê·∫£m b·∫£o article c√≥ ƒë·ªß field t·ªëi thi·ªÉu."""
                    if not obj:
                        return False
                    # parser.parse c√≥ th·ªÉ tr·∫£ list[Article], ho·∫∑c m·ªôt Article ƒë∆°n
                    if isinstance(obj, list):
                        if not obj:
                            return False
                        obj = obj[0]
                    try:
                        t = getattr(obj, "tieu_de", None) or getattr(
                            obj, "Ti√™u ƒë·ªÅ", None
                        )
                        d = getattr(obj, "ngay_phat_hanh", None) or getattr(
                            obj, "Ng√†y ph√°t h√†nh", None
                        )
                        l = getattr(obj, "link_bai_bao", None) or getattr(
                            obj, "Link", None
                        )
                        return bool(t and d and l)
                    except Exception:
                        return False

                # üî• FAST-PATH: n·∫øu c√≥ ng√†y ISO + c√≥ ti√™u ƒë·ªÅ + HTML ƒë·ªß d√†i ‚Üí b·ªè qua LLM
                if meta.get("published_iso") and title_rb and len(art_html) >= 1500:
                    quick_json = json.dumps(
                        {
                            "Ti√™u ƒë·ªÅ": title_rb,
                            "Ng√†y ph√°t h√†nh": meta["published_iso"],
                            "Ngu·ªìn tr√≠ch ng√†y": meta.get("source_published_text") or "",
                            "T√≥m t·∫Øt": _quick_summary_from_html(art_html),
                            "Link": link,
                        },
                        ensure_ascii=False,
                    )
                    try:
                        parsed = self.parser.parse(
                            quick_json, media_source, industry_name
                        )
                    except Exception as e:
                        logger.warning(f"Fast-path parse l·ªói ‚Üí fallback LLM. Err: {e}")
                        parsed = None

                # N·∫øu fast-path kh√¥ng h·ª£p l·ªá ‚Üí fallback LLM
                if not _is_valid_article(parsed):
                    if meta.get("published_iso"):
                        prompt = self.build_prompt_with_known_date(
                            link=link,
                            known_date_iso=meta["published_iso"],
                            known_date_source=meta.get("source_published_text") or "",
                            known_title=title_rb,
                        )
                    else:
                        # fallback: ch∆∞a ch·∫Øc ng√†y ‚Üí d√πng prompt ƒë·∫ßy ƒë·ªß
                        prompt = self.build_prompt(link, start_date, end_date)

                    # 3) G·ªçi agent
                    resp = await self._arun_with_user_fallback(
                        prompt,
                        session_id=self.session_id,
                        preferred_provider=getattr(self.config, "provider", None),
                        preferred_model=getattr(self.config, "model", None),
                    )
                    text = await _get_response_text(resp)
                    logger.info(f"... HubAgent response for {link}:\n{text[:2000]}")
                    parsed = self.parser.parse(text, media_source, industry_name)

                return parsed

            tasks = []
            for link in valid_links[:8]:

                async def worker(l=link):
                    async with sem:
                        return await _retry_process_link(process_link, l, max_retries=2)

                tasks.append(asyncio.create_task(worker()))

            parsed_lists = await asyncio.gather(*tasks)
            all_articles = [a for lst in parsed_lists for a in lst]

            duration = time.monotonic() - start_t

            def to_date(x):
                if isinstance(x, datetime):
                    return x.date()
                if isinstance(x, date):
                    return x
                if isinstance(x, str):
                    # ∆Øu ti√™n ISO
                    try:
                        from datetime import datetime as _dt

                        return _dt.fromisoformat(x).date()
                    except Exception:
                        m = _VN_DATE_RE.search(x)
                        if m:
                            d, mo, y = (
                                int(m.group("d")),
                                int(m.group("m")),
                                int(m.group("y")),
                            )
                            from datetime import date as _date

                            return _date(y, mo, d)
                return None

            def in_range(a: Article):
                pub = to_date(a.ngay_phat_hanh)
                return pub is not None and start_date.date() <= pub <= end_date.date()

            filtered = [a for a in all_articles if in_range(a)]

            return CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=hub_url,
                articles_found=filtered,
                crawl_status="success" if filtered else "failed",
                error_message="" if filtered else "Kh√¥ng c√≥ b√†i h·ª£p l·ªá trong hub",
                crawl_duration=duration,
            )

        except Exception as e:
            logger.error(f"[{media_source.name}] ‚ùå L·ªói crawl hub: {e}", exc_info=True)
            return CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=hub_url if "hub_url" in locals() else "",
                articles_found=[],
                crawl_status="failed",
                error_message=str(e),
                crawl_duration=duration if "duration" in locals() else 0.0,
            )

    def build_prompt_with_known_date(
        self,
        link: str,
        known_date_iso: str,
        known_date_source: str = "",
        known_title: str | None = None,
    ) -> str:
        """
        Khi ƒë√£ r√∫t ƒë∆∞·ª£c ng√†y ƒëƒÉng (ISO) b·∫±ng rule-based, kho√° ng√†y ƒë√≥ l·∫°i ƒë·ªÉ LLM kh√¥ng ƒëo√°n sai.
        """
        return f"""
        Nhi·ªám v·ª•: Truy c·∫≠p URL: {link}, ƒë·ªçc b√†i v√† TR·∫¢ V·ªÄ JSON ƒë√∫ng schema b√™n d∆∞·ªõi.
        L∆∞u √Ω: Ng√†y ph√°t h√†nh ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh ch·∫Øc ch·∫Øn t·ª´ metadata: {known_date_iso}.
        B·∫°n KH√îNG ƒë∆∞·ª£c suy ƒëo√°n hay thay ƒë·ªïi ng√†y n√†y.

        {{
        "Ti√™u ƒë·ªÅ": "{(known_title or '').replace('"','').strip()}" if empty -> tr√≠ch t·ª´ b√†i,
        "Ng√†y ph√°t h√†nh": "{known_date_iso}",
        "Ngu·ªìn tr√≠ch ng√†y": "{known_date_source.replace('"','')[:160]}",
        "T√≥m t·∫Øt": "‚â§ 100 t·ª´, n√™u s·ª± ki·ªán ch√≠nh, c√°c b√™n li√™n quan, k·∫øt qu·∫£/t√°c ƒë·ªông, kh√¥ng l·∫∑p l·∫°i ti√™u ƒë·ªÅ",
        "Link": "{link}"
        }}
        """

    def build_prompt(
        self,
        link: str,
        start_date: datetime,
        end_date: datetime,
    ) -> str:
        date_filter = f"t·ª´ ng√†y {start_date.strftime('%Y-%m-%d')} ƒë·∫øn ng√†y {end_date.strftime('%Y-%m-%d')}"
        return f"""  
        Truy c·∫≠p URL: {link} v√† ph√¢n t√≠ch b√†i b√°o. TR·∫¢ V·ªÄ DUY NH·∫§T M·ªòT OBJECT JSON theo schema d∆∞·ªõi ƒë√¢y (kh√¥ng markdown, kh√¥ng gi·∫£i th√≠ch).

        {{
        "Ti√™u ƒë·ªÅ": "Ti√™u ƒë·ªÅ ƒë·∫ßy ƒë·ªß c·ªßa b√†i vi·∫øt",
        "Ng√†y ph√°t h√†nh": "DD-MM-YYYY",
        "Ngu·ªìn tr√≠ch ng√†y": "Chu·ªói ng√†y/gi·ªù ƒë√∫ng NGUY√äN VƒÇN b·∫°n th·∫•y (v√≠ d·ª•: '31/07/2025 11:00 (GMT+7)')",
        "Ng√†y c·∫≠p nh·∫≠t": "DD-MM-YYYY ho·∫∑c null n·∫øu kh√¥ng c√≥",
        "T√≥m t·∫Øt": "‚â§ 100 t·ª´, n√™u s·ª± ki·ªán ch√≠nh, c√°c b√™n li√™n quan, k·∫øt qu·∫£/t√°c ƒë·ªông",
        "Link": "{link}"
        }}

        QUY T·∫ÆC L·∫§Y NG√ÄY:
        1) ƒê∆Ø·ª¢C PH√âP d√πng metadata: JSON-LD Article.datePublished, meta[article:published_time], <time datetime>.
        2) KH√îNG d√πng ng√†y giao di·ªán (top bar, sidebar, footer).
        3) N·∫øu c√≥ nhi·ªÅu m·ªëc (ƒëƒÉng/c·∫≠p nh·∫≠t), ∆∞u ti√™n NG√ÄY ƒêƒÇNG G·ªêC (published). Ch·ªâ ƒëi·ªÅn "Ng√†y c·∫≠p nh·∫≠t" n·∫øu t√¨m th·∫•y m·ªëc c·∫≠p nh·∫≠t.
        4) Ch·ªâ nh·∫≠n b√†i trong kho·∫£ng {date_filter}. N·∫øu ng√†y ph√°t h√†nh ngo√†i kho·∫£ng, tr·∫£ JSON nh∆∞ng ng√†y ph√°t h√†nh ph·∫£i l√† ng√†y ƒë√∫ng b·∫°n t√¨m th·∫•y (ƒë·ª´ng t·ª± ƒë·ªïi).
        5) ƒê·ªãnh d·∫°ng ng√†y b·∫Øt bu·ªôc: YYYY-MM-DD.
        """


# <--- Agent Class -->
class CrawlerAgent(LLMUserFallbackMixin):
    """Agent chuy√™n crawl web, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát."""

    def __init__(
        self,
        model: Any,
        config: CrawlConfig,
        parser: ArticleParser,
        session_id: Optional[str] = None,
        check_cancelled: Optional[callable] = None,
        check_paused: Optional[callable] = None,
        check_pause_or_cancel: Optional[callable] = None,
        user_email: Optional[str] = None,
        status: Optional[BotStatus] = None,
        on_progress_update: Optional[callable] = None,
    ):
        self.parser = parser
        self.config = config
        self.session_id = session_id
        self.search_tools = [
            ddgs_search_text,
            # ArxivTools(),
            # BaiduSearchTools(),
            # HackerNewsTools(),
            # PubmedTools(),
            # WikipediaTools(),
            # GoogleSearchWithDelay(
            #     fixed_language="vi", timeout=60, fixed_max_results=50
            # ),
        ]
        self.search_tool_index = 0
        self.model = model
        self.agent = None
        self.cache_manager = SafeCacheManager(
            cache_dir="cache/crawl_results",
            ttl_hours=self.config.cache_duration_hours,
            version="1.1",
        )
        self.check_cancelled = check_cancelled or (lambda: False)
        self.check_paused = check_paused or (lambda: False)
        self.check_pause_or_cancel = check_pause_or_cancel
        self.user_email = user_email
        self.rotate_index = 0
        self.status = status
        self.on_progress_update = on_progress_update
        self.hub_tool = HubCrawlTool(
            model=self.model,
            config=self.config,
            session_id=self.session_id,
            parser=self.parser,
            check_pause_or_cancel=self.check_pause_or_cancel,
        )

    def return_partial_result(self, media_source: MediaSource) -> CrawlResult:
        unique_articles = {
            article.link_bai_bao: article
            for article in getattr(self, "current_articles", [])
        }
        articles = list(unique_articles.values())

        return CrawlResult(
            source_name=media_source.name,
            source_type=media_source.type,
            url=media_source.domain,
            articles_found=articles,
            crawl_status="partial" if articles else "timeout",
            error_message="Tr·∫£ v·ªÅ c√°c b√†i ƒë√£ crawl ƒë∆∞·ª£c tr∆∞·ªõc khi timeout.",
            crawl_duration=self.config.crawl_timeout,
        )

    def _create_agent(self):
        if self.agent:
            del self.agent
        current_tool = self.search_tools[self.search_tool_index]

        self.agent = Agent(
            name="MediaCrawler",
            role="Web Crawler v√† Content Extractor",
            model=self.model,
            tools=[Crawl4aiTools(max_length=2000), current_tool],
            instructions=[
                "B·∫°n l√† m·ªôt chuy√™n gia crawl web ƒë·ªÉ theo d√µi truy·ªÅn th√¥ng t·∫°i Vi·ªát Nam.",
                "Nhi·ªám v·ª•: Crawl c√°c website b√°o ch√≠ ƒë·ªÉ t√¨m b√†i vi·∫øt v·ªÅ c√°c ƒë·ªëi th·ªß c·∫°nh tranh d·ª±a tr√™n keywords, nh√£n h√†ng v√† ng√†nh h√†ng.",
                "∆Øu ti√™n tin t·ª©c m·ªõi nh·∫•t trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.",
                "Ch·ªâ l·∫•y c√°c b√†i vi·∫øt ƒë∆∞·ª£c ƒëƒÉng trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c y√™u c·∫ßu.",
                "N·∫øu kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ b√†i vi·∫øt n√†o, KH√îNG t·ª± t·∫°o n·ªôi dung, KH√îNG tr·∫£ v·ªÅ k·∫øt qu·∫£ gi·∫£, v√† ƒë·ªÉ ph·∫£n h·ªìi tr·ªëng.",
                "Kh√¥ng l·∫•y c√°c b√†i vi·∫øt ƒëƒÉng tr∆∞·ªõc ho·∫∑c sau kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh.",
                "Tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON h·ª£p l·ªá ch·ª©a danh s√°ch b√†i b√°o v·ªõi c√°c tr∆∞·ªùng: ti√™u ƒë·ªÅ, ng√†y ph√°t h√†nh (DD-MM-YYYY), t√≥m t·∫Øt n·ªôi dung, link b√†i b√°o.",
            ],
            show_tool_calls=True,
            markdown=True,
            add_datetime_to_instructions=True,
        )

    # def _rotate_tool(self):
    #     self.search_tool_index = (self.search_tool_index + 1) % len(self.search_tools)
    #     self._create_agent()

    @retry(
        stop=stop_after_attempt(2),  # thay v√¨ 3-5
        wait=wait_exponential(multiplier=0.5, min=0.5, max=2),
        reraise=True,
    )
    async def crawl_media_source(
        self,
        media_source: MediaSource,
        industry_name: str,
        keywords: List[str],
        start_date: datetime,
        end_date: datetime,
    ) -> CrawlResult:
        start_time = datetime.now()
        date_filter = f"t·ª´ ng√†y {start_date.strftime('%Y-%m-%d')} ƒë·∫øn ng√†y {end_date.strftime('%Y-%m-%d')}"
        domain_url = media_source.domain
        if domain_url and not domain_url.startswith("http"):
            domain_url = f"https://{domain_url}"

        def to_date(dt):
            return dt.date() if isinstance(dt, datetime) else dt

        keyword_groups = [[kw] for kw in keywords]
        articles: List[Article] = []
        self.current_articles = []

        # Kh√¥i ph·ª•c checkpoint n·∫øu c√≥
        task = next(
            (
                t
                for t in task_manager.get_tasks(self.user_email)
                if t["session_id"] == self.session_id
            ),
            None,
        )
        checkpoint = (
            task.get("crawl_checkpoint", {}).get(media_source.name, {}) if task else {}
        )
        tools_to_try = checkpoint.get("tool_order")
        if tools_to_try is None:
            base_order = list(range(len(self.search_tools)))
            start_tool_index = self.rotate_index
            self.rotate_index = (self.rotate_index + 1) % len(self.search_tools)
            tools_to_try = base_order[start_tool_index:] + base_order[:start_tool_index]

        start_group_index = checkpoint.get("group_index", 0)
        start_tool_index = checkpoint.get("tool_index", 0)

        try:
            # L·∫∂P T·ª™NG KEYWORD
            for g_idx in range(start_group_index, len(keyword_groups)):
                await _maybe_await(self.check_pause_or_cancel)
                group = keyword_groups[g_idx]  # v√≠ d·ª• ["T∆∞·ªùng An"]
                tool_index0 = start_tool_index if g_idx == start_group_index else 0
                kw_str = ", ".join(group)

                # progress text r√µ keyword
                if self.on_progress_update:
                    self.on_progress_update(
                        source_name=media_source.name,
                        completed=self.status.completed_sources,
                        failed=self.status.failed_sources,
                        progress=(
                            (self.status.completed_sources + self.status.failed_sources)
                            / max(1, self.status.total_sources)
                        )
                        * 50.0,
                        current_task=f"Crawling {media_source.name} ({industry_name}) ‚Äì keyword: {kw_str}",
                    )
                found_for_this_keyword = False

                if self.config.use_hub_page:
                    try:
                        result = await self.hub_tool.run(
                            media_source, group, start_date, end_date, industry_name
                        )
                        if result and result.articles_found:
                            # 1) Gi·ªØ batch hi·ªán t·∫°i ƒë·ªÉ UI hi·ªÉn th·ªã ngay
                            self.current_articles = list(
                                result.articles_found
                            )  # shallow copy

                            # c·ªông v√†o kho ƒëang t√≠ch l≈©y ƒë·ªÉ tr·∫£ v·ªÅ:
                            articles.extend(result.articles_found)

                            # ƒê√°nh d·∫•u ƒë√£ t√¨m th·∫•y ·ªü hub -> KH√îNG ch·∫°y fallback cho keyword n√†y
                            found_for_this_keyword = True

                            # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô (tu·ª≥ UI c·ªßa b·∫°n)
                            if self.on_progress_update:
                                self.on_progress_update(
                                    source_name=media_source.name,
                                    completed=self.status.completed_sources,
                                    failed=self.status.failed_sources,
                                    progress=(
                                        (
                                            self.status.completed_sources
                                            + self.status.failed_sources
                                        )
                                        / self.status.total_sources
                                    )
                                    * 50.0,
                                    current_task=(
                                        f"ƒê√£ t√¨m {len(self.current_articles)} b√†i t·ª´ hub "
                                        f"({industry_name}) ‚Äì chuy·ªÉn sang t·ª´ kh√≥a ti·∫øp theo"
                                    ),
                                )

                            # Chuy·ªÉn sang t·ª´ kh√≥a k·∫ø ti·∫øp (b·ªè nh√°nh fallback cho keyword n√†y)
                            continue

                    except Exception as e:
                        # Kh√¥ng ƒë·ªÉ l·ªói hub ch·∫∑n lu·ªìng; fallback s·∫Ω x·ª≠ l√Ω ti·∫øp
                        logger.exception(f"[{media_source.name}] Hub crawl error: {e}")

                # Use cache if true
                # if self.config.use_cache:
                #     cache_key = self.cache_manager.make_cache_key(
                #         media_source.name,
                #         industry_name,
                #         keywords,
                #         start_date,
                #         end_date,
                #     )
                #     cached_data = self.cache_manager.load_cache(cache_key)

                #     if cached_data:
                #         logger.info(f"[{media_source.name}] ‚úÖ Loaded from cache.")
                #         return CrawlResult(**cached_data)

                # max_keywords_per_query = 3
                # keyword_groups = [
                #     keywords[i : i + max_keywords_per_query]
                #     for i in range(0, len(keywords), max_keywords_per_query)
                # ]

                if not found_for_this_keyword:
                    for i in range(tool_index0, len(tools_to_try)):
                        await _maybe_await(self.check_pause_or_cancel)
                        tool_index = tools_to_try[i]
                        tool = self.search_tools[tool_index]
                        tool_name = getattr(tool, "__name__", tool.__class__.__name__)

                        self.search_tool_index = tool_index
                        self._create_agent()

                        new_articles_this_tool = 0
                        self.current_articles = []

                        group_index = g_idx
                        group = keyword_groups[group_index]
                        keywords_str = ", ".join(group)

                        # Update progress
                        if self.on_progress_update:
                            flat_keywords = keywords_str
                            self.on_progress_update(
                                source_name=media_source.name,
                                completed=self.status.completed_sources,
                                failed=self.status.failed_sources,
                                progress=(
                                    (
                                        self.status.completed_sources
                                        + self.status.failed_sources
                                    )
                                    / self.status.total_sources
                                )
                                * 50.0,
                                current_task=f"Crawling {media_source.name} ({industry_name}) ‚Äì t·ª´ kh√≥a: {flat_keywords}",
                            )

                            query_variants = [
                                # f"C√¥ng ty {industry_name} {keywords_str} site:{media_source.domain} th√°ng {start_date.month} {start_date.year}",
                                f"{keywords_str} {industry_name} tin t·ª©c m·ªõi nh·∫•t th√°ng {start_date.month} {start_date.year} site:{media_source.domain}",
                                f"site:{media_source.domain} {industry_name} {keywords_str} th√°ng {start_date.month} {start_date.year}",
                            ]
                            query_lines = "\n".join([f"- {q}" for q in query_variants])

                            crawl_query = f"""
                            Crawl website: {domain_url or media_source.name}
                            T√¨m c√°c b√†i b√°o tr√™n {domain_url or media_source.name} c√≥ ch·ª©a c√°c t·ª´ kh√≥a: {keywords_str} trong ti√™u ƒë·ªÅ ho·∫∑c trong b√†i vi·∫øt v√† PH·∫¢I li√™n quan ƒë·∫øn ng√†nh h√†ng: {industry_name}
                            Th·ªùi gian: {date_filter}
                            B·∫°n n√™n th·ª≠ t·∫•t c·∫£ c√°c c√¢u truy v·∫•n sau:
                            {query_lines}
                            Y√™u c·∫ßu:
                            - Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ, t√≥m t·∫Øt, ng√†y ph√°t h√†nh, link g·ªëc.
                            - Ng√†y ph√°t h√†nh (ngay_phat_hanh) ph·∫£i l√† ng√†y ƒë∆∞·ª£c ghi trong n·ªôi dung b√†i vi·∫øt.
                            - Tuy·ªát ƒë·ªëi KH√îNG ƒë∆∞·ª£c l·∫•y c√°c ng√†y n·∫±m ·ªü ph·∫ßn **header**, **menu**, **sidebar**, hay **g√≥c tr√™n c√πng c·ªßa trang** (v√¨ ƒë√≥ l√† ng√†y hi·ªán t·∫°i hi·ªÉn th·ªã giao di·ªán, KH√îNG ph·∫£i ng√†y ƒëƒÉng b√†i vi·∫øt).
                            - Ng√†y ph√°t h√†nh ph·∫£i:
                                + Xu·∫•t hi·ªán b√™n trong n·ªôi dung b√†i vi·∫øt.
                                + C√≥ th·ªÉ n·∫±m d∆∞·ªõi ti√™u ƒë·ªÅ, g·∫ßn t√™n t√°c gi·∫£, ho·∫∑c cu·ªëi b√†i vi·∫øt.
                                + N·∫øu c√≥ nhi·ªÅu ng√†y trong n·ªôi dung, b·∫°n PH·∫¢I ch·ªçn ng√†y:
                                    - C√≥ ƒë·ªãnh d·∫°ng h·ª£p l·ªá (v√≠ d·ª•: 31/07/2025, 2025-07-31, ho·∫∑c c√≥ th√™m gi·ªù)
                                    - Kh√°c v·ªõi ng√†y ƒë·∫ßu trang
                                    - L√† ng√†y nh·ªè h∆°n (s·ªõm h∆°n)
                            - C√°c v√≠ d·ª•:
                                ‚úÖ ƒê√∫ng: "31/07/2025 17:25" n·∫±m g·∫ßn t√°c gi·∫£ ho·∫∑c cu·ªëi b√†i vi·∫øt.
                                ‚ùå Sai: "Th·ª© Ba, ng√†y 05/08/2025" n·∫±m ·ªü ƒë·∫ßu trang, trong header.
                            - Ch·ªâ l·∫•y b√†i vi·∫øt li√™n quan ƒë·∫øn ng√†nh h√†ng v√† t·ª´ kh√≥a.
                            - QUY T·∫ÆC NO-RESULT:
                                + N·∫øu danh s√°ch b√†i === r·ªóng ‚Üí tr·∫£ v·ªÅ JSON m·∫£ng r·ªóng: []
                                + N·∫øu b√†i n√†o c√≥ "text" < 500 k√Ω t·ª± ‚Üí b·ªè b√†i ƒë√≥ (kh√¥ng t√≥m t·∫Øt)
                                + Tuy·ªát ƒë·ªëi KH√îNG t·∫°o b√†i khi kh√¥ng c√≥ d·ªØ li·ªáu.
                            - T·∫°o t√≥m t·∫Øt chi ti·∫øt (d∆∞·ªõi 100 t·ª´), n√™u b·∫≠t c√°c th√¥ng tin ch√≠nh nh∆∞: s·ª± ki·ªán ch√≠nh, c√°c b√™n li√™n quan, v√† k·∫øt qu·∫£ ho·∫∑c t√°c ƒë·ªông c·ªßa s·ª± ki·ªán. Kh√¥ng ch·ªâ l·∫∑p l·∫°i ti√™u ƒë·ªÅ.
                            - Format: Ti√™u ƒë·ªÅ | Ng√†y ph√°t h√†nh | T√≥m t·∫Øt | Link
                            """
                            logger.info(
                                f"[{media_source.name}] Using {tool_name} for group {group_index + 1}/{len(keyword_groups)}"
                            )

                            try:
                                logger.info(
                                    f"[{media_source.name}] Searching with keywords: {keywords_str}"
                                )
                                response = await self._arun_with_user_fallback(
                                    crawl_query,
                                    session_id=self.session_id,
                                    preferred_provider=getattr(
                                        self.config, "provider", None
                                    ),
                                    preferred_model=getattr(self.config, "model", None),
                                )
                                logger.info(
                                    f"[DEBUG] Raw response content:\n{response.content}"
                                )
                                await asyncio.sleep(1)

                                if response and response.content:
                                    if (
                                        "enable javascript" in response.content.lower()
                                        or "captcha" in response.content.lower()
                                    ):
                                        logger.warning(
                                            f"[{media_source.name}] Blocked or captcha required for {tool_name}"
                                        )
                                        continue

                                    parsed_articles = self.parser.parse(
                                        response.content, media_source, industry_name
                                    )

                                    filtered_articles = [
                                        a
                                        for a in parsed_articles
                                        if not any(
                                            exclude in a.link_bai_bao
                                            for exclude in self.config.exclude_domains
                                        )
                                    ]

                                    valid_new_articles = []
                                    seen_links = set()
                                    for a in filtered_articles:
                                        if (
                                            not a
                                            or not a.link_bai_bao
                                            or not a.ngay_phat_hanh
                                        ):
                                            continue

                                        # Ki·ªÉm tra ng√†y
                                        pub_date = to_date(a.ngay_phat_hanh)
                                        if not (
                                            to_date(start_date)
                                            <= pub_date
                                            <= to_date(end_date)
                                        ):
                                            continue

                                        # Ki·ªÉm tra tr√πng link (sau khi ch·∫Øc ch·∫Øn l√† b√†i h·ª£p l·ªá)
                                        if a.link_bai_bao in seen_links:
                                            continue
                                        norm = await validate_and_normalize_link(
                                            a.link_bai_bao, media_source.domain
                                        )
                                        if not norm:
                                            # log l√Ω do lo·∫°i n·∫øu mu·ªën
                                            continue

                                        if norm in seen_links:
                                            continue
                                        a.link_bai_bao = norm
                                        seen_links.add(a.link_bai_bao)

                                        valid_new_articles.append(a)

                                    if valid_new_articles:
                                        articles.extend(valid_new_articles)
                                        self.current_articles.extend(valid_new_articles)
                                        new_articles_this_tool += len(
                                            valid_new_articles
                                        )

                                        logger.info(
                                            f"[{media_source.name}] Found {len(valid_new_articles)} articles with {tool_name} for keywords: {keywords_str}"
                                        )
                                        logger.debug(
                                            f"[{media_source.name}] Total articles so far: {len(articles)}"
                                        )
                                    else:
                                        logger.warning(
                                            f"[{media_source.name}] No articles parsed from {tool_name} for keywords: {keywords_str}"
                                        )
                                else:
                                    logger.warning(
                                        f"[{media_source.name}] No response from {tool_name} for keywords: {keywords_str}"
                                    )

                            except (
                                httpx.HTTPStatusError,
                                httpx.RequestError,
                                APITimeoutError,
                                ValueError,
                            ) as e:
                                logger.warning(
                                    f"[{media_source.name}] Error in {tool_name} for keywords {keywords_str}: {e}"
                                )
                                await asyncio.sleep(1)
                            except Exception as e:
                                logger.error(
                                    f"[{media_source.name}] Unexpected error in {tool_name} for keywords {keywords_str}: {e}",
                                    exc_info=True,
                                )
                                await asyncio.sleep(1)
                            gc.collect()

                        if new_articles_this_tool > 0:
                            logger.info(
                                f"[{media_source.name}] Stopping search as articles were found by {tool_name}"
                            )
                            break

                    unique_articles = {
                        article.link_bai_bao: article
                        for article in articles
                        if article.link_bai_bao
                    }
                    articles = list(unique_articles.values())
                    logger.info(
                        f"[{media_source.name}] Crawled {len(articles)} unique articles"
                    )

            result = CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=media_source.domain,
                articles_found=articles,
                crawl_status="success" if articles else "failed",
                error_message=(
                    ""
                    if articles
                    else f"Th·ª≠ h·∫øt {len(self.search_tools)} search tool nh∆∞ng kh√¥ng t√¨m th·∫•y b√†i b√°o"
                ),
                crawl_duration=(datetime.now() - start_time).total_seconds(),
            )
            return result

            # if (
            #     self.config.use_cache
            #     and result.crawl_status == "success"
            #     and len(result.articles_found) > 0
            # ):
            #     self.cache_manager.save_cache(cache_key, result)

        except Exception as e:
            logger.error(f"[{media_source.name}] Crawl failed: {e}", exc_info=True)
            return CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=media_source.domain,
                articles_found=[],
                crawl_status="failed",
                error_message=f"Crawl failed: {str(e)}",
                crawl_duration=(datetime.now() - start_time).total_seconds(),
            )
        finally:
            gc.collect()

    def close_final(self):
        logger.info("üßπ ƒê√≥ng ho√†n to√†n CrawlerAgent, gi·∫£i ph√≥ng agent v√† tools.")
        self.agent = None
        for tool in self.search_tools:
            if hasattr(tool, "close"):
                try:
                    tool.close()
                except Exception as e:
                    logger.warning(f"Tool {tool} ƒë√≥ng kh√¥ng th√†nh c√¥ng: {e}")
        gc.collect()


class ProcessorAgent(LLMUserFallbackMixin):
    """Agent chuy√™n x·ª≠ l√Ω v√† ph√¢n t√≠ch n·ªôi dung, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát chi ti·∫øt."""

    def __init__(
        self,
        model: Any,
        config: CrawlConfig,
        session_id: Optional[str] = None,
    ):
        self.agent = None
        self.session_id = session_id
        self.config = config
        self.model = model

    def _create_agent(self):
        self.agent = Agent(
            name="ContentProcessor",
            role="Chuy√™n gia Ph√¢n t√≠ch v√† Ph√¢n lo·∫°i N·ªôi dung",
            model=self.model,
            instructions=[
                "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch n·ªôi dung truy·ªÅn th√¥ng cho ng√†nh FMCG t·∫°i Vi·ªát Nam.",
                "Nhi·ªám v·ª• c·ªßa b·∫°n: Ph√¢n t√≠ch v√† ph√¢n lo·∫°i c√°c b√†i b√°o theo ng√†nh h√†ng v√† nh√£n hi·ªáu.",
                "B·∫°n B·∫ÆT BU·ªòC ph·∫£i tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch JSON (JSON list) h·ª£p l·ªá c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng Article ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß.",
            ],
            markdown=True,
        )

    def extract_json(self, text: str) -> str:
        """
        Tr√≠ch xu·∫•t JSON t·ª´ ph·∫£n h·ªìi LLM v·ªõi th·ª© t·ª± ∆∞u ti√™n:
        1. T√¨m ƒëo·∫°n trong ```json ... ```
        2. N·∫øu kh√¥ng c√≥, b·ªè d·∫•u '...' ngo√†i c√πng (n·∫øu c√≥)
        3. T√¨m ƒëo·∫°n JSON {...} ho·∫∑c [...]
        """

        text = text.strip()

        # 1Ô∏è. ∆Øu ti√™n t√¨m ```json ... ```
        matches = re.findall(r"```json(.*?)```", text, re.DOTALL)
        if matches:
            for match in matches:
                match = match.strip()
                try:
                    json.loads(match)
                    return match  # ‚úÖ Tr·∫£ v·ªÅ JSON ƒë√∫ng lu√¥n
                except json.JSONDecodeError:
                    continue

        # 2. N·∫øu kh√¥ng c√≥, x·ª≠ l√Ω d·∫•u '...' ngo√†i c√πng
        if text.startswith("'") and text.endswith("'"):
            text = text[1:-1].strip()

        # 3Ô∏è. T√¨m ƒëo·∫°n JSON {...} ho·∫∑c [...]
        candidates = re.findall(r"(\{.*?\}|\[.*?\])", text, re.DOTALL)
        for candidate in candidates:
            candidate = candidate.strip()
            try:
                json.loads(candidate)
                return candidate  # ‚úÖ Tr·∫£ v·ªÅ JSON ƒë√∫ng
            except json.JSONDecodeError:
                continue

        # N·∫øu kh√¥ng t√¨m th·∫•y
        raise ValueError("Kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá trong ph·∫£n h·ªìi.")

    async def process_articles(
        self, raw_articles: List[Article], keywords_config: Dict[str, List[str]]
    ) -> List[Article]:
        """
        Processes a list of raw articles to classify industries, brands, content clusters,
        and extract summaries and keywords. Optimizes memory usage by processing articles in batches.

        Args:
            raw_articles: List of raw Article objects to process.
            keywords_config: Dictionary mapping industries to lists of keywords.

        Returns:
            List of processed Article objects.
        """
        if not raw_articles:
            logger.info("No articles to process.")
            return []

        processed_articles = []
        batch_size = 10  # Process 10 articles per batch to reduce memory usage

        try:
            # Extract brand list (uppercase keywords) for competitor identification
            brand_list = list(
                set(
                    kw
                    for kws in keywords_config.values()
                    for kw in kws
                    if kw[0].isupper()
                )
            )

            # Define content clusters with associated keywords
            content_clusters = {
                ContentCluster.HOAT_DONG_DOANH_NGHIEP: [
                    "s·∫£n xu·∫•t",
                    "nh√† m√°y",
                    "tuy·ªÉn d·ª•ng",
                    "doanh nghi·ªáp",
                    "ho·∫°t ƒë·ªông",
                    "ƒë·∫ßu t∆∞",
                ],
                ContentCluster.CHUONG_TRINH_CSR: [
                    "t√†i tr·ª£",
                    "m√¥i tr∆∞·ªùng",
                    "c·ªông ƒë·ªìng",
                    "CSR",
                    "t·ª´ thi·ªán",
                ],
                ContentCluster.MARKETING_CAMPAIGN: [
                    "truy·ªÅn th√¥ng",
                    "KOL",
                    "khuy·∫øn m√£i",
                    "qu·∫£ng c√°o",
                    "chi·∫øn d·ªãch",
                ],
                ContentCluster.PRODUCT_LAUNCH: [
                    "s·∫£n ph·∫©m m·ªõi",
                    "bao b√¨",
                    "c√¥ng th·ª©c",
                    "ra m·∫Øt",
                    "ph√°t h√†nh",
                ],
                ContentCluster.PARTNERSHIP: [
                    "MOU",
                    "li√™n doanh",
                    "k√Ω k·∫øt",
                    "h·ª£p t√°c",
                    "ƒë·ªëi t√°c",
                ],
                ContentCluster.FINANCIAL_REPORT: [
                    "l·ª£i nhu·∫≠n",
                    "doanh thu",
                    "tƒÉng tr∆∞·ªüng",
                    "b√°o c√°o t√†i ch√≠nh",
                    "k·∫øt qu·∫£ kinh doanh",
                ],
                ContentCluster.FOOD_SAFETY: [
                    "an to√†n th·ª±c ph·∫©m",
                    "ATTP",
                    "ng·ªô ƒë·ªôc",
                    "nhi·ªÖm khu·∫©n",
                    "thu h·ªìi s·∫£n ph·∫©m",
                    "ch·∫•t c·∫•m",
                    "ki·ªÉm tra ATTP",
                    "thanh tra an to√†n th·ª±c ph·∫©m",
                    "truy xu·∫•t ngu·ªìn g·ªëc",
                    "blockchain th·ª±c ph·∫©m",
                    "tem QR",
                    "chu·ªói cung ·ª©ng s·∫°ch",
                    "cam k·∫øt ch·∫•t l∆∞·ª£ng th·ª±c ph·∫©m",
                    "quy ƒë·ªãnh an to√†n th·ª±c ph·∫©m",
                    "x·ª≠ ph·∫°t vi ph·∫°m ATTP",
                    "s·ª©c kh·ªèe",
                    "thu h·ªìi s·∫£n ph·∫©m",
                ],
                ContentCluster.OTHER: [],
            }

            # Process articles in batches
            for i in range(0, len(raw_articles), batch_size):
                batch = raw_articles[i : i + batch_size]
                logger.info(
                    f"Processing batch {i // batch_size + 1} with {len(batch)} articles"
                )

                # Create analysis prompt for the batch
                analysis_prompt = f"""
                Ph√¢n t√≠ch v√† ph√¢n lo·∫°i {len(batch)} b√†i b√°o sau ƒë√¢y.

                Danh s√°ch c√°c nh√£n h√†ng ƒë·ªëi th·ªß c·∫ßn x√°c ƒë·ªãnh:
                {json.dumps(brand_list, ensure_ascii=False, indent=2)}
                
                Keywords config:
                {json.dumps(keywords_config, ensure_ascii=False, indent=2)}
                
                Raw articles:
                {json.dumps([a.model_dump(mode='json') for a in batch], ensure_ascii=False, indent=2)}
                
                Y√™u c·∫ßu:
                1. Ph√¢n lo·∫°i ch√≠nh x√°c ng√†nh h√†ng cho t·ª´ng b√†i (d·ª±a theo b·ªëi c·∫£nh b√†i v√† danh s√°ch nh√£n h√†ng ng√†nh h√†ng t∆∞∆°ng ·ª©ng).
                2. Tr√≠ch xu·∫•t `nhan_hang`:
                    - ƒê·ªçc n·ªôi dung b√†i vi·∫øt v√† ki·ªÉm tra xem c√≥ nh√£n h√†ng n√†o trong danh s√°ch sau xu·∫•t hi·ªán hay kh√¥ng: 
                    {json.dumps(brand_list, ensure_ascii=False, indent=2)}
                    - Ch·ªâ ghi nh·∫≠n nh·ªØng nh√£n h√†ng th·ª±c s·ª± xu·∫•t hi·ªán trong b√†i vi·∫øt (b·∫•t k·ªÉ vi·∫øt hoa hay vi·∫øt th∆∞·ªùng).
                    - N·∫øu kh√¥ng th·∫•y nh√£n h√†ng n√†o th√¨ ƒë·ªÉ `nhan_hang` l√† `[]`. Kh√¥ng t·ª± b·ªãa ho·∫∑c t·ª± suy ƒëo√°n th√™m.
                3. Ph√¢n lo·∫°i l·∫°i c·ª•m n·ªôi dung (`cum_noi_dung`). N·∫øu b√†i vi·∫øt c√≥ n·ªôi dung t∆∞∆°ng ƒë∆∞∆°ng, ƒë·ªìng nghƒ©a ho·∫∑c g·∫ßn gi·ªëng v·ªõi c√°c c·ª•m t·ª´ kh√≥a: {json.dumps({k.value: v for k, v in content_clusters.items()}, ensure_ascii=False, indent=2)}, h√£y ph√¢n lo·∫°i v√†o c·ª•m ƒë√≥.
                4. N·∫øu kh√¥ng t√¨m th·∫•y c·ª•m n·ªôi dung n√†o kh·ªõp v·ªõi danh s√°ch t·ª´ kh√≥a c·ª•m n·ªôi dung, B·∫ÆT BU·ªòC g√°n tr∆∞·ªùng (`cum_noi_dung`) l√† '{ContentCluster.OTHER.value}', KH√îNG ƒê∆Ø·ª¢C ƒë·ªÉ tr·ªëng ho·∫∑c tr·∫£ v·ªÅ none hay null.
                5. Vi·∫øt l·∫°i n·ªôi dung chi ti·∫øt, ng·∫Øn g·ªçn v√† mang t√≠nh m√¥ t·∫£ kh√°i qu√°t cho tr∆∞·ªùng `cum_noi_dung_chi_tiet` d·ª±a tr√™n n·ªôi dung ƒë√£ c√≥ s·∫µn:
                    - L√† 1 d√≤ng m√¥ t·∫£ ng·∫Øn (~10‚Äì20 t·ª´) cho b√†i b√°o, c√≥ c·∫•u tr√∫c:  
                    `[Lo·∫°i th√¥ng tin]: [T√≥m t·∫Øt n·ªôi dung n·ªïi b·∫≠t]`
                    - V√≠ d·ª•: "Th√¥ng tin doanh nghi·ªáp: T∆∞·ªùng An kh·∫≥ng ƒë·ªãnh v·ªã th·∫ø d·ªãp T·∫øt 2025"
                6. Tr√≠ch xu·∫•t v√† ghi v√†o `keywords_found`:
                    - L√† t·∫•t c·∫£ c√°c t·ª´ kh√≥a ng√†nh li√™n quan th·ª±c s·ª± xu·∫•t hi·ªán trong b√†i vi·∫øt.
                    - Ch·ªâ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ c√°c t·ª´ kh√≥a ƒë√£ cung c·∫•p trong `keywords_config`.
                    - N·∫øu kh√¥ng t√¨m th·∫•y t·ª´ kh√≥a n√†o, ƒë·ªÉ `keywords_found` l√† []
                7. Ch·ªâ gi·ªØ b√†i vi·∫øt li√™n quan ƒë·∫øn ng√†nh FMCG (D·∫ßu ƒÉn, Gia v·ªã, S·ªØa, v.v.) d·ª±a tr√™n t·ª´ kh√≥a trong `keywords_config`. Lo·∫°i b·ªè b√†i kh√¥ng li√™n quan (e.g., ch√≠nh tr·ªã, s·ª©c kh·ªèe kh√¥ng li√™n quan).
                8. ƒê·ªãnh d·∫°ng ng√†y ph√°t h√†nh b·∫Øt bu·ªôc: dd/mm/yyyy (VD: 01/07/2025)"
                9. N·∫øu m·ªôt b√†i b√°o ƒë·ªÅ c·∫≠p nhi·ªÅu nh√£n h√†ng th√¨ ghi t·∫•t c·∫£ nh√£n h√†ng trong danh s√°ch `nhan_hang`.
                10. N·∫øu b√†i li√™n quan nhi·ªÅu ng√†nh (v√≠ d·ª• s·∫£n ph·∫©m ƒëa d·ª•ng), h√£y ch·ªçn ng√†nh ch√≠nh nh·∫•t li√™n quan ƒë·∫øn b·ªëi c·∫£nh.
                11. Gi·ªØ nguy√™n `tom_tat_noi_dung`, kh√¥ng c·∫Øt b·ªõt, sinh ra hay thay ƒë·ªïi n·ªôi dung.

                ƒê·ªãnh d·∫°ng ƒë·∫ßu ra:
                Tr·∫£ v·ªÅ m·ªôt danh s√°ch JSON h·ª£p l·ªá ch·ª©a c√°c ƒë·ªëi t∆∞·ª£ng Article ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω. C·∫•u tr√∫c JSON c·ªßa m·ªói ƒë·ªëi t∆∞·ª£ng ph·∫£i kh·ªõp v·ªõi Pydantic model. ƒê√¢y l√† 1 v√≠ d·ª• cho b·∫°n l√†m m·∫´u:
                [
                    {{
                        "stt": 1,
                        "ngay_phat_hanh": "01/07/2025",
                        "dau_bao": "VNEXPRESS",
                        "cum_noi_dung": "Chi·∫øn d·ªãch Marketing",
                        "cum_noi_dung_chi_tiet": "Chi·∫øn d·ªãch T·∫øt 2025 c·ªßa Vinamilk chinh ph·ª•c ng∆∞·ªùi ti√™u d√πng tr·∫ª",
                        "tom_tat_noi_dung": "Vinamilk tung chi·∫øn d·ªãch T·∫øt 2025...",
                        "link_bai_bao": "https://vnexpress.net/...",
                        "nganh_hang": "S·ªØa (UHT)",
                        "nhan_hang": ["Vinamilk"],
                        "keywords_found": ["T·∫øt", "TV qu·∫£ng c√°o", "Vinamilk"]
                    }}
                ]
                [
                    {{
                        "stt": 2,
                        "ngay_phat_hanh": "06/07/2025",
                        "dau_bao": "Thanh Nien",
                        "cum_noi_dung": "Ho·∫°t ƒë·ªông doanh nghi·ªáp v√† th√¥ng tin s·∫£n ph·∫©m",
                        "cum_noi_dung_chi_tiet": "Th√¥ng tin doanh nghi·ªáp: T∆∞·ªùng An v√† Coba m·ªü r·ªông th·ªã ph·∫ßn d·∫ßu ƒÉn mi·ªÅn T√¢y",
                        "tom_tat_noi_dung": "T∆∞·ªùng An v√† Coba tƒÉng c∆∞·ªùng ƒë·∫ßu t∆∞ v√† ph√¢n ph·ªëi s·∫£n ph·∫©m d·∫ßu ƒÉn t·∫°i khu v·ª±c mi·ªÅn T√¢y Nam B·ªô.",
                        "link_bai_bao": "https://thanhnien.vn/tuong-an-coba-dau-an",
                        "nganh_hang": "D·∫ßu ƒÉn",
                        "nhan_hang": ["T∆∞·ªùng An", "Coba"],
                        "keywords_found": ["T∆∞·ªùng An", "Coba", "d·∫ßu ƒÉn", "th·ªã ph·∫ßn"]
                    }}
                ]
                [
                    {{
                        "stt": 3,
                        "ngay_phat_hanh": "07/07/2025",
                        "dau_bao": "Tuoi Tre",
                        "cum_noi_dung": "Ch∆∞∆°ng tr√¨nh CSR",
                        "cum_noi_dung_chi_tiet": "CSR: Doanh nghi·ªáp ƒë·ªãa ph∆∞∆°ng h·ªó tr·ª£ c·ªông ƒë·ªìng mi·ªÅn n√∫i",
                        "tom_tat_noi_dung": "M·ªôt s·ªë doanh nghi·ªáp ƒë·ªãa ph∆∞∆°ng ph·ªëi h·ª£p t·ªï ch·ª©c ch∆∞∆°ng tr√¨nh h·ªó tr·ª£ b√† con mi·ªÅn n√∫i m√πa m∆∞a l≈©.",
                        "link_bai_bao": "https://tuoitre.vn/csr-mien-nui",
                        "nganh_hang": "D·∫ßu ƒÉn",
                        "nhan_hang": [],
                        "keywords_found": ["h·ªó tr·ª£ c·ªông ƒë·ªìng", "CSR", "mi·ªÅn n√∫i"]
                    }}
                ]
                """

                try:
                    response = await self._arun_with_user_fallback(
                        analysis_prompt,
                        session_id=self.session_id,
                        preferred_provider=getattr(self.config, "provider", None),
                        preferred_model=getattr(self.config, "model", None),
                    )
                    logger.debug(f"LLM raw output:\n{response.content}")

                    if response and response.content:
                        try:
                            raw_json = self.extract_json(response.content)
                            articles_data = json.loads(raw_json)

                            km = KeywordManager(
                                CONFIG_DIR / "content_cluster_keywords.json"
                            )
                            valid_clusters = [c.value for c in ContentCluster]

                            for item in articles_data:
                                text_to_check = (
                                    item.get("tom_tat_noi_dung", "")
                                    + " "
                                    + (item.get("cum_noi_dung_chi_tiet", "") or "")
                                ).lower()

                                # Fallback c·ª•m n·ªôi dung n·∫øu c·∫ßn
                                if (
                                    item.get("cum_noi_dung")
                                    in [None, "null", "", "Kh√°c"]
                                    or item.get("cum_noi_dung") not in valid_clusters
                                ):
                                    text_to_check = (
                                        item.get("tom_tat_noi_dung", "")
                                        + " "
                                        + (item.get("cum_noi_dung_chi_tiet", "") or "")
                                    ).strip()
                                    fallback_cluster = km.map_to_cluster(text_to_check)
                                    item["cum_noi_dung"] = fallback_cluster

                                # Fallback keywords_found
                                if (
                                    "keywords_found" not in item
                                    or not item["keywords_found"]
                                ):
                                    item["keywords_found"] = []
                                    for industry, keywords in keywords_config.items():
                                        for kw in keywords:
                                            if kw.lower() in text_to_check:
                                                item["keywords_found"].append(kw)
                                    # Lo·∫°i tr√πng
                                    item["keywords_found"] = list(
                                        set(item["keywords_found"])
                                    )

                                # Fallback nh√£n h√†ng
                                if "nhan_hang" not in item or not item["nhan_hang"]:
                                    item["nhan_hang"] = []
                                    for brand in brand_list:
                                        if brand.lower() in text_to_check:
                                            item["nhan_hang"].append(brand)
                                    # Lo·∫°i tr√πng
                                    item["nhan_hang"] = list(set(item["nhan_hang"]))

                            processed_articles.extend(
                                [Article(**item) for item in articles_data]
                            )
                            logger.info(
                                f"Batch {i // batch_size + 1} processed: {len(articles_data)} articles"
                            )

                        except (json.JSONDecodeError, TypeError) as e:
                            logger.error(
                                f"Failed to parse JSON response for batch {i // batch_size + 1}: {e}"
                            )
                            # Fallback: g√°n c·ª•m n·ªôi dung b·∫±ng KeywordManager khi l·ªói
                            km = KeywordManager(
                                CONFIG_DIR / "content_cluster_keywords.json"
                            )
                            for article in batch:
                                text = (
                                    article.tom_tat_noi_dung
                                    + " "
                                    + (article.cum_noi_dung_chi_tiet or "")
                                )
                                article.cum_noi_dung = km.map_to_cluster(text)
                            processed_articles.extend(batch)

                    else:
                        logger.warning(
                            f"No valid response for batch {i // batch_size + 1}"
                        )
                        # Fallback t∆∞∆°ng t·ª± khi kh√¥ng c√≥ response
                        km = KeywordManager(
                            CONFIG_DIR / "content_cluster_keywords.json"
                        )
                        for article in batch:
                            text = (
                                article.tom_tat_noi_dung
                                + " "
                                + (article.cum_noi_dung_chi_tiet or "")
                            )
                            article.cum_noi_dung = km.map_to_cluster(text)
                        processed_articles.extend(batch)

                except Exception as e:
                    logger.error(
                        f"Error processing batch {i // batch_size + 1}: {e}",
                        exc_info=True,
                    )
                    processed_articles.extend(
                        batch
                    )  # Keep raw articles if processing fails

                # Free memory after each batch
                gc.collect()

            logger.info(
                f"Processing completed. Total processed articles: {len(processed_articles)}"
            )
            return processed_articles

        except Exception as e:
            logger.error(f"Article processing failed: {e}", exc_info=True)
            return raw_articles  # Return raw articles if the entire process fails
        finally:
            gc.collect()  # Final memory cleanup

    def close(self):
        del self.agent
        gc.collect()


class ReportAgent(LLMUserFallbackMixin):
    """Agent chuy√™n t·∫°o b√°o c√°o, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát."""

    def __init__(
        self, model: Any, config: CrawlConfig, session_id: Optional[str] = None
    ):
        self.agent = None
        self.model = model
        self.session_id = session_id
        self.config = config

    def _create_agent(self):
        schema = CompetitorReport.model_json_schema()
        schema_text = json.dumps(schema, ensure_ascii=False)
        self.agent = Agent(
            name="ReportGenerator",
            role="Chuy√™n gia T·∫°o B√°o c√°o v√† Ph√¢n t√≠ch D·ªØ li·ªáu",
            model=self.model,
            instructions=[
                "B·∫°n l√† chuy√™n gia t·∫°o b√°o c√°o ph√¢n t√≠ch truy·ªÅn th√¥ng cho ng√†nh FMCG.",
                "Nhi·ªám v·ª•: T·∫°o m·ªôt b√°o c√°o ph√¢n t√≠ch ƒë·ªëi th·ªß c·∫°nh tranh t·ª´ d·ªØ li·ªáu c√°c b√†i b√°o ƒë√£ ƒë∆∞·ª£c cung c·∫•p.",
                "B·∫°n B·∫ÆT BU·ªòC ph·∫£i tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON (JSON object) duy nh·∫•t, h·ª£p l·ªá v√† tu√¢n th·ªß nghi√™m ng·∫∑t theo c·∫•u tr√∫c c·ªßa JSON Schema b√™n d∆∞·ªõi",
                "N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ho·∫∑c c√≥ l·ªói, tr·∫£ v·ªÅ CompetitorReport r·ªóng h·ª£p l·ªá v·ªõi c√°c tr∆∞·ªùng l√† [] ho·∫∑c 0.",
                f"JSON_SCHEMA:\n{schema_text}\n",
            ],
            markdown=False,
        )

    def extract_json(self, text: str) -> str:
        text = text.strip()

        # ∆Øu ti√™n t√¨m ƒëo·∫°n ```json ... ```
        matches = re.findall(r"```json(.*?)```", text, re.DOTALL)
        if matches:
            for match in matches:
                match = match.strip()
                try:
                    obj = json.loads(match)
                    if isinstance(obj, dict):
                        return match
                except:
                    continue

        # N·∫øu kh√¥ng c√≥, t√¨m JSON object ngo√†i c√πng
        try:
            # Th·ª≠ parse to√†n b·ªô text lu√¥n n·∫øu l√† dict
            obj = json.loads(text)
            if isinstance(obj, dict):
                return text
        except:
            pass

        # Fallback t√¨m c√°c c·∫∑p { } l·ªõn nh·∫•t
        dict_candidates = re.findall(r"(\{.*\})", text, re.DOTALL)
        for candidate in dict_candidates:
            try:
                obj = json.loads(candidate.strip())
                if isinstance(obj, dict):
                    return candidate.strip()
            except:
                continue

        raise ValueError("Kh√¥ng t√¨m th·∫•y JSON dict h·ª£p l·ªá trong ph·∫£n h·ªìi.")

    def _sanitize_report_payload(self, d: dict, articles, date_range: str) -> dict:
        d = dict(d or {})

        # ---- overall_summary ----
        osum = dict(d.get("overall_summary") or {})
        industries = osum.get("industries")

        # 1) industries ph·∫£i l√† list
        if isinstance(industries, dict):
            industries = [industries]
        elif not isinstance(industries, list):
            industries = []

        fixed_industries = []
        for it in industries:
            if not isinstance(it, dict):
                continue
            it = dict(it)

            # 2) Chu·∫©n h√≥a/ƒë·ªïi t√™n kh√≥a v·ªÅ ƒë√∫ng schema
            if "nganh_hang" not in it:
                it["nganh_hang"] = it.pop(
                    "industry", it.pop("nganh", it.pop("sector", "Kh√°c"))
                )
            if "nhan_hang" not in it:
                it["nhan_hang"] = it.pop("brands", it.pop("brand", []))
            if "cum_noi_dung" not in it:
                it["cum_noi_dung"] = it.pop("clusters", it.pop("topics", []))
            if "so_luong_bai" not in it:
                it["so_luong_bai"] = it.pop("count", it.pop("total", 0))
            if "cac_dau_bao" not in it:
                it["cac_dau_bao"] = it.pop("sources", [])

            # 3) ƒê·∫£m b·∫£o ki·ªÉu d·ªØ li·ªáu t·ªëi thi·ªÉu
            if not isinstance(it["nhan_hang"], list):
                it["nhan_hang"] = [it["nhan_hang"]]
            if not isinstance(it["cum_noi_dung"], list):
                it["cum_noi_dung"] = [it["cum_noi_dung"]]
            if not isinstance(it["cac_dau_bao"], list):
                it["cac_dau_bao"] = [it["cac_dau_bao"]]
            if not isinstance(it["so_luong_bai"], int):
                try:
                    it["so_luong_bai"] = int(it["so_luong_bai"])
                except Exception:
                    it["so_luong_bai"] = 0

            fixed_industries.append(it)

        osum["industries"] = fixed_industries
        osum.setdefault("thoi_gian_trich_xuat", date_range)
        osum.setdefault("tong_so_bai", len(articles))
        d["overall_summary"] = osum

        # ---- industry_summaries ----
        iss = d.get("industry_summaries")
        if not isinstance(iss, list) or not all(isinstance(x, dict) for x in iss):
            # n·∫øu LLM kh√¥ng tr·∫£, d√πng l·∫°i danh s√°ch ƒë√£ fix t·ª´ overall_summary
            iss = fixed_industries
        else:
            fixed_iss = []
            for it in iss:
                it = dict(it)
                it.setdefault("nganh_hang", osum.get("nganh_hang", "Kh√°c"))
                it.setdefault("nhan_hang", [])
                it.setdefault("cum_noi_dung", [])
                it.setdefault("so_luong_bai", 0)
                it.setdefault("cac_dau_bao", [])
                fixed_iss.append(it)
            iss = fixed_iss
        d["industry_summaries"] = iss

        # ---- c√°c tr∆∞·ªùng g·ªëc kh√°c ----
        d.setdefault("total_articles", len(articles))
        d.setdefault("date_range", date_range)

        return d

    async def generate_report(
        self, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        """
        Generates a competitor analysis report from a list of articles for a given date range.
        """

        if not articles:
            logger.info(
                "No articles provided for report generation. Returning basic report."
            )
            return self._create_basic_report([], date_range)

        try:
            logger.info(f"Generating full report for {len(articles)} articles...")

            report_prompt = f"""
            T·∫°o m·ªôt b√°o c√°o ph√¢n t√≠ch ƒë·ªëi th·ªß c·∫°nh tranh t·ª´ {len(articles)} b√†i b√°o sau ƒë√¢y cho kho·∫£ng th·ªùi gian: {date_range}.
            
            D·ªØ li·ªáu ƒë·∫ßu v√†o:
            - Input: M·ªôt danh s√°ch c√°c b√†i b√°o ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß, kh√¥ng c·∫ßn s·ª≠a ƒë·ªïi g√¨ th√™m: {json.dumps([a.model_dump(mode='json') for a in articles], ensure_ascii=False, indent=2)}

            Y√™u c·∫ßu nhi·ªám v·ª•:
            1. D√πng danh s√°ch articles tr√™n ƒë·ªÉ t·∫°o `overall_summary` v√† `industry_summaries`.
            2. T·∫°o 'overall_summary' (t√≥m t·∫Øt t·ªïng quan), bao g·ªìm: thoi_gian_trich_xuat, industries (nganh_hang, nhan_hang, cum_noi_dung, so_luong_bai, cac_dau_bao), cac_dau_bao v√† tong_so_bai.
            3. T·∫°o danh s√°ch 'industry_summaries' (t√≥m t·∫Øt theo ng√†nh), m·ªói ng√†nh l√† 1 m·ª•c (nganh_hang), bao g·ªìm: nhan_hang, cum_noi_dung (tr∆∞·ªùng cum_noi_dung s·∫Ω l√† bao g·ªìm h·∫øt t·∫•t c·∫£ c√°c c·ª•m n·ªôi dung c·ªßa t·∫•t c·∫£ c√°c b√†i trong c√πng 1 ng√†nh), cac_dau_bao, so_luong_bai.
            4. Quy t·∫Øc khi t·∫°o tr∆∞·ªùng `cum_noi_dung` trong `industry_summaries`:
                - `cum_noi_dung` ch·ªâ ƒë∆∞·ª£c ch·ªçn trong danh s√°ch sau (kh√¥ng th√™m m√¥ t·∫£ chi ti·∫øt):
                - "Ho·∫°t ƒë·ªông doanh nghi·ªáp v√† th√¥ng tin s·∫£n ph·∫©m"
                - "Ch∆∞∆°ng tr√¨nh CSR"
                - "Chi·∫øn d·ªãch Marketing"
                - "Ra m·∫Øt s·∫£n ph·∫©m"
                - "H·ª£p t√°c ƒë·ªëi t√°c"
                - "B√°o c√°o t√†i ch√≠nh"
                - "An to√†n th·ª±c ph·∫©m"
                - "Kh√°c"
            5. N·∫øu c·∫ßn m√¥ t·∫£ chi ti·∫øt, h√£y ghi v√†o `cum_noi_dung_chi_tiet`, kh√¥ng ƒë∆∞·ª£c ghi v√†o `cum_noi_dung`.
            6. ƒê·∫£m b·∫£o tr·∫£ v·ªÅ ƒë√∫ng 1 ƒë·ªëi t∆∞·ª£ng JSON duy nh·∫•t, kh√¥ng c√≥ markdown, kh√¥ng c√≥ gi·∫£i th√≠ch ngo√†i l·ªÅ.

            Tr·∫£ v·ªÅ ƒë√∫ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON duy nh·∫•t v·ªõi c·∫•u tr√∫c sau:
            {{
                overall_summary: { ... },
                industry_summaries: [ ... ],
                total_articles={len(articles)},
                date_range={date_range}
            }}
            
            Quy t·∫Øc b·∫Øt bu·ªôc:
            - B·∫Øt ƒë·∫ßu output b·∫±ng '{' v√† k·∫øt th√∫c b·∫±ng '}' duy nh·∫•t.
            """
            response = await self._arun_with_user_fallback(
                report_prompt,
                session_id=self.session_id,
                preferred_provider=getattr(self.config, "provider", None),
                preferred_model=getattr(self.config, "model", None),
            )
            logger.debug(f"Raw LLM response: {response.content}")
            if response and response.content:
                try:
                    try:
                        # ∆Øu ti√™n parse b·∫±ng json.loads
                        summary_data = json.loads(response.content)
                    except json.JSONDecodeError:
                        try:
                            # N·∫øu LLM tr·∫£ v·ªÅ d·∫°ng {'key': 'value'}, d√πng ast.literal_eval
                            summary_data = ast.literal_eval(response.content)
                        except Exception:
                            # Fallback d√πng extract_json ƒë·ªÉ t√¨m ƒë√∫ng ƒëo·∫°n JSON
                            raw_json = self.extract_json(response.content)
                            summary_data = json.loads(raw_json)

                    # Check c√≥ ph·∫£i dict kh√¥ng
                    if not isinstance(summary_data, dict):
                        logger.error("LLM tr·∫£ v·ªÅ list ho·∫∑c sai schema. Fallback.")
                        return self._create_basic_report(articles, date_range)

                    # G·ªôp l·∫°i articles t·ª´ input, kh√¥ng cho LLM sinh
                    summary_data["articles"] = [
                        a.model_dump(mode="json") for a in articles
                    ]
                    summary_data = self._sanitize_report_payload(
                        summary_data, articles, date_range
                    )

                    # Truy·ªÅn v√†o CompetitorReport
                    return CompetitorReport(**summary_data)

                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(
                        f"Kh√¥ng th·ªÉ ph√¢n t√≠ch ph·∫£n h·ªìi t·ª´ ReportAgent d∆∞·ªõi d·∫°ng JSON: {e}. ƒêang t·∫°o b√°o c√°o c∆° b·∫£n."
                    )
                    return self._create_basic_report(articles, date_range)
            return self._create_basic_report(articles, date_range)
        except Exception as e:
            logger.error(f"T·∫°o b√°o c√°o th·∫•t b·∫°i: {e}", exc_info=True)
            return self._create_basic_report(articles, date_range)
        finally:
            gc.collect()

    def _create_basic_report(
        self, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        industry_groups = {}
        for article in articles:
            industry = article.nganh_hang
            if industry not in industry_groups:
                industry_groups[industry] = []
            industry_groups[industry].append(article)

        industry_summaries = [
            IndustrySummary(
                nganh_hang=industry,
                nhan_hang=list(
                    set(
                        brand
                        for article in industry_articles
                        for brand in article.nhan_hang
                    )
                ),
                cum_noi_dung=list(
                    set(
                        c
                        for article in industry_articles
                        if (c := article.cum_noi_dung) is not None
                    )
                )
                or [ContentCluster.OTHER.value],
                so_luong_bai=len(industry_articles),
                cac_dau_bao=list(set(article.dau_bao for article in industry_articles)),
            )
            for industry, industry_articles in industry_groups.items()
        ]

        overall_summary = OverallSummary(
            thoi_gian_trich_xuat=date_range,
            industries=industry_summaries,
            tong_so_bai=len(articles),
        )

        return CompetitorReport(
            overall_summary=overall_summary,
            industry_summaries=industry_summaries,
            articles=articles,
            total_articles=len(articles),
            date_range=date_range,
        )

    def close(self):
        del self.agent
        gc.collect()


class MediaTrackerTeam:
    """ƒê·ªôi ƒëi·ªÅu ph·ªëi ch√≠nh cho to√†n b·ªô quy tr√¨nh."""

    def __init__(
        self,
        config: CrawlConfig,
        start_date: datetime,
        end_date: datetime,
        user_email: Optional[str] = None,
        session_id: Optional[str] = None,
        on_progress_update: Optional[callable] = None,
        check_cancelled: Optional[callable] = None,
        check_paused: Optional[callable] = None,
        articles_so_far=None,
        source_status_list=None,
        provider: Optional[str] = None,
        model: Optional[str] = None,
    ):
        model_runtime = get_llm_model(provider, model)
        self.config = config

        def _source_key_of(s):
            return getattr(s, "reference_name", None) or f"{s.type}|{s.domain}"

        sel = set(getattr(self.config, "selected_sources", []) or [])
        if sel:
            self.config.media_sources = [
                s for s in self.config.media_sources if _source_key_of(s) in sel
            ]

        self.session_id = session_id
        self.user_email = user_email
        self.status = BotStatus()
        self.check_cancelled = check_cancelled or (lambda: False)
        self.check_paused = check_paused or (lambda: False)
        self.articles_so_far = articles_so_far or []
        self.on_progress_update = on_progress_update
        parser = ArticleParser()
        self.crawler = CrawlerAgent(
            model_runtime,
            config,
            parser,
            session_id,
            check_cancelled,
            check_paused,
            check_pause_or_cancel=self.check_pause_or_cancel,
            user_email=user_email,
            status=self.status,
            on_progress_update=self.on_progress_update,
        )
        # self.processor = ProcessorAgent(get_llm_model("openai", "gpt-4o"))
        # self.reporter = ReportAgent(get_llm_model("openai", "gpt-4o"))
        self.processor = ProcessorAgent(model_runtime, config, session_id)
        self.reporter = ReportAgent(model_runtime, config, session_id)
        self.start_date = start_date
        self.end_date = end_date
        self.source_status_list = source_status_list or []
        self.config.use_hub_page = True

    @staticmethod
    def _norm_url(u: str) -> str:
        try:
            p = urlparse(u or "")
            path = (p.path or "").rstrip("/")
            qs = [
                (k, v)
                for k, v in parse_qsl(p.query or "", keep_blank_values=True)
                if not k.lower().startswith(("utm_", "fbclid", "gclid"))
            ]
            return urlunparse(
                (
                    p.scheme or "https",
                    (p.netloc or "").lower().lstrip("www."),
                    path,
                    "",
                    urlencode(qs, doseq=True),
                    "",
                )
            )
        except Exception:
            return u or ""

    def _merge_articles(self, new_articles):
        acc = (self.articles_so_far or []) + (new_articles or [])
        seen, uniq = set(), []
        for a in acc:
            url = (
                getattr(a, "link_bai_bao", None)
                or getattr(a, "url", None)
                or getattr(a, "link", None)
                or ""
            )
            key = self._norm_url(url)
            if key and key not in seen:
                uniq.append(a)
                seen.add(key)
        self.articles_so_far = uniq

    async def check_pause_or_cancel(self):
        if self.check_cancelled():
            logger.info("‚õî Cancelled. Stop pipeline.")
            raise asyncio.CancelledError("Pipeline is cancelled.")

        while self.check_paused():
            logger.info("‚è∏Ô∏è  Pipeline paused. Waiting to resume...")
            await asyncio.sleep(2)

            if self.check_cancelled():
                logger.info(f"[{self.session_id}] Task was cancelled during pause.")
                raise asyncio.CancelledError("Cancelled during pause")

    async def run_full_pipeline(self) -> Optional[CompetitorReport]:
        """
        Executes the full media tracking pipeline: crawling, processing, and report generation.
        Optimizes memory usage by limiting concurrent tasks and processing articles in batches.
        """
        self.status.is_running = True
        self.status.current_task = "Initializing pipeline"
        self.status.total_sources = len(self.config.media_sources)
        self.status.progress = 0.0
        # all_articles = self.articles_so_far.copy()

        logger.info(
            f"üìÖ Kho·∫£ng th·ªùi gian crawl d·ªØ li·ªáu: t·ª´ ng√†y {self.start_date.strftime('%d/%m/%Y')} ƒë·∫øn ng√†y {self.end_date.strftime('%d/%m/%Y')}"
        )

        try:
            # Step 1: Crawl data from media sources
            self.status.current_task = "Crawling data from media sources"
            logger.info(f"Starting crawl for {self.status.total_sources} sources.")

            # Limit concurrent crawl tasks using Semaphore
            self.semaphore = asyncio.Semaphore(self.config.max_concurrent_sources)

            async def wrapped_crawl(media_source, industry_name, keywords):
                # N·∫øu ƒë√£ completed r·ªìi th√¨ skip
                if any(
                    s["source_name"] == media_source.name and s["status"] == "completed"
                    for s in self.source_status_list
                ):
                    logger.info(
                        f"[{media_source.name}] ‚úÖ ƒê√£ ho√†n th√†nh t·ª´ tr∆∞·ªõc, b·ªè qua."
                    )
                    return media_source, None

                async with self.semaphore:
                    try:
                        await _maybe_await(self.check_pause_or_cancel)
                        result = await self.crawler.crawl_media_source(
                            media_source=media_source,
                            industry_name=industry_name,
                            keywords=keywords,
                            start_date=self.start_date,
                            end_date=self.end_date,
                        )

                        if result and result.articles_found:
                            self._merge_articles(result.articles_found)

                        task_manager.update_task(
                            self.user_email,
                            self.session_id,
                            {
                                "articles_so_far": [
                                    a.model_dump(mode="json")
                                    for a in self.articles_so_far
                                ],
                                "source_status_list": self.source_status_list,
                            },
                        )

                        return media_source, result

                    except asyncio.TimeoutError:
                        logger.warning(
                            f"[{media_source.name}] ‚è∞ Timeout sau {self.config.crawl_timeout} gi√¢y."
                        )
                        partial_result = self.crawler.return_partial_result(
                            media_source
                        )

                        if partial_result.articles_found:
                            self._merge_articles(partial_result.articles_found)

                        self.source_status_list.append(
                            {"source_name": media_source.name, "status": "failed"}
                        )

                        task_manager.update_task(
                            self.user_email,
                            self.session_id,
                            {
                                "articles_so_far": [
                                    a.model_dump(mode="json")
                                    for a in self.articles_so_far
                                ],
                                "source_status_list": self.source_status_list,
                            },
                        )

                        return media_source, partial_result

            # Prepare all keywords
            # all_keywords = list(
            #     set(kw for kws in self.config.keywords.values() for kw in kws)
            # )
            tasks = []
            for industry_name, keywords in self.config.keywords.items():
                for media_source in self.config.media_sources:
                    already_done = next(
                        (
                            s
                            for s in self.source_status_list
                            if s["source_name"] == media_source.name
                        ),
                        None,
                    )
                    if already_done and already_done["status"] == "completed":
                        continue

                    task = asyncio.create_task(
                        wrapped_crawl(media_source, industry_name, keywords)
                    )
                    tasks.append((media_source, task))

            # Execute crawl tasks
            tasks_only = [t[1] for t in tasks]
            results = await asyncio.gather(*tasks_only, return_exceptions=True)

            for (media_source, _), result in zip(tasks, results):
                await _maybe_await(self.check_pause_or_cancel)

                try:
                    if isinstance(result, Exception):
                        logger.error(
                            f"[{media_source.name}] ‚ùå L·ªói: {result}", exc_info=True
                        )
                        self.source_status_list.append(
                            {"source_name": media_source.name, "status": "failed"}
                        )
                        continue

                    media_source, crawl_result = result
                    self.status.completed_sources += 1

                    crawl_status = (
                        getattr(crawl_result, "crawl_status", "failed")
                        if crawl_result
                        else "failed"
                    )
                    status = "completed" if crawl_status == "success" else "failed"

                    self.source_status_list.append(
                        {"source_name": media_source.name, "status": status}
                    )

                    completed = len(
                        [
                            s
                            for s in self.source_status_list
                            if s["status"] == "completed"
                        ]
                    )
                    failed = len(
                        [s for s in self.source_status_list if s["status"] == "failed"]
                    )

                    if self.on_progress_update:
                        self.on_progress_update(
                            source_name=media_source.name,
                            completed=completed,
                            failed=failed,
                            progress=((completed + failed) / self.status.total_sources)
                            * 50.0,
                            current_task=f"Crawling {media_source.name}",
                        )

                    gc.collect()

                except asyncio.CancelledError:
                    logger.warning(f"Crawl task cancelled.")
                    for _, t in tasks:
                        t.cancel()
                    self.status.failed_sources += 1
                    raise
                except Exception as e:
                    self.status.failed_sources += 1
                    logger.error(
                        f"Unexpected error crawling source {media_source.name}: {e}",
                        exc_info=True,
                    )

            if not self.articles_so_far:
                logger.warning("‚ö†Ô∏è No articles found from crawling or cache.")
                return None

            all_articles = self.articles_so_far.copy()
            logger.info(f"Crawling completed. Found {len(all_articles)} raw articles.")

            # Step 2: Process articles in batches
            self.status.current_task = "Processing and analyzing articles"
            self.status.progress = 60.0
            batch_size = 10  # Process 20 articles per batch
            processed_articles = []
            for i in range(0, len(all_articles), batch_size):
                await _maybe_await(self.check_pause_or_cancel)

                batch = all_articles[i : i + batch_size]
                batch_processed = await self.processor.process_articles(
                    batch, self.config.keywords
                )
                processed_articles.extend(batch_processed)
                logger.info(
                    f"Processed batch {i // batch_size + 1}: {len(batch_processed)} articles"
                )
                gc.collect()  # Free memory after each batch

            logger.info(
                f"Processing completed. Retained {len(processed_articles)} articles."
            )
            self.status.progress = 80.0

            # Step 3: Generate report in batches
            self.status.current_task = "Generating final report"
            date_range_str = f"T·ª´ ng√†y {self.start_date.strftime('%d/%m/%Y')} ƒë·∫øn ng√†y {self.end_date.strftime('%d/%m/%Y')}"

            report = None
            if not processed_articles:
                logger.warning("No valid articles to generate report.")
                return None

            await _maybe_await(self.check_pause_or_cancel)

            report = await self.reporter.generate_report(
                processed_articles, date_range_str
            )

            if report is None:
                logger.warning("No valid articles to generate report.")
                return None

            # Update overall summary
            report.overall_summary.tong_so_bai = len(report.articles)
            report.total_articles = len(report.articles)
            report.date_range = date_range_str

            self.status.progress = 100.0
            self.status.current_task = "Pipeline completed"
            logger.info("Pipeline completed successfully.")
            await asyncio.sleep(0.5)
            return report

        except (InterruptedError, asyncio.CancelledError) as e:
            self.status.current_task = f"Stopped: {str(e)}"
            for _, task in tasks:
                task.cancel()
            results = await asyncio.gather(
                *[t[1] for t in tasks], return_exceptions=True
            )
            for (media_source, _), result in zip(tasks, results):
                if isinstance(result, Exception):
                    logger.error(f"[{media_source.name}] ‚ùå L·ªói: {result}")
                    self.source_status_list.append(
                        {"source_name": media_source.name, "status": "failed"}
                    )
                else:
                    logger.info(f"[{media_source.name}] ‚úÖ Crawl th√†nh c√¥ng")
                    self.source_status_list.append(
                        {"source_name": media_source.name, "status": "completed"}
                    )
            self.status.current_task = "ƒê√£ h·ªßy"
            raise
        except Exception as e:
            self.status.current_task = f"Failed: {str(e)}"
            logger.error(f"Pipeline failed: {e}", exc_info=True)
            return None
        finally:
            self.status.is_running = False
            self.status.last_run = datetime.now()
            if self.status.progress < 100.0:
                self.status.progress = 100.0
            if "completed" not in self.status.current_task.lower():
                self.status.current_task = "Ready"
            self.cleanup()
            gc.collect()  # Final memory cleanup

    def get_status(self) -> BotStatus:
        return self.status

    def cleanup(self):
        logger.info("üîß ƒêang gi·∫£i ph√≥ng t√†i nguy√™n pipeline...")
        # ƒê√≥ng pool Playwright (kh√¥ng ch·∫∑n thread g·ªçi cleanup)
        try:
            asyncio.create_task(PlaywrightPool.instance().close())
        except RuntimeError:
            # n·∫øu kh√¥ng c√≥ loop ƒëang ch·∫°y, ƒë√≥ng ƒë·ªìng b·ªô ‚Äúbest effort‚Äù
            try:
                loop = asyncio.get_event_loop()
                loop.run_until_complete(PlaywrightPool.instance().close())
            except Exception:
                pass
        self.crawler.close_final()
        self.processor.close()
        self.reporter.close()
        gc.collect()
