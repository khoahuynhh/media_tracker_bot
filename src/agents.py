# src/agents.py
"""
Multi-Agent System for the Media Tracker Bot.
Phi√™n b·∫£n n√†y s·∫Ω c·∫£i thi·ªán l·∫°i c√°c prompt chi ti·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng ph√¢n t√≠ch,
ƒë·ªìng th·ªùi gi·ªØ l·∫°i y√™u c·∫ßu output d·∫°ng JSON ƒë·ªÉ h·ªá th·ªëng ho·∫°t ƒë·ªông b·ªÅn b·ªâ.
"""

import asyncio
import json
import logging
import gc
import threading
import httpx
import re
import ast

from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from playwright.sync_api import sync_playwright
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from collections import defaultdict

# Import Agno
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.models.groq import Groq
from agno.tools.crawl4ai import Crawl4aiTools
from agno.tools.googlesearch import GoogleSearchTools
from ddgs import DDGS
from agno.tools.arxiv import ArxivTools
from agno.tools.baidusearch import BaiduSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.tools.pubmed import PubmedTools
from agno.tools.searxng import SearxngTools
from agno.tools.wikipedia import WikipediaTools

# Import modules
from openai import APITimeoutError
from .parsing import ArticleParser
from .models import (
    MediaSource,
    Article,
    IndustrySummary,
    OverallSummary,
    CompetitorReport,
    CrawlResult,
    CrawlConfig,
    BotStatus,
    ContentCluster,
    KeywordManager,
)
from .configs import CONFIG_DIR
from .cache_manager import SafeCacheManager

logger = logging.getLogger(__name__)


def get_llm_model(provider: str, model_id: str) -> Any:
    """Factory function to get an LLM model instance."""
    if provider == "groq":
        return Groq(id=model_id)
    return OpenAIChat(id=model_id, timeout=90)


class CrawlerAgent:
    """Agent chuy√™n crawl web, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát."""

    def __init__(
        self, model: Any, config: CrawlConfig, parser: ArticleParser, timeout: int = 10
    ):
        self.parser = parser
        self.config = config
        self.search_tools = [
            GoogleSearchTools(timeout),
            DDGS(),
            # ArxivTools(),
            # BaiduSearchTools(),
            # HackerNewsTools(),
            PubmedTools(),
            WikipediaTools(),
        ]
        self.search_tool_index = 0
        self.model = model
        self.agent = None
        self._create_agent()
        self.cache_manager = SafeCacheManager(
            cache_dir="cache/crawl_results",
            ttl_hours=self.config.cache_duration_hours,
            version="1.0",
        )

    async def _check_pause(self):
        if hasattr(self, "pause_event"):
            while self.pause_event.is_set():
                logging.info("‚è∏ CrawlerAgent paused... waiting to resume.")
                await asyncio.sleep(1)

    async def _check_stop(self):
        if hasattr(self, "stop_event"):
            while self.stop_event.is_set():
                raise InterruptedError("Pipeline stopped by user.")

    def return_partial_result(self, media_source: MediaSource) -> CrawlResult:
        unique_articles = {
            article.link_bai_bao: article
            for article in getattr(self, "current_articles", [])
        }
        articles = list(unique_articles.values())

        return CrawlResult(
            source_name=media_source.name,
            source_type=media_source.type,
            url=media_source.domain,
            articles_found=articles,
            crawl_status="partial" if articles else "timeout",
            error_message="Tr·∫£ v·ªÅ c√°c b√†i ƒë√£ crawl ƒë∆∞·ª£c tr∆∞·ªõc khi timeout.",
            crawl_duration=self.config.crawl_timeout,
        )

    def _create_agent(self):
        if self.agent:
            del self.agent
        current_tool = self.search_tools[self.search_tool_index]

        self.agent = Agent(
            name="MediaCrawler",
            role="Web Crawler v√† Content Extractor",
            model=self.model,
            tools=[Crawl4aiTools(max_length=2000), current_tool],
            instructions=[
                "B·∫°n l√† m·ªôt chuy√™n gia crawl web ƒë·ªÉ theo d√µi truy·ªÅn th√¥ng t·∫°i Vi·ªát Nam.",
                "Nhi·ªám v·ª•: Crawl c√°c website b√°o ch√≠ ƒë·ªÉ t√¨m b√†i vi·∫øt v·ªÅ c√°c ƒë·ªëi th·ªß c·∫°nh tranh d·ª±a tr√™n keywords v√† ng√†nh h√†ng.",
                "∆Øu ti√™n tin t·ª©c m·ªõi nh·∫•t trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.",
                "Ch·ªâ l·∫•y c√°c b√†i vi·∫øt ƒë∆∞·ª£c ƒëƒÉng trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c y√™u c·∫ßu.",
                "N·∫øu kh√¥ng t√¨m th·∫•y b√†i vi·∫øt th√¨ ƒë·ªÉ tr·ªëng, ƒë·ª´ng t·ª± t·∫°o n·ªôi dung hay format.",
                "Kh√¥ng l·∫•y c√°c b√†i vi·∫øt ƒëƒÉng tr∆∞·ªõc ho·∫∑c sau kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh.",
                "Tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON h·ª£p l·ªá ch·ª©a danh s√°ch b√†i b√°o v·ªõi c√°c tr∆∞·ªùng: ti√™u ƒë·ªÅ, ng√†y ph√°t h√†nh (DD-MM-YYYY), t√≥m t·∫Øt n·ªôi dung, link b√†i b√°o.",
            ],
            show_tool_calls=True,
            markdown=True,
            add_datetime_to_instructions=True,
        )

    def _rotate_tool(self):
        self.search_tool_index = (self.search_tool_index + 1) % len(self.search_tools)
        self._create_agent()

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type(
            (APITimeoutError, httpx.ConnectTimeout, httpx.ReadTimeout)
        ),
        reraise=True,
    )
    async def crawl_media_source(
        self,
        media_source: MediaSource,
        industry_name: str,
        keywords: List[str],
        start_date: datetime,
        end_date: datetime,
    ) -> CrawlResult:
        start_time = datetime.now()

        # Use cache if true
        if self.config.use_cache:
            cache_key = self.cache_manager.make_cache_key(
                media_source.name,
                industry_name,
                keywords,
                start_date,
                end_date,
            )
            cached_data = self.cache_manager.load_cache(cache_key)

            if cached_data:
                logger.info(f"[{media_source.name}] ‚úÖ Loaded from cache.")
                return CrawlResult(**cached_data)

        date_filter = f"t·ª´ ng√†y {start_date.strftime('%Y-%m-%d')} ƒë·∫øn ng√†y {end_date.strftime('%Y-%m-%d')}"
        domain_url = media_source.domain
        if domain_url and not domain_url.startswith("http"):
            domain_url = f"https://{domain_url}"

        max_keywords_per_query = 3
        keyword_groups = [
            keywords[i : i + max_keywords_per_query]
            for i in range(0, len(keywords), max_keywords_per_query)
        ]
        articles = []
        session_id = (
            f"crawler-{media_source.name}-{datetime.now().strftime('%Y%m%d%H%M%S')}"
        )

        def to_date(dt):
            return dt.date() if isinstance(dt, datetime) else dt

        try:
            for tool in self.search_tools:
                await self._check_stop()
                await self._check_pause()
                self._rotate_tool()
                tool_name = type(self.agent.tools[1]).__name__
                logger.info(f"[{media_source.name}] ƒêang d√πng search tool: {tool_name}")

                new_articles_this_tool = 0
                self.current_articles = []
                for group in keyword_groups:
                    await self._check_stop()
                    await self._check_pause()
                    keywords_str = ", ".join(group)

                    crawl_query = f"""
                    Crawl website: {domain_url or media_source.name}
                    T√¨m c√°c b√†i b√°o c√≥ ch·ª©a c√°c t·ª´ kh√≥a: {keywords_str} v√† PH·∫¢I li√™n quan ƒë·∫øn ng√†nh h√†ng: {industry_name}
                    Th·ªùi gian: {date_filter}
                    Y√™u c·∫ßu:
                    - Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ, t√≥m t·∫Øt, ng√†y ƒëƒÉng, link g·ªëc.
                    - Ch·ªâ l·∫•y b√†i vi·∫øt li√™n quan ƒë·∫øn ng√†nh h√†ng v√† t·ª´ kh√≥a.
                    - T·∫°o t√≥m t·∫Øt chi ti·∫øt (d∆∞·ªõi 100 t·ª´), n√™u b·∫≠t c√°c th√¥ng tin ch√≠nh nh∆∞: s·ª± ki·ªán ch√≠nh, c√°c b√™n li√™n quan, v√† k·∫øt qu·∫£ ho·∫∑c t√°c ƒë·ªông c·ªßa s·ª± ki·ªán. Kh√¥ng ch·ªâ l·∫∑p l·∫°i ti√™u ƒë·ªÅ.
                    - Format: Ti√™u ƒë·ªÅ | Ng√†y | T√≥m t·∫Øt | Link
                    """

                    try:
                        logger.info(
                            f"[{media_source.name}] Searching with keywords: {keywords_str}"
                        )
                        response = await self.agent.arun(
                            crawl_query, session_id=session_id
                        )
                        logger.info(
                            f"[DEBUG] Raw response content:\n{response.content}"
                        )
                        await asyncio.sleep(1)

                        if response and response.content:
                            if (
                                "enable javascript" in response.content.lower()
                                or "captcha" in response.content.lower()
                            ):
                                logger.warning(
                                    f"[{media_source.name}] Blocked or captcha required for {tool_name}"
                                )
                                continue

                            parsed_articles = self.parser.parse(
                                response.content, media_source
                            )
                            valid_new_articles = [
                                a
                                for a in parsed_articles
                                if a
                                and a.link_bai_bao
                                and a.ngay_phat_hanh
                                and to_date(start_date)
                                <= to_date(a.ngay_phat_hanh)
                                <= to_date(end_date)
                            ]

                            if valid_new_articles:
                                articles.extend(valid_new_articles)
                                self.current_articles.extend(valid_new_articles)
                                new_articles_this_tool += len(valid_new_articles)

                                logger.info(
                                    f"[{media_source.name}] Found {len(valid_new_articles)} articles with {tool_name} for keywords: {keywords_str}"
                                )
                                logger.debug(
                                    f"[{media_source.name}] Total articles so far: {len(articles)}"
                                )
                            else:
                                logger.warning(
                                    f"[{media_source.name}] No articles parsed from {tool_name} for keywords: {keywords_str}"
                                )
                        else:
                            logger.warning(
                                f"[{media_source.name}] No response from {tool_name} for keywords: {keywords_str}"
                            )

                    except (
                        httpx.HTTPStatusError,
                        httpx.RequestError,
                        APITimeoutError,
                        ValueError,
                    ) as e:
                        logger.warning(
                            f"[{media_source.name}] Error in {tool_name} for keywords {keywords_str}: {e}"
                        )
                        await asyncio.sleep(1)
                    except Exception as e:
                        logger.error(
                            f"[{media_source.name}] Unexpected error in {tool_name} for keywords {keywords_str}: {e}",
                            exc_info=True,
                        )
                        await asyncio.sleep(1)

                    gc.collect()

                if new_articles_this_tool > 0:
                    logger.info(
                        f"[{media_source.name}] Stopping search as articles were found by {tool_name}"
                    )
                    break

            unique_articles = {
                article.link_bai_bao: article
                for article in articles
                if article.link_bai_bao
            }
            articles = list(unique_articles.values())
            logger.info(
                f"[{media_source.name}] Crawled {len(articles)} unique articles"
            )

            result = CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=media_source.domain,
                articles_found=articles,
                crawl_status="success" if articles else "failed",
                error_message=(
                    ""
                    if articles
                    else f"Th·ª≠ h·∫øt {len(self.search_tools)} search tool nh∆∞ng kh√¥ng t√¨m th·∫•y b√†i b√°o"
                ),
                crawl_duration=(datetime.now() - start_time).total_seconds(),
            )

            if (
                self.config.use_cache
                and result.crawl_status == "success"
                and len(result.articles_found) > 0
            ):
                self.cache_manager.save_cache(cache_key, result)

            return result

        except Exception as e:
            logger.error(f"[{media_source.name}] Crawl failed: {e}", exc_info=True)
            return CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=media_source.domain,
                articles_found=[],
                crawl_status="failed",
                error_message=f"Crawl failed: {str(e)}",
                crawl_duration=(datetime.now() - start_time).total_seconds(),
            )
        finally:
            gc.collect()

    def close_final(self):
        logger.info("üßπ ƒê√≥ng ho√†n to√†n CrawlerAgent, gi·∫£i ph√≥ng agent v√† tools.")
        self.agent = None
        for tool in self.search_tools:
            if hasattr(tool, "close"):
                try:
                    tool.close()
                except Exception as e:
                    logger.warning(f"Tool {tool} ƒë√≥ng kh√¥ng th√†nh c√¥ng: {e}")
        gc.collect()


class ProcessorAgent:
    """Agent chuy√™n x·ª≠ l√Ω v√† ph√¢n t√≠ch n·ªôi dung, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát chi ti·∫øt."""

    def __init__(self, model: Any):
        self.agent = Agent(
            name="ContentProcessor",
            role="Chuy√™n gia Ph√¢n t√≠ch v√† Ph√¢n lo·∫°i N·ªôi dung",
            model=model,
            instructions=[
                "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch n·ªôi dung truy·ªÅn th√¥ng cho ng√†nh FMCG t·∫°i Vi·ªát Nam.",
                "Nhi·ªám v·ª• c·ªßa b·∫°n: Ph√¢n t√≠ch v√† ph√¢n lo·∫°i c√°c b√†i b√°o theo ng√†nh h√†ng v√† nh√£n hi·ªáu.",
                "B·∫°n B·∫ÆT BU·ªòC ph·∫£i tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch JSON (JSON list) h·ª£p l·ªá c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng Article ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß.",
            ],
            markdown=True,
        )

    def extract_json(self, text: str) -> str:
        """
        Tr√≠ch xu·∫•t JSON t·ª´ ph·∫£n h·ªìi LLM v·ªõi th·ª© t·ª± ∆∞u ti√™n:
        1. T√¨m ƒëo·∫°n trong ```json ... ```
        2. N·∫øu kh√¥ng c√≥, b·ªè d·∫•u '...' ngo√†i c√πng (n·∫øu c√≥)
        3. T√¨m ƒëo·∫°n JSON {...} ho·∫∑c [...]
        """

        text = text.strip()

        # 1Ô∏è. ∆Øu ti√™n t√¨m ```json ... ```
        matches = re.findall(r"```json(.*?)```", text, re.DOTALL)
        if matches:
            for match in matches:
                match = match.strip()
                try:
                    json.loads(match)
                    return match  # ‚úÖ Tr·∫£ v·ªÅ JSON ƒë√∫ng lu√¥n
                except json.JSONDecodeError:
                    continue

        # 2. N·∫øu kh√¥ng c√≥, x·ª≠ l√Ω d·∫•u '...' ngo√†i c√πng
        if text.startswith("'") and text.endswith("'"):
            text = text[1:-1].strip()

        # 3Ô∏è. T√¨m ƒëo·∫°n JSON {...} ho·∫∑c [...]
        candidates = re.findall(r"(\{.*?\}|\[.*?\])", text, re.DOTALL)
        for candidate in candidates:
            candidate = candidate.strip()
            try:
                json.loads(candidate)
                return candidate  # ‚úÖ Tr·∫£ v·ªÅ JSON ƒë√∫ng
            except json.JSONDecodeError:
                continue

        # N·∫øu kh√¥ng t√¨m th·∫•y
        raise ValueError("Kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá trong ph·∫£n h·ªìi.")

    async def process_articles(
        self, raw_articles: List[Article], keywords_config: Dict[str, List[str]]
    ) -> List[Article]:
        """
        Processes a list of raw articles to classify industries, brands, content clusters,
        and extract summaries and keywords. Optimizes memory usage by processing articles in batches.

        Args:
            raw_articles: List of raw Article objects to process.
            keywords_config: Dictionary mapping industries to lists of keywords.

        Returns:
            List of processed Article objects.
        """
        if not raw_articles:
            logger.info("No articles to process.")
            return []

        processed_articles = []
        batch_size = 10  # Process 20 articles per batch to reduce memory usage

        try:
            # Extract brand list (uppercase keywords) for competitor identification
            brand_list = list(
                set(
                    kw
                    for kws in keywords_config.values()
                    for kw in kws
                    if kw[0].isupper()
                )
            )

            # Define content clusters with associated keywords
            content_clusters = {
                ContentCluster.HOAT_DONG_DOANH_NGHIEP: [
                    "s·∫£n xu·∫•t",
                    "nh√† m√°y",
                    "tuy·ªÉn d·ª•ng",
                    "doanh nghi·ªáp",
                    "ho·∫°t ƒë·ªông",
                    "ƒë·∫ßu t∆∞",
                ],
                ContentCluster.CHUONG_TRINH_CSR: [
                    "t√†i tr·ª£",
                    "m√¥i tr∆∞·ªùng",
                    "c·ªông ƒë·ªìng",
                    "CSR",
                    "t·ª´ thi·ªán",
                ],
                ContentCluster.MARKETING_CAMPAIGN: [
                    "truy·ªÅn th√¥ng",
                    "KOL",
                    "khuy·∫øn m√£i",
                    "qu·∫£ng c√°o",
                    "chi·∫øn d·ªãch",
                ],
                ContentCluster.PRODUCT_LAUNCH: [
                    "s·∫£n ph·∫©m m·ªõi",
                    "bao b√¨",
                    "c√¥ng th·ª©c",
                    "ra m·∫Øt",
                    "ph√°t h√†nh",
                ],
                ContentCluster.PARTNERSHIP: [
                    "MOU",
                    "li√™n doanh",
                    "k√Ω k·∫øt",
                    "h·ª£p t√°c",
                    "ƒë·ªëi t√°c",
                ],
                ContentCluster.FINANCIAL_REPORT: [
                    "l·ª£i nhu·∫≠n",
                    "doanh thu",
                    "tƒÉng tr∆∞·ªüng",
                    "b√°o c√°o t√†i ch√≠nh",
                    "k·∫øt qu·∫£ kinh doanh",
                ],
                ContentCluster.FOOD_SAFETY: [
                    "an to√†n th·ª±c ph·∫©m",
                    "ATTP",
                    "ng·ªô ƒë·ªôc",
                    "nhi·ªÖm khu·∫©n",
                    "thu h·ªìi s·∫£n ph·∫©m",
                    "ch·∫•t c·∫•m",
                    "ki·ªÉm tra ATTP",
                    "thanh tra an to√†n th·ª±c ph·∫©m",
                    "truy xu·∫•t ngu·ªìn g·ªëc",
                    "blockchain th·ª±c ph·∫©m",
                    "tem QR",
                    "chu·ªói cung ·ª©ng s·∫°ch",
                    "cam k·∫øt ch·∫•t l∆∞·ª£ng th·ª±c ph·∫©m",
                    "quy ƒë·ªãnh an to√†n th·ª±c ph·∫©m",
                    "x·ª≠ ph·∫°t vi ph·∫°m ATTP",
                    "s·ª©c kh·ªèe",
                    "thu h·ªìi s·∫£n ph·∫©m",
                ],
                ContentCluster.OTHER: [],
            }

            # Process articles in batches
            for i in range(0, len(raw_articles), batch_size):
                batch = raw_articles[i : i + batch_size]
                logger.info(
                    f"Processing batch {i // batch_size + 1} with {len(batch)} articles"
                )

                # Create analysis prompt for the batch
                analysis_prompt = f"""
                Ph√¢n t√≠ch v√† ph√¢n lo·∫°i {len(batch)} b√†i b√°o sau ƒë√¢y.

                Danh s√°ch c√°c nh√£n h√†ng ƒë·ªëi th·ªß c·∫ßn x√°c ƒë·ªãnh:
                {json.dumps(brand_list, ensure_ascii=False, indent=2)}
                
                Keywords config:
                {json.dumps(keywords_config, ensure_ascii=False, indent=2)}
                
                Raw articles:
                {json.dumps([a.model_dump(mode='json') for a in batch], ensure_ascii=False, indent=2)}
                
                Y√™u c·∫ßu:
                1. Ph√¢n lo·∫°i ch√≠nh x√°c ng√†nh h√†ng cho t·ª´ng b√†i (d·ª±a theo b·ªëi c·∫£nh b√†i v√† danh s√°ch nh√£n h√†ng ng√†nh h√†ng t∆∞∆°ng ·ª©ng).
                2. Tr√≠ch xu·∫•t `nhan_hang`:
                    - ƒê·ªçc n·ªôi dung b√†i vi·∫øt v√† ki·ªÉm tra xem c√≥ nh√£n h√†ng n√†o trong danh s√°ch sau xu·∫•t hi·ªán hay kh√¥ng: 
                    {json.dumps(brand_list, ensure_ascii=False, indent=2)}
                    - Ch·ªâ ghi nh·∫≠n nh·ªØng nh√£n h√†ng th·ª±c s·ª± xu·∫•t hi·ªán trong b√†i vi·∫øt (b·∫•t k·ªÉ vi·∫øt hoa hay vi·∫øt th∆∞·ªùng).
                    - N·∫øu kh√¥ng th·∫•y nh√£n h√†ng n√†o th√¨ ƒë·ªÉ `nhan_hang` l√† `[]`. Kh√¥ng t·ª± b·ªãa ho·∫∑c t·ª± suy ƒëo√°n th√™m.
                3. Ph√¢n lo·∫°i l·∫°i c·ª•m n·ªôi dung (`cum_noi_dung`). N·∫øu b√†i vi·∫øt c√≥ n·ªôi dung t∆∞∆°ng ƒë∆∞∆°ng, ƒë·ªìng nghƒ©a ho·∫∑c g·∫ßn gi·ªëng v·ªõi c√°c c·ª•m t·ª´ kh√≥a: {json.dumps({k.value: v for k, v in content_clusters.items()}, ensure_ascii=False, indent=2)}, h√£y ph√¢n lo·∫°i v√†o c·ª•m ƒë√≥.
                4. N·∫øu kh√¥ng t√¨m th·∫•y c·ª•m n·ªôi dung n√†o kh·ªõp v·ªõi danh s√°ch t·ª´ kh√≥a c·ª•m n·ªôi dung, B·∫ÆT BU·ªòC g√°n tr∆∞·ªùng (`cum_noi_dung`) l√† '{ContentCluster.OTHER.value}', KH√îNG ƒê∆Ø·ª¢C ƒë·ªÉ tr·ªëng ho·∫∑c tr·∫£ v·ªÅ none hay null.
                5. Vi·∫øt l·∫°i n·ªôi dung chi ti·∫øt, ng·∫Øn g·ªçn v√† mang t√≠nh m√¥ t·∫£ kh√°i qu√°t cho tr∆∞·ªùng `cum_noi_dung_chi_tiet` d·ª±a tr√™n n·ªôi dung ƒë√£ c√≥ s·∫µn:
                    - L√† 1 d√≤ng m√¥ t·∫£ ng·∫Øn (~10‚Äì20 t·ª´) cho b√†i b√°o, c√≥ c·∫•u tr√∫c:  
                    `[Lo·∫°i th√¥ng tin]: [T√≥m t·∫Øt n·ªôi dung n·ªïi b·∫≠t]`
                    - V√≠ d·ª•: "Th√¥ng tin doanh nghi·ªáp: T∆∞·ªùng An kh·∫≥ng ƒë·ªãnh v·ªã th·∫ø d·ªãp T·∫øt 2025"
                6. Tr√≠ch xu·∫•t v√† ghi v√†o `keywords_found`:
                    - L√† t·∫•t c·∫£ c√°c t·ª´ kh√≥a ng√†nh li√™n quan th·ª±c s·ª± xu·∫•t hi·ªán trong b√†i vi·∫øt.
                    - Ch·ªâ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ c√°c t·ª´ kh√≥a ƒë√£ cung c·∫•p trong `keywords_config`.
                    - N·∫øu kh√¥ng t√¨m th·∫•y t·ª´ kh√≥a n√†o, ƒë·ªÉ `keywords_found` l√† []
                7. Ch·ªâ gi·ªØ b√†i vi·∫øt li√™n quan ƒë·∫øn ng√†nh FMCG (D·∫ßu ƒÉn, Gia v·ªã, S·ªØa, v.v.) d·ª±a tr√™n t·ª´ kh√≥a trong `keywords_config`. Lo·∫°i b·ªè b√†i kh√¥ng li√™n quan (e.g., ch√≠nh tr·ªã, s·ª©c kh·ªèe kh√¥ng li√™n quan).
                8. ƒê·ªãnh d·∫°ng ng√†y ph√°t h√†nh b·∫Øt bu·ªôc: dd/mm/yyyy (VD: 01/07/2025)"
                9. N·∫øu m·ªôt b√†i b√°o ƒë·ªÅ c·∫≠p nhi·ªÅu nh√£n h√†ng th√¨ ghi t·∫•t c·∫£ nh√£n h√†ng trong danh s√°ch `nhan_hang`.
                10. N·∫øu b√†i li√™n quan nhi·ªÅu ng√†nh (v√≠ d·ª• s·∫£n ph·∫©m ƒëa d·ª•ng), h√£y ch·ªçn ng√†nh ch√≠nh nh·∫•t li√™n quan ƒë·∫øn b·ªëi c·∫£nh.
                11. Gi·ªØ nguy√™n `tom_tat_noi_dung`, kh√¥ng c·∫Øt b·ªõt, sinh ra hay thay ƒë·ªïi n·ªôi dung.

                ƒê·ªãnh d·∫°ng ƒë·∫ßu ra:
                Tr·∫£ v·ªÅ m·ªôt danh s√°ch JSON h·ª£p l·ªá ch·ª©a c√°c ƒë·ªëi t∆∞·ª£ng Article ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω. C·∫•u tr√∫c JSON c·ªßa m·ªói ƒë·ªëi t∆∞·ª£ng ph·∫£i kh·ªõp v·ªõi Pydantic model. ƒê√¢y l√† 1 v√≠ d·ª• cho b·∫°n l√†m m·∫´u:
                [
                    {{
                        "stt": 1,
                        "ngay_phat_hanh": "01/07/2025",
                        "dau_bao": "VNEXPRESS",
                        "cum_noi_dung": "Chi·∫øn d·ªãch Marketing",
                        "cum_noi_dung_chi_tiet": "Chi·∫øn d·ªãch T·∫øt 2025 c·ªßa Vinamilk chinh ph·ª•c ng∆∞·ªùi ti√™u d√πng tr·∫ª",
                        "tom_tat_noi_dung": "Vinamilk tung chi·∫øn d·ªãch T·∫øt 2025...",
                        "link_bai_bao": "https://vnexpress.net/...",
                        "nganh_hang": "S·ªØa",
                        "nhan_hang": ["Vinamilk"],
                        "keywords_found": ["T·∫øt", "TV qu·∫£ng c√°o", "Vinamilk"]
                    }}
                ]
                [
                    {{
                        "stt": 2,
                        "ngay_phat_hanh": "06/07/2025",
                        "dau_bao": "Thanh Nien",
                        "cum_noi_dung": "Ho·∫°t ƒë·ªông doanh nghi·ªáp v√† th√¥ng tin s·∫£n ph·∫©m",
                        "cum_noi_dung_chi_tiet": "Th√¥ng tin doanh nghi·ªáp: T∆∞·ªùng An v√† Coba m·ªü r·ªông th·ªã ph·∫ßn d·∫ßu ƒÉn mi·ªÅn T√¢y",
                        "tom_tat_noi_dung": "T∆∞·ªùng An v√† Coba tƒÉng c∆∞·ªùng ƒë·∫ßu t∆∞ v√† ph√¢n ph·ªëi s·∫£n ph·∫©m d·∫ßu ƒÉn t·∫°i khu v·ª±c mi·ªÅn T√¢y Nam B·ªô.",
                        "link_bai_bao": "https://thanhnien.vn/tuong-an-coba-dau-an",
                        "nganh_hang": "D·∫ßu ƒÉn",
                        "nhan_hang": ["T∆∞·ªùng An", "Coba"],
                        "keywords_found": ["T∆∞·ªùng An", "Coba", "d·∫ßu ƒÉn", "th·ªã ph·∫ßn"]
                    }}
                ]
                [
                    {{
                        "stt": 3,
                        "ngay_phat_hanh": "07/07/2025",
                        "dau_bao": "Tuoi Tre",
                        "cum_noi_dung": "Ch∆∞∆°ng tr√¨nh CSR",
                        "cum_noi_dung_chi_tiet": "CSR: Doanh nghi·ªáp ƒë·ªãa ph∆∞∆°ng h·ªó tr·ª£ c·ªông ƒë·ªìng mi·ªÅn n√∫i",
                        "tom_tat_noi_dung": "M·ªôt s·ªë doanh nghi·ªáp ƒë·ªãa ph∆∞∆°ng ph·ªëi h·ª£p t·ªï ch·ª©c ch∆∞∆°ng tr√¨nh h·ªó tr·ª£ b√† con mi·ªÅn n√∫i m√πa m∆∞a l≈©.",
                        "link_bai_bao": "https://tuoitre.vn/csr-mien-nui",
                        "nganh_hang": "D·∫ßu ƒÉn",
                        "nhan_hang": [],
                        "keywords_found": ["h·ªó tr·ª£ c·ªông ƒë·ªìng", "CSR", "mi·ªÅn n√∫i"]
                    }}
                ]
                """

                try:
                    response = await self.agent.arun(analysis_prompt)
                    logger.error(f"LLM raw output:\n{response.content}")

                    if response and response.content:
                        try:
                            raw_json = self.extract_json(response.content)
                            articles_data = json.loads(raw_json)

                            km = KeywordManager(
                                CONFIG_DIR / "content_cluster_keywords.json"
                            )
                            valid_clusters = [c.value for c in ContentCluster]

                            for item in articles_data:
                                text_to_check = (
                                    item.get("tom_tat_noi_dung", "")
                                    + " "
                                    + (item.get("cum_noi_dung_chi_tiet", "") or "")
                                ).lower()

                                # Fallback c·ª•m n·ªôi dung n·∫øu c·∫ßn
                                if (
                                    item.get("cum_noi_dung")
                                    in [None, "null", "", "Kh√°c"]
                                    or item.get("cum_noi_dung") not in valid_clusters
                                ):
                                    text_to_check = (
                                        item.get("tom_tat_noi_dung", "")
                                        + " "
                                        + (item.get("cum_noi_dung_chi_tiet", "") or "")
                                    ).strip()
                                    fallback_cluster = km.map_to_cluster(text_to_check)
                                    item["cum_noi_dung"] = fallback_cluster

                                # Fallback keywords_found
                                if (
                                    "keywords_found" not in item
                                    or not item["keywords_found"]
                                ):
                                    item["keywords_found"] = []
                                    for industry, keywords in keywords_config.items():
                                        for kw in keywords:
                                            if kw.lower() in text_to_check:
                                                item["keywords_found"].append(kw)
                                    # Lo·∫°i tr√πng
                                    item["keywords_found"] = list(
                                        set(item["keywords_found"])
                                    )

                                # Fallback nh√£n h√†ng
                                if "nhan_hang" not in item or not item["nhan_hang"]:
                                    item["nhan_hang"] = []
                                    for brand in brand_list:
                                        if brand.lower() in text_to_check:
                                            item["nhan_hang"].append(brand)
                                    # Lo·∫°i tr√πng
                                    item["nhan_hang"] = list(set(item["nhan_hang"]))

                            processed_articles.extend(
                                [Article(**item) for item in articles_data]
                            )
                            logger.info(
                                f"Batch {i // batch_size + 1} processed: {len(articles_data)} articles"
                            )

                        except (json.JSONDecodeError, TypeError) as e:
                            logger.error(
                                f"Failed to parse JSON response for batch {i // batch_size + 1}: {e}"
                            )
                            # Fallback: g√°n c·ª•m n·ªôi dung b·∫±ng KeywordManager khi l·ªói
                            km = KeywordManager(
                                CONFIG_DIR / "content_cluster_keywords.json"
                            )
                            for article in batch:
                                text = (
                                    article.tom_tat_noi_dung
                                    + " "
                                    + (article.cum_noi_dung_chi_tiet or "")
                                )
                                article.cum_noi_dung = km.map_to_cluster(text)
                            processed_articles.extend(batch)

                    else:
                        logger.warning(
                            f"No valid response for batch {i // batch_size + 1}"
                        )
                        # Fallback t∆∞∆°ng t·ª± khi kh√¥ng c√≥ response
                        km = KeywordManager(
                            CONFIG_DIR / "content_cluster_keywords.json"
                        )
                        for article in batch:
                            text = (
                                article.tom_tat_noi_dung
                                + " "
                                + (article.cum_noi_dung_chi_tiet or "")
                            )
                            article.cum_noi_dung = km.map_to_cluster(text)
                        processed_articles.extend(batch)

                except Exception as e:
                    logger.error(
                        f"Error processing batch {i // batch_size + 1}: {e}",
                        exc_info=True,
                    )
                    processed_articles.extend(
                        batch
                    )  # Keep raw articles if processing fails

                # Free memory after each batch
                gc.collect()

            logger.info(
                f"Processing completed. Total processed articles: {len(processed_articles)}"
            )
            return processed_articles

        except Exception as e:
            logger.error(f"Article processing failed: {e}", exc_info=True)
            return raw_articles  # Return raw articles if the entire process fails
        finally:
            gc.collect()  # Final memory cleanup

    def close(self):
        del self.agent
        gc.collect()


class ReportAgent:
    """Agent chuy√™n t·∫°o b√°o c√°o, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát."""

    def __init__(self, model: Any):
        self.agent = Agent(
            name="ReportGenerator",
            role="Chuy√™n gia T·∫°o B√°o c√°o v√† Ph√¢n t√≠ch D·ªØ li·ªáu",
            model=model,
            instructions=[
                "B·∫°n l√† chuy√™n gia t·∫°o b√°o c√°o ph√¢n t√≠ch truy·ªÅn th√¥ng cho ng√†nh FMCG.",
                "Nhi·ªám v·ª•: T·∫°o m·ªôt b√°o c√°o ph√¢n t√≠ch ƒë·ªëi th·ªß c·∫°nh tranh t·ª´ d·ªØ li·ªáu c√°c b√†i b√°o ƒë√£ ƒë∆∞·ª£c cung c·∫•p.",
                "B·∫°n B·∫ÆT BU·ªòC ph·∫£i tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON (JSON object) duy nh·∫•t, h·ª£p l·ªá v√† tu√¢n th·ªß nghi√™m ng·∫∑t theo c·∫•u tr√∫c c·ªßa Pydantic model 'CompetitorReport'.",
                "N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ho·∫∑c c√≥ l·ªói, tr·∫£ v·ªÅ CompetitorReport r·ªóng h·ª£p l·ªá v·ªõi c√°c tr∆∞·ªùng l√† [] ho·∫∑c 0.",
            ],
            markdown=False,
        )

    def extract_json(self, text: str) -> str:
        text = text.strip()

        # ∆Øu ti√™n t√¨m ƒëo·∫°n ```json ... ```
        matches = re.findall(r"```json(.*?)```", text, re.DOTALL)
        if matches:
            for match in matches:
                match = match.strip()
                try:
                    obj = json.loads(match)
                    if isinstance(obj, dict):
                        return match
                except:
                    continue

        # N·∫øu kh√¥ng c√≥, t√¨m JSON object ngo√†i c√πng
        try:
            # Th·ª≠ parse to√†n b·ªô text lu√¥n n·∫øu l√† dict
            obj = json.loads(text)
            if isinstance(obj, dict):
                return text
        except:
            pass

        # Fallback t√¨m c√°c c·∫∑p { } l·ªõn nh·∫•t
        dict_candidates = re.findall(r"(\{.*\})", text, re.DOTALL)
        for candidate in dict_candidates:
            try:
                obj = json.loads(candidate.strip())
                if isinstance(obj, dict):
                    return candidate.strip()
            except:
                continue

        raise ValueError("Kh√¥ng t√¨m th·∫•y JSON dict h·ª£p l·ªá trong ph·∫£n h·ªìi.")

    async def generate_report(
        self, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        """
        Generates a competitor analysis report from a list of articles for a given date range.
        """

        if not articles:
            logger.info(
                "No articles provided for report generation. Returning basic report."
            )
            return self._create_basic_report([], date_range)

        try:
            logger.info(f"Generating full report for {len(articles)} articles...")

            report_prompt = f"""
            T·∫°o m·ªôt b√°o c√°o ph√¢n t√≠ch ƒë·ªëi th·ªß c·∫°nh tranh t·ª´ {len(articles)} b√†i b√°o sau ƒë√¢y cho kho·∫£ng th·ªùi gian: {date_range}.
            
            D·ªØ li·ªáu ƒë·∫ßu v√†o:
            - Input: M·ªôt danh s√°ch c√°c b√†i b√°o ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß, kh√¥ng c·∫ßn s·ª≠a ƒë·ªïi g√¨ th√™m: {json.dumps([a.model_dump(mode='json') for a in articles], ensure_ascii=False, indent=2)}

            Y√™u c·∫ßu nhi·ªám v·ª•:
            1. D√πng danh s√°ch articles tr√™n ƒë·ªÉ t·∫°o `overall_summary` v√† `industry_summaries`.
            2. T·∫°o 'overall_summary' (t√≥m t·∫Øt t·ªïng quan), bao g·ªìm: thoi_gian_trich_xuat, industries (nganh_hang, nhan_hang, cum_noi_dung, so_luong_bai, cac_dau_bao), cac_dau_bao v√† tong_so_bai.
            3. T·∫°o danh s√°ch 'industry_summaries' (t√≥m t·∫Øt theo ng√†nh), m·ªói ng√†nh l√† 1 m·ª•c (nganh_hang), bao g·ªìm: nhan_hang, cum_noi_dung (tr∆∞·ªùng cum_noi_dung s·∫Ω l√† bao g·ªìm h·∫øt t·∫•t c·∫£ c√°c c·ª•m n·ªôi dung c·ªßa t·∫•t c·∫£ c√°c b√†i trong c√πng 1 ng√†nh), cac_dau_bao, so_luong_bai.
            4. Quy t·∫Øc khi t·∫°o tr∆∞·ªùng `cum_noi_dung` trong `industry_summaries`:
                - `cum_noi_dung` ch·ªâ ƒë∆∞·ª£c ch·ªçn trong danh s√°ch sau (kh√¥ng th√™m m√¥ t·∫£ chi ti·∫øt):
                - "Ho·∫°t ƒë·ªông doanh nghi·ªáp v√† th√¥ng tin s·∫£n ph·∫©m"
                - "Ch∆∞∆°ng tr√¨nh CSR"
                - "Chi·∫øn d·ªãch Marketing"
                - "Ra m·∫Øt s·∫£n ph·∫©m"
                - "H·ª£p t√°c ƒë·ªëi t√°c"
                - "B√°o c√°o t√†i ch√≠nh"
                - "An to√†n th·ª±c ph·∫©m"
                - "Kh√°c"
            5. N·∫øu c·∫ßn m√¥ t·∫£ chi ti·∫øt, h√£y ghi v√†o `cum_noi_dung_chi_tiet`, kh√¥ng ƒë∆∞·ª£c ghi v√†o `cum_noi_dung`.
            6. ƒê·∫£m b·∫£o tr·∫£ v·ªÅ ƒë√∫ng 1 ƒë·ªëi t∆∞·ª£ng JSON duy nh·∫•t, kh√¥ng c√≥ markdown, kh√¥ng c√≥ gi·∫£i th√≠ch ngo√†i l·ªÅ.

            Tr·∫£ v·ªÅ ƒë√∫ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON duy nh·∫•t v·ªõi c·∫•u tr√∫c sau:
            {{
                overall_summary: { ... },
                industry_summaries: [ ... ],
                total_articles={len(articles)},
                date_range={date_range}
            }}
            
            Quy t·∫Øc b·∫Øt bu·ªôc:
            - B·∫Øt ƒë·∫ßu output b·∫±ng '{' v√† k·∫øt th√∫c b·∫±ng '}' duy nh·∫•t.
            """
            response = await self.agent.arun(report_prompt)
            logger.error(f"Raw LLM response: {response.content}")
            if response and response.content:
                try:
                    try:
                        # ∆Øu ti√™n parse b·∫±ng json.loads
                        summary_data = json.loads(response.content)
                    except json.JSONDecodeError:
                        try:
                            # N·∫øu LLM tr·∫£ v·ªÅ d·∫°ng {'key': 'value'}, d√πng ast.literal_eval
                            summary_data = ast.literal_eval(response.content)
                        except Exception:
                            # Fallback d√πng extract_json ƒë·ªÉ t√¨m ƒë√∫ng ƒëo·∫°n JSON
                            raw_json = self.extract_json(response.content)
                            summary_data = json.loads(raw_json)

                    # Check c√≥ ph·∫£i dict kh√¥ng
                    if not isinstance(summary_data, dict):
                        logger.error("LLM tr·∫£ v·ªÅ list ho·∫∑c sai schema. Fallback.")
                        return self._create_basic_report(articles, date_range)

                    # G·ªôp l·∫°i articles t·ª´ input, kh√¥ng cho LLM sinh
                    summary_data["articles"] = [
                        a.model_dump(mode="json") for a in articles
                    ]

                    # Truy·ªÅn v√†o CompetitorReport
                    return CompetitorReport(**summary_data)

                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(
                        f"Kh√¥ng th·ªÉ ph√¢n t√≠ch ph·∫£n h·ªìi t·ª´ ReportAgent d∆∞·ªõi d·∫°ng JSON: {e}. ƒêang t·∫°o b√°o c√°o c∆° b·∫£n."
                    )
                    return self._create_basic_report(articles, date_range)
            return self._create_basic_report(articles, date_range)
        except Exception as e:
            logger.error(f"T·∫°o b√°o c√°o th·∫•t b·∫°i: {e}", exc_info=True)
            return self._create_basic_report(articles, date_range)
        finally:
            gc.collect()

    def _create_basic_report(
        self, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        industry_groups = {}
        for article in articles:
            industry = article.nganh_hang
            if industry not in industry_groups:
                industry_groups[industry] = []
            industry_groups[industry].append(article)

        industry_summaries = [
            IndustrySummary(
                nganh_hang=industry,
                nhan_hang=list(
                    set(
                        brand
                        for article in industry_articles
                        for brand in article.nhan_hang
                    )
                ),
                cum_noi_dung=list(
                    set(
                        c
                        for article in industry_articles
                        if (c := article.cum_noi_dung) is not None
                    )
                )
                or [ContentCluster.OTHER],
                so_luong_bai=len(industry_articles),
                cac_dau_bao=list(set(article.dau_bao for article in industry_articles)),
            )
            for industry, industry_articles in industry_groups.items()
        ]

        overall_summary = OverallSummary(
            thoi_gian_trich_xuat=date_range,
            industries=industry_summaries,
            tong_so_bai=len(articles),
        )

        return CompetitorReport(
            overall_summary=overall_summary,
            industry_summaries=industry_summaries,
            articles=articles,
            total_articles=len(articles),
            date_range=date_range,
        )

    def close(self):
        del self.agent
        gc.collect()


class MediaTrackerTeam:
    """ƒê·ªôi ƒëi·ªÅu ph·ªëi ch√≠nh cho to√†n b·ªô quy tr√¨nh."""

    def __init__(
        self,
        config: CrawlConfig,
        start_date: datetime,
        end_date: datetime,
        stop_event: Optional[threading.Event] = None,
        pause_event=threading.Event,
    ):
        self.config = config
        self.stop_event = stop_event or threading.Event()
        self.pause_event = pause_event
        self.status = BotStatus()

        parser = ArticleParser()
        self.crawler = CrawlerAgent(
            get_llm_model("openai", "gpt-4o-mini"), config, parser
        )
        self.crawler.pause_event = self.pause_event
        self.crawler.stop_event = self.stop_event
        self.processor = ProcessorAgent(get_llm_model("openai", "gpt-4o"))
        self.reporter = ReportAgent(get_llm_model("openai", "gpt-4o"))
        self.start_date = start_date
        self.end_date = end_date

    async def _check_pause(self):
        while self.pause_event.is_set():
            logger.info("‚è∏ Pipeline paused... waiting to resume.")
            await asyncio.sleep(1)

    async def run_full_pipeline(self) -> Optional[CompetitorReport]:
        """
        Executes the full media tracking pipeline: crawling, processing, and report generation.
        Optimizes memory usage by limiting concurrent tasks and processing articles in batches.
        """
        self.status.is_running = True
        self.status.current_task = "Initializing pipeline"
        self.status.total_sources = len(self.config.media_sources)
        self.status.progress = 0.0
        all_articles = []

        try:
            # Step 1: Crawl data from media sources
            self.status.current_task = "Crawling data from media sources"
            logger.info(f"Starting crawl for {self.status.total_sources} sources.")

            # Limit concurrent crawl tasks using Semaphore
            self.semaphore = asyncio.Semaphore(self.config.max_concurrent_sources)

            async def bounded_crawl(media_source, industry_name, keywords):
                async with self.semaphore:
                    try:
                        return await asyncio.wait_for(
                            self.crawler.crawl_media_source(
                                media_source=media_source,
                                industry_name=industry_name,
                                keywords=keywords,
                                start_date=self.start_date,
                                end_date=self.end_date,
                            ),
                            timeout=self.config.crawl_timeout,  # Timeout m·ªói ngu·ªìn b√°o (VD: 30s)
                        )
                    except asyncio.TimeoutError:
                        logger.warning(
                            f"[{media_source.name}] ‚è∞ Timeout sau {self.config.crawl_timeout} gi√¢y."
                        )
                        return self.crawler.return_partial_result(media_source)

            # Prepare all keywords
            # all_keywords = list(
            #     set(kw for kws in self.config.keywords.values() for kw in kws)
            # )
            tasks = []
            for industry_name, keywords in self.config.keywords.items():
                for media_source in self.config.media_sources:
                    tasks.append(bounded_crawl(media_source, industry_name, keywords))

            # Execute crawl tasks
            for i, task in enumerate(asyncio.as_completed(tasks)):
                await self._check_pause()
                if self.stop_event.is_set():
                    logger.warning("Pipeline stopped by user during crawling.")
                    raise InterruptedError("Pipeline stopped by user during crawling.")

                try:
                    result = await task
                    self.status.completed_sources += 1
                    self.status.progress = (
                        self.status.completed_sources / self.status.total_sources
                    ) * 50.0

                    if result.crawl_status == "success":
                        all_articles.extend(result.articles_found)
                        logger.info(
                            f"Crawled {len(result.articles_found)} articles from {result.source_name}"
                        )
                    else:
                        self.status.failed_sources += 1
                        logger.warning(
                            f"Crawl failed for {result.source_name}: {result.error_message}"
                        )

                    # Free memory after each source
                    gc.collect()

                except asyncio.CancelledError:
                    logger.warning(f"Crawl task {i} cancelled.")
                    self.status.failed_sources += 1
                except Exception as e:
                    self.status.failed_sources += 1
                    logger.error(
                        f"Unexpected error crawling source {i + 1}: {e}", exc_info=True
                    )

            logger.info(f"Crawling completed. Found {len(all_articles)} raw articles.")

            if self.stop_event.is_set():
                raise InterruptedError("Pipeline stopped by user after crawling.")

            # Step 2: Process articles in batches
            self.status.current_task = "Processing and analyzing articles"
            self.status.progress = 60.0
            batch_size = 10  # Process 20 articles per batch
            processed_articles = []
            for i in range(0, len(all_articles), batch_size):
                await self._check_pause()
                if self.stop_event.is_set():
                    raise InterruptedError("Pipeline stopped by user after processing.")
                batch = all_articles[i : i + batch_size]
                batch_processed = await self.processor.process_articles(
                    batch, self.config.keywords
                )
                processed_articles.extend(batch_processed)
                logger.info(
                    f"Processed batch {i // batch_size + 1}: {len(batch_processed)} articles"
                )
                gc.collect()  # Free memory after each batch

            logger.info(
                f"Processing completed. Retained {len(processed_articles)} articles."
            )
            self.status.progress = 80.0

            # Step 3: Generate report in batches
            self.status.current_task = "Generating final report"
            end_date = datetime.now()
            start_date = end_date - timedelta(days=self.config.date_range_days)
            date_range_str = f"T·ª´ ng√†y {start_date.strftime('%d/%m/%Y')} ƒë·∫øn ng√†y {end_date.strftime('%d/%m/%Y')}"

            report = None
            await self._check_pause()
            if self.stop_event.is_set():
                raise InterruptedError(
                    "Pipeline stopped by user before report generation."
                )

            if not processed_articles:
                logger.warning("No valid articles to generate report.")
                return None

            report = await self.reporter.generate_report(
                processed_articles, date_range_str
            )

            await self._check_pause()
            if self.stop_event.is_set():
                raise InterruptedError(
                    "Pipeline stopped by user after report generation."
                )

            if report is None:
                logger.warning("No valid articles to generate report.")
                return None

            # Update overall summary
            report.overall_summary.tong_so_bai = len(report.articles)
            report.total_articles = len(report.articles)
            report.date_range = date_range_str

            self.status.progress = 100.0
            self.status.current_task = "Pipeline completed"
            logger.info("Pipeline completed successfully.")
            await asyncio.sleep(0.5)
            return report

        except (InterruptedError, asyncio.CancelledError) as e:
            self.status.current_task = f"Stopped: {str(e)}"
            logger.warning(f"Pipeline stopped or cancelled: {e}")
            return None
        except Exception as e:
            self.status.current_task = f"Failed: {str(e)}"
            logger.error(f"Pipeline failed: {e}", exc_info=True)
            return None
        finally:
            self.status.is_running = False
            self.status.last_run = datetime.now()
            if self.status.progress < 100.0:
                self.status.progress = 100.0
            if "completed" not in self.status.current_task.lower():
                self.status.current_task = "Ready"
            gc.collect()  # Final memory cleanup

    def get_status(self) -> BotStatus:
        return self.status

    def cleanup(self):
        logger.info("üîß ƒêang gi·∫£i ph√≥ng t√†i nguy√™n pipeline...")
        self.crawler.close_final()
        self.processor.close()
        self.reporter.close()
        gc.collect()
