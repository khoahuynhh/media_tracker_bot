# src/agents.py
"""
Multi-Agent System for the Media Tracker Bot.
Phi√™n b·∫£n n√†y s·∫Ω c·∫£i thi·ªán l·∫°i c√°c prompt chi ti·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng ph√¢n t√≠ch,
ƒë·ªìng th·ªùi gi·ªØ l·∫°i y√™u c·∫ßu output d·∫°ng JSON ƒë·ªÉ h·ªá th·ªëng ho·∫°t ƒë·ªông b·ªÅn b·ªâ.
"""

import asyncio
import json
import logging
import gc
import httpx
import re
import ast
import time
import random
import os
import inspect
import requests

from datetime import datetime, date
from typing import List, Dict, Optional, Any, Tuple
from playwright.sync_api import sync_playwright
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
from bs4 import BeautifulSoup
from asyncio import Semaphore

# Import Agno
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.models.groq import Groq
from agno.models.google import Gemini
from agno.tools.crawl4ai import Crawl4aiTools
from agno.tools.googlesearch import GoogleSearchTools
from ddgs import DDGS

# Import modules
from openai import APITimeoutError
from .parsing import ArticleParser
from .event import event_bus, decision_bus, fallback_lock_by_session
from .models import (
    MediaSource,
    Article,
    IndustrySummary,
    OverallSummary,
    CompetitorReport,
    CrawlResult,
    CrawlConfig,
    BotStatus,
    ContentCluster,
    KeywordManager,
    HubPageNotFound,
)
from .configs import CONFIG_DIR, settings
from .cache_manager import SafeCacheManager
from .task_state import task_manager

logger = logging.getLogger(__name__)

PROVIDER_MODEL_MAP = {
    "openai": {"default": "gpt-4o-mini", "report": "gpt-4o-mini"},
    "groq": {"default": "llama-3.1-70b-versatile", "report": "llama-3.1-70b-versatile"},
    "gemini": {"default": "gemini-2.0-flash", "report": "gemini-2.0-flash"},
}

# Async http_client v·ªõi retry t·ªëi ƒëa 3 l·∫ßn
http_client = httpx.AsyncClient(
    timeout=60.0,
    limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
    transport=httpx.AsyncHTTPTransport(retries=3),  # <-- retry t·ªëi ƒëa 3 l·∫ßn
)


def get_llm_model(provider: Optional[str], model_id: Optional[str]) -> Any:
    status = settings.get_api_key_status()
    chosen = (provider or status["default_provider"] or "openai").lower()

    # NEW: ∆∞u ti√™n model_id truy·ªÅn v√†o; n·∫øu kh√¥ng c√≥ th√¨ l·∫•y t·ª´ ENV (DEFAULT_MODEL_ID)
    mid = (
        model_id
        or os.getenv("DEFAULT_MODEL_ID")
        or PROVIDER_MODEL_MAP.get(chosen, PROVIDER_MODEL_MAP["openai"])["default"]
    )

    if chosen == "groq":
        return Groq(id=mid)
    elif chosen == "gemini":
        return Gemini(id=mid)
    else:
        return OpenAIChat(id=mid, http_client=http_client)


# API Errors
def _configured_providers_in_order(settings, preferred: str | None = None) -> list[str]:
    st = settings.get_api_key_status()
    # h√†m b·∫°n ƒë√£ c√≥
    enabled_ok = {
        "openai": bool(st.get("openai_configured"))
        and getattr(settings, "openai_enabled", True),
        "groq": bool(st.get("groq_configured"))
        and getattr(settings, "groq_enabled", True),
        "gemini": bool(st.get("google_configured"))
        and getattr(settings, "google_enabled", True),
    }
    if preferred:
        p = preferred.lower()
        return [p] if enabled_ok.get(p) else []
    return [p for p, ok in enabled_ok.items() if ok]


def _map_llm_error(err: Exception) -> tuple[str, str]:
    sc = getattr(getattr(err, "response", None), "status_code", None)
    code = None
    try:
        body = getattr(err, "response", None)
        if body is not None and hasattr(body, "json"):
            j = body.json()
            code = (j.get("error") or {}).get("code")
    except Exception:
        pass

    if sc in (401, 403):
        return "PROVIDER_AUTH", "API key invalid/expired"
    if sc == 429:
        if (code or "").lower() == "insufficient_quota":
            return "PROVIDER_NO_QUOTA", "Insufficient quota"
        return "PROVIDER_RATE_LIMIT", "Rate limit"
    if isinstance(err, (asyncio.TimeoutError, httpx.ReadTimeout, httpx.ConnectTimeout)):
        return "PROVIDER_TIMEOUT", "Timeout"
    return "PROVIDER_ERROR", str(err)


# Search tools
def ddgs_search_text(query: str):
    time.sleep(1.5)
    # headers = {
    #     "User-Agent": f"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/{random.randint(100,120)}.0.{random.randint(1000,9999)}.100 Safari/537.36"
    # }
    with DDGS(timeout=60) as ddg:
        return list(ddg.text(query, max_results=30, backend="auto"))


class GoogleSearchWithDelay(GoogleSearchTools):
    def run(self, query: str):
        time.sleep(random.uniform(2.5, 5.0))  # Delay t·ª± nhi√™n
        return super().run(query)


def google_cse_search(domain, keywords):
    API_KEY = os.getenv("GOOGLE_API_KEY")
    query = f"site:{domain} {' '.join(keywords)} tag"
    url = "https://www.googleapis.com/customsearch/v1"
    params = {"key": API_KEY, "cx": "16c863775f52f42cd", "q": query, "num": 10}
    resp = requests.get(url, params=params)
    data = resp.json()

    if "items" not in data:
        print("‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£:", data)
        return None

    for item in data["items"]:
        print(item["link"])
    return data["items"][0]["link"]


def _norm_host(h):
    h = h.lower()
    return h[4:] if h.startswith("www.") else h


def get_first_search_link(domain: str, keywords: list[str]) -> str | None:
    wanted_host = _norm_host(domain)

    def search_ddg():
        headers = {
            "User-Agent": f"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/{random.randint(100,120)}.0.{random.randint(1000,9999)}.100 Safari/537.36"
        }
        query = f"site:{domain} {' '.join(keywords)} tin t·ª©c"
        with DDGS(headers=headers, timeout=60) as ddg:
            results = list(ddg.text(query, max_results=20, backend="auto"))
            logger.info(
                f"üîç DuckDuckGo results ({domain}): {[r.get('href') for r in results]}"
            )

            same_domain = []
            for r in results:
                href = r.get("href") or ""
                host = _norm_host(urlparse(href).netloc)
                if host == wanted_host:
                    same_domain.append(href)

            def is_tag_hub(u: str) -> bool:
                lu = u.lower()
                return (
                    "/tag" in lu
                    or "/tags/" in lu
                    or "/chu-de/" in lu
                    or "/the/" in lu
                    or "/topic/" in lu
                )

            def contains_kw(u: str) -> bool:
                lu = u.lower()
                return any(kw.lower() in lu for kw in keywords)

            # a) tag-hub + ch·ª©a keyword trong URL
            for href in same_domain:
                if is_tag_hub(href) and contains_kw(href):
                    return href
            # b) ch·ªâ c·∫ßn l√† tag-hub ƒë√∫ng domain
            for href in same_domain:
                if is_tag_hub(href):
                    return href
            # c) fallback: b·∫•t k·ª≥ link n√†o ƒë√∫ng domain
            if same_domain:
                return same_domain[0]

            # d) fallback to√†n b·ªô results
            for r in results:
                href = r.get("href") or ""
                if _norm_host(urlparse(href).netloc) == wanted_host:
                    return href

            return None

    # --- 1. DDG l·∫ßn 1 ---
    link = search_ddg()
    if link:
        return link

    # --- 2. DDG l·∫ßn 2 ---
    logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y hub link cho {domain}, th·ª≠ l·∫°i sau 10s...")
    time.sleep(10)
    link = search_ddg()
    if link:
        return link

    # --- 3. Fallback sang Google CSE ---
    logger.warning(f"‚ö†Ô∏è DDG th·∫•t b·∫°i, fallback sang Google CSE cho {domain}")
    return google_cse_search(domain, keywords)


_playwright_sem = Semaphore(2)


def crawl_with_playwright_sync(url: str) -> str:
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                # hai flag d∆∞·ªõi ch·ªâ d√πng n·∫øu m√°y/antivirus kh√≥ t√≠nh:
                # "--single-process", "--no-zygote",
            ],
        )
        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/123.0 Safari/537.36"
            )
        )
        page = context.new_page()
        try:
            page.goto(url, timeout=60_000, wait_until="domcontentloaded")
            # n·∫øu c√≤n request ng·∫ßm, cho ƒë·ª£i th√™m ch√∫t
            try:
                page.wait_for_load_state("networkidle", timeout=5_000)
            except:
                pass
            html = page.content()
            return html
        finally:
            context.close()
            browser.close()


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=4),
    reraise=True,
)
async def crawl_with_playwright(url: str) -> str:
    # wrapper async g·ªçi b·∫£n sync trong thread pool, c√≥ semaphore h·∫°n ch·∫ø song song
    async with _playwright_sem:
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, crawl_with_playwright_sync, url)


# --- START: Helpers function ---
async def _maybe_await(fn):
    if inspect.iscoroutinefunction(fn):
        return await fn()
    res = fn()
    if inspect.isawaitable(res):
        return await res
    return res


async def _get_response_text(resp):
    if hasattr(resp, "content"):
        return resp.content
    if hasattr(resp, "text"):
        return resp.text
    # stream
    if hasattr(resp, "__aiter__"):
        chunks = []
        async for part in resp:
            chunks.append(getattr(part, "content", str(part)))
        return "".join(chunks)
    return str(resp)


# B·∫Øt dd/mm/yyyy (c√≥ th·ªÉ k√®m gi·ªù)
_VN_DATE_RE = re.compile(
    r"(?P<d>\d{1,2})[/-](?P<m>\d{1,2})[/-](?P<y>\d{4})(?:\s+(?P<h>\d{1,2}):(?P<min>\d{2}))?"
)


def _norm_iso(s: str | None) -> str | None:
    if not s:
        return None
    s = s.strip()
    # ISO trong meta/json-ld
    try:
        from datetime import datetime

        s2 = s.replace("Z", "+00:00")
        return datetime.fromisoformat(s2).date().isoformat()
    except Exception:
        pass
    # dd/mm/yyyy (VN)
    m = _VN_DATE_RE.search(s)
    if m:
        d, mo, y = int(m.group("d")), int(m.group("m")), int(m.group("y"))
        if 1 <= d <= 31 and 1 <= mo <= 12:
            return f"{y:04d}-{mo:02d}-{d:02d}"
    return None


def extract_dates_rule_based(html: str, url: str):
    """
    Tr·∫£ v·ªÅ dict:
    {
      'published_iso': 'YYYY-MM-DD' | None,
      'modified_iso': 'YYYY-MM-DD' | None,
      'source_published_text': 'chu·ªói g·ªëc' | None
    }
    ∆Øu ti√™n: JSON-LD -> meta article:published_time -> <time datetime> -> text kh·ªëi meta g·∫ßn ti√™u ƒë·ªÅ.
    """
    soup = BeautifulSoup(html, "html.parser")
    head = soup.find("head") or soup

    # 1) JSON-LD (NewsArticle/Article)
    for s in head.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string or "")
            items = data if isinstance(data, list) else [data]
            for it in items:
                if isinstance(it, dict) and it.get("@type") in (
                    "NewsArticle",
                    "Article",
                    "Report",
                ):
                    dp = it.get("datePublished") or it.get("dateCreated")
                    dm = it.get("dateModified")
                    pub_iso = _norm_iso(dp)
                    mod_iso = _norm_iso(dm)
                    if pub_iso:
                        return {
                            "published_iso": pub_iso,
                            "modified_iso": mod_iso,
                            "source_published_text": dp,
                        }
        except Exception:
            pass

    # 2) OpenGraph/Article meta
    meta_pub = head.find("meta", {"property": "article:published_time"}) or head.find(
        "meta", {"name": "pubdate"}
    )
    if meta_pub and meta_pub.get("content"):
        pub_iso = _norm_iso(meta_pub["content"])
        meta_mod = head.find("meta", {"property": "article:modified_time"})
        mod_iso = (
            _norm_iso(meta_mod["content"])
            if meta_mod and meta_mod.get("content")
            else None
        )
        if pub_iso:
            return {
                "published_iso": pub_iso,
                "modified_iso": mod_iso,
                "source_published_text": meta_pub.get("content"),
            }

    # 3) <time datetime="...">
    t = soup.find("time", datetime=True)
    if t:
        pub_iso = _norm_iso(t.get("datetime"))
        if pub_iso:
            return {
                "published_iso": pub_iso,
                "modified_iso": None,
                "source_published_text": t.get_text(" ", strip=True)
                or t.get("datetime"),
            }

    # 4) Text g·∫ßn ti√™u ƒë·ªÅ (ph√π h·ª£p Lao ƒê·ªông / Thanh Ni√™n)
    # Gom c√°c kh·ªëi meta ph·ªï bi·∫øn quanh ti√™u ƒë·ªÅ
    meta_blocks = []
    for sel in [
        ".article__meta",
        ".meta",
        ".date",
        ".ldo-meta",
        "span.time",
        ".details__meta",
        ".details__time",
    ]:
        meta_blocks += [el.get_text(" ", strip=True) for el in soup.select(sel)]
    joined = " | ".join([b for b in meta_blocks if b])

    m = _VN_DATE_RE.search(joined)
    if m:
        d, mo, y = int(m.group("d")), int(m.group("m")), int(m.group("y"))
        pub_iso = f"{y:04d}-{mo:02d}-{d:02d}"
        return {
            "published_iso": pub_iso,
            "modified_iso": None,
            "source_published_text": joined[m.start() : m.end()],
        }

    return {"published_iso": None, "modified_iso": None, "source_published_text": None}


def extract_title_rule_based(html: str) -> str | None:
    soup = BeautifulSoup(html, "html.parser")
    # ∆Øu ti√™n th·∫ª <h1>, r·ªìi meta og:title
    h1 = soup.find("h1")
    if h1 and h1.get_text(strip=True):
        return h1.get_text(strip=True)
    og = soup.find("meta", {"property": "og:title"})
    if og and og.get("content"):
        return og["content"].strip()
    if soup.title and soup.title.get_text(strip=True):
        return soup.title.get_text(strip=True)
    return None


async def validate_and_normalize_link(
    raw_url: str, allowed_domain: str, timeout=10
) -> str | None:
    """Tr·∫£ v·ªÅ URL ƒë√£ chu·∫©n ho√° n·∫øu h·ª£p l·ªá, ng∆∞·ª£c l·∫°i None."""
    try:
        async with httpx.AsyncClient(
            follow_redirects=True,
            timeout=timeout,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123 Safari/537.36"
            },
        ) as client:
            r = await client.get(raw_url)
        if r.status_code != 200:
            return None
        ctype = r.headers.get("Content-Type", "")
        if "text/html" not in ctype:
            return None

        final_url = str(r.url)

        # B·ªè tracking params
        u = urlparse(final_url)
        q = [
            (k, v)
            for k, v in parse_qsl(u.query, keep_blank_values=True)
            if not k.lower().startswith(("utm_", "fbclid"))
        ]
        final_url = urlunparse(u._replace(query=urlencode(q, doseq=True)))

        # Canonical n·∫øu c√≥
        soup = BeautifulSoup(r.text, "html.parser")
        can = soup.find("link", rel=lambda x: x and "canonical" in x.lower())
        if can and can.get("href"):
            final_url = can["href"].strip()

        # Check domain cu·ªëi c√πng
        host = urlparse(final_url).netloc.lower().lstrip("www.")
        if host != allowed_domain.lower().lstrip("www."):
            return None

        # (tu·ª≥ ch·ªçn) basic sanity: ph·∫£i c√≥ <h1> ho·∫∑c og:title
        if not (soup.find("h1") or soup.find("meta", {"property": "og:title"})):
            return None

        return final_url
    except Exception:
        return None


# --- END: date/title/url extract helpers ---
class AgentManager:
    _instance: Optional["AgentManager"] = None

    @classmethod
    def get_instance(cls) -> "AgentManager":
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self):
        # Map (session_id, provider) -> asyncio.Task
        self._tasks: Dict[Tuple[str, str], asyncio.Task] = {}
        # Map (session_id, provider) -> cancel token/c·ªù t·ª± nguy·ªán (n·∫øu worker c·∫ßn)
        self._tokens: Dict[Tuple[str, str], asyncio.Event] = {}

    def get_token(self, session_id: str, provider: str) -> asyncio.Event:
        key = (session_id, provider)
        tok = self._tokens.get(key)
        if tok is None:
            tok = asyncio.Event()
            self._tokens[key] = tok
        return tok

    # G·ªçi khi b·∫°n kh·ªüi ch·∫°y m·ªôt provider
    def register_provider_task(
        self, session_id: str, provider: str, task: asyncio.Task
    ):
        key = (session_id, provider)
        self._tasks[key] = task
        # t·∫°o token cho cooperative cancel (n·∫øu worker check token)
        self._tokens.setdefault(key, asyncio.Event())
        logger.info("Registered provider task: %s %s", session_id, provider)

        def _cleanup(_):
            # Task k·∫øt th√∫c th√¨ d·ªçn registry
            self._tasks.pop(key, None)
            self._tokens.pop(key, None)
            logger.info("Cleaned up provider task: %s %s", session_id, provider)

        task.add_done_callback(_cleanup)

    def get_cancel_event(self, session_id: str, provider: str) -> asyncio.Event:
        # Worker c√≥ th·ªÉ g·ªçi h√†m n√†y ƒë·ªÉ l·∫•y event ki·ªÉm tra trong v√≤ng l·∫∑p
        return self._tokens.setdefault((session_id, provider), asyncio.Event())

    def cancel_provider(self, session_id: str, provider: str) -> bool:
        """
        H·ªßy provider ƒëang ch·∫°y cho session. H·ªó tr·ª£ c·∫£ hard-cancel (task.cancel)
        v√† soft-cancel (ƒë·∫∑t event ƒë·ªÉ worker t·ª± tho√°t).
        """
        key = (session_id, provider)
        task = self._tasks.get(key)
        if not task:
            return False

        # ƒê·∫∑t soft-cancel flag tr∆∞·ªõc (cooperative)
        token = self._tokens.get(key)
        if token and not token.is_set():
            token.set()
            logger.info(
                "Set cancel flag for provider %s (session %s)", provider, session_id
            )

        # Hard-cancel n·∫øu task v·∫´n ch∆∞a k·∫øt th√∫c
        if not task.done() and not task.cancelled():
            task.cancel()
            logger.info(
                "Cancelled asyncio task for provider %s (session %s)",
                provider,
                session_id,
            )
        return True

    # Tu·ª≥ b·∫°n: h·ªßy t·∫•t c·∫£ provider trong m·ªôt session
    def cancel_all_providers(self, session_id: str):
        keys = [k for k in self._tasks.keys() if k[0] == session_id]
        for _, provider in keys:
            self.cancel_provider(session_id, provider)

    def is_running(self, session_id: str, provider: str) -> bool:
        """Tr·∫£ v·ªÅ True n·∫øu provider c√≤n task ƒëang ch·∫°y."""
        return (session_id, provider) in self._tasks and not self._tasks[
            (session_id, provider)
        ].done()


class LLMUserFallbackMixin:
    async def _arun_with_user_fallback(
        self,
        prompt: str,
        *,
        session_id: str,
        preferred_provider: str | None = None,
        preferred_model: str | None = None,
    ):
        last_err = None
        providers = _configured_providers_in_order(settings, preferred_provider)
        asked = set()  # tr√°nh h·ªèi tr√πng 1 provider/model trong c√πng l∆∞·ª£t
        seq = 0
        if not providers:
            raise RuntimeError("No LLM provider configured")

        for i, prov in enumerate(providers):
            model_id = (
                preferred_model
                if prov == (preferred_provider or "").lower() and preferred_model
                else PROVIDER_MODEL_MAP.get(prov, PROVIDER_MODEL_MAP["openai"])[
                    "default"
                ]
            )
            try:
                token = AgentManager.get_instance().get_token(session_id, prov)

                # N·∫øu ƒë√£ b·ªã h·ªßy tr∆∞·ªõc khi g·ªçi
                if token.is_set():
                    raise asyncio.CancelledError()
                # y√™u c·∫ßu class c√≥ self._create_agent() v√† self.agent
                self.model = get_llm_model(prov, model_id)
                self._create_agent()
                # T·∫°o task + ƒëƒÉng k√Ω ƒë·ªÉ cancel c·ª©ng ƒë∆∞·ª£c qua AgentManager
                task = asyncio.create_task(
                    self.agent.arun(prompt, session_id=session_id)
                )
                AgentManager.get_instance().register_provider_task(
                    session_id, prov, task
                )
                return await asyncio.wait_for(task, timeout=35)

            except Exception as e:
                code, msg = _map_llm_error(e)
                last_err = e
                remaining = providers[i + 1 :]

                async with fallback_lock_by_session[session_id]:
                    # n·∫øu ƒë√£ h·ªèi provider/model n√†y r·ªìi th√¨ b·ªè qua l·∫ßn h·ªèi n·ªØa
                    if (prov, model_id) in asked:
                        continue
                    asked.add((prov, model_id))

                    if not remaining:
                        await event_bus.publish(
                            session_id,
                            {
                                "type": "provider_error",
                                "provider": prov,
                                "model": model_id,
                                "code": code,
                                "message": msg,
                                "final": True,
                                "next_options": [],
                            },
                        )
                        break

                    next_choice = remaining[0]
                    await event_bus.publish(
                        session_id,
                        {
                            "type": "propose_switch",
                            "from_provider": prov,
                            "from_model": model_id,
                            "message": msg,
                            "next_options": remaining,
                            "suggested": next_choice,
                        },
                    )

                    decision = await decision_bus.wait(session_id, timeout=15.0)

                if not decision:
                    # kh√¥ng ph·∫£n h·ªìi ‚Üí auto th·ª≠ provider k·∫ø
                    continue
                act = (decision.get("action") or "").lower()
                if act == "abort":
                    raise RuntimeError("User aborted run")
                if act == "switch":
                    picked = (decision.get("provider") or next_choice).lower()
                    if picked in remaining:
                        # ƒë∆∞a provider user ch·ªçn l√™n ti·∫øp theo
                        providers = (
                            providers[: i + 1]
                            + [picked]
                            + [x for x in remaining if x != picked]
                        )
                    continue

        raise last_err or RuntimeError("No provider available")


class HubCrawlTool(LLMUserFallbackMixin):
    def __init__(
        self,
        model,
        config,
    session_id,
        parser: ArticleParser,
        check_pause_or_cancel: Optional[callable] = None,
    ):
        self.model = model
        self.session_id = session_id
        self.config = config
        self.parser = parser
        self.check_pause_or_cancel = check_pause_or_cancel or (lambda: None)
        self.agent = Agent(
            name="HubCrawler",
            role="Web Crawler v√† Content Extractor",
            tools=[Crawl4aiTools(max_length=10000)],
            model=self.model,
            instructions="""
            B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch n·ªôi dung b√°o ch√≠ tr·ª±c tuy·∫øn, ƒë·∫∑c bi·ªát th√†nh th·∫°o trong vi·ªác nh·∫≠n di·ªán v√† x·ª≠ l√Ω c√°c ƒë·ªãnh d·∫°ng ng√†y th√°ng ƒëa d·∫°ng trong vƒÉn b·∫£n b√°o ch√≠.

            Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc v√† hi·ªÉu n·ªôi dung b√†i b√°o trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh, sau ƒë√≥ tr√≠ch xu·∫•t ch√≠nh x√°c c√°c th√¥ng tin sau:
            - Ti√™u ƒë·ªÅ b√†i b√°o  
            - Ng√†y ph√°t h√†nh b√†i b√°o
            - T√≥m t·∫Øt n·ªôi dung ch√≠nh  
            - ƒê∆∞·ªùng d·∫´n g·ªëc c·ªßa b√†i vi·∫øt

            Ch·ªâ cung c·∫•p d·ªØ li·ªáu ·ªü ƒë·ªãnh d·∫°ng JSON theo y√™u c·∫ßu. Kh√¥ng gi·∫£i th√≠ch th√™m b·∫•t k·ª≥ ƒëi·ªÅu g√¨ kh√°c.
            """,
            show_tool_calls=True,
            markdown=True,
        )

    async def run(
        self,
        media_source: MediaSource,
        keywords: List[str],
        start_date: datetime,
        end_date: datetime,
    ) -> CrawlResult:
        try:
            hub_url = get_first_search_link(media_source.domain, keywords)
            if not hub_url:
                logger.warning(
                    f"[{media_source.name}] ‚ùå Kh√¥ng t√¨m th·∫•y hub ph√π h·ª£p cho t·ª´ kh√≥a: {keywords}"
                )
                raise HubPageNotFound("System error. Please try again.")

            logger.info(f"[{media_source.name}] üîó Hub URL: {hub_url}")

            await _maybe_await(self.check_pause_or_cancel)
            html = await crawl_with_playwright(hub_url)
            soup = BeautifulSoup(html, "html.parser")

            article_links = []
            for a in soup.select("a"):
                href = a.get("href", "")
                if not href or "/video/" in href or "/tags/" in href:
                    continue
                if any(kw.lower() in href.lower() for kw in keywords):
                    full_url = (
                        href
                        if href.startswith("http")
                        else f"https://{media_source.domain}{href}"
                    )
                    article_links.append(full_url)

            valid_links = list(
                dict.fromkeys(
                    link
                    for link in article_links
                    if media_source.domain in link
                    and "admicro" not in link
                    and "adn" not in link
                )
            )

            logger.info(
                f"[{media_source.name}] üîó T√¨m ƒë∆∞·ª£c {len(valid_links)} b√†i vi·∫øt t·ª´ hub"
            )

            sem = Semaphore(4)  # tu·ª≥ quota
            start_t = time.monotonic()
            all_articles = []

            async def process_link(link):
                await _maybe_await(self.check_pause_or_cancel)

                # 1) T·ª± crawl HTML b√†i ƒë·ªÉ l·∫•y ng√†y/title rule-based (·ªïn cho Lao ƒê·ªông)
                art_html = await crawl_with_playwright(link)
                meta = extract_dates_rule_based(art_html, link)
                title_rb = extract_title_rule_based(art_html)

                # 2) Build prompt tu·ª≥ tr∆∞·ªùng h·ª£p
                if meta.get("published_iso"):
                    prompt = self.build_prompt_with_known_date(
                        link=link,
                        known_date_iso=meta["published_iso"],
                        known_date_source=meta.get("source_published_text") or "",
                        known_title=title_rb,
                    )
                else:
                    # fallback: ch∆∞a ch·∫Øc ng√†y ‚Üí d√πng prompt ƒë·∫ßy ƒë·ªß
                    prompt = self.build_prompt(link, start_date, end_date)

                # 3) G·ªçi agent
                resp = await self._arun_with_user_fallback(
                    prompt,
                    session_id=self.session_id,
                    preferred_provider=getattr(self.config, "provider", None),
                    preferred_model=getattr(self.config, "model", None),
                )
                text = await _get_response_text(resp)
                logger.info(f"... HubAgent response for {link}:\n{text[:2000]}")
                return self.parser.parse(text, media_source)

            tasks = []
            for link in valid_links[:8]:

                async def worker(l=link):
                    async with sem:
                        try:
                            return await process_link(l)
                        except Exception as e:
                            logger.warning(f"Parse fail {l}: {e}")
                            return []

                tasks.append(asyncio.create_task(worker()))

            parsed_lists = await asyncio.gather(*tasks)
            all_articles = [a for lst in parsed_lists for a in lst]

            duration = time.monotonic() - start_t

            def to_date(x):
                if isinstance(x, datetime):
                    return x.date()
                if isinstance(x, date):
                    return x
                if isinstance(x, str):
                    # ∆Øu ti√™n ISO
                    try:
                        from datetime import datetime as _dt

                        return _dt.fromisoformat(x).date()
                    except Exception:
                        m = _VN_DATE_RE.search(x)
                        if m:
                            d, mo, y = (
                                int(m.group("d")),
                                int(m.group("m")),
                                int(m.group("y")),
                            )
                            from datetime import date as _date

                            return _date(y, mo, d)
                return None

            def in_range(a: Article):
                pub = to_date(a.ngay_phat_hanh)
                return pub is not None and start_date.date() <= pub <= end_date.date()

            filtered = [a for a in all_articles if in_range(a)]

            return CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=hub_url,
                articles_found=filtered,
                crawl_status="success" if filtered else "failed",
                error_message="" if filtered else "Kh√¥ng c√≥ b√†i h·ª£p l·ªá trong hub",
                crawl_duration=duration,
            )

        except Exception as e:
            logger.error(f"[{media_source.name}] ‚ùå L·ªói crawl hub: {e}", exc_info=True)
            return CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=hub_url if "hub_url" in locals() else "",
                articles_found=[],
                crawl_status="failed",
                error_message=str(e),
                crawl_duration=duration if "duration" in locals() else 0.0,
            )

    def build_prompt_with_known_date(
        self,
        link: str,
        known_date_iso: str,
        known_date_source: str = "",
        known_title: str | None = None,
    ) -> str:
        """
        Khi ƒë√£ r√∫t ƒë∆∞·ª£c ng√†y ƒëƒÉng (ISO) b·∫±ng rule-based, kho√° ng√†y ƒë√≥ l·∫°i ƒë·ªÉ LLM kh√¥ng ƒëo√°n sai.
        """
        return f"""
        Nhi·ªám v·ª•: Truy c·∫≠p URL: {link}, ƒë·ªçc b√†i v√† TR·∫¢ V·ªÄ JSON ƒë√∫ng schema b√™n d∆∞·ªõi.
        L∆∞u √Ω: Ng√†y ph√°t h√†nh ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh ch·∫Øc ch·∫Øn t·ª´ metadata: {known_date_iso}.
        B·∫°n KH√îNG ƒë∆∞·ª£c suy ƒëo√°n hay thay ƒë·ªïi ng√†y n√†y.

        {{
        "Ti√™u ƒë·ªÅ": "{(known_title or '').replace('"','').strip()}" if empty -> tr√≠ch t·ª´ b√†i,
        "Ng√†y ph√°t h√†nh": "{known_date_iso}",
        "Ngu·ªìn tr√≠ch ng√†y": "{known_date_source.replace('"','')[:160]}",
        "T√≥m t·∫Øt": "‚â§ 100 t·ª´, n√™u s·ª± ki·ªán ch√≠nh, c√°c b√™n li√™n quan, k·∫øt qu·∫£/t√°c ƒë·ªông, kh√¥ng l·∫∑p l·∫°i ti√™u ƒë·ªÅ",
        "Link": "{link}"
        }}
        """

    def build_prompt(
        self,
        link: str,
        start_date: datetime,
        end_date: datetime,
    ) -> str:
        date_filter = f"t·ª´ ng√†y {start_date.strftime('%Y-%m-%d')} ƒë·∫øn ng√†y {end_date.strftime('%Y-%m-%d')}"
        return f"""  
        Truy c·∫≠p URL: {link} v√† ph√¢n t√≠ch b√†i b√°o. TR·∫¢ V·ªÄ DUY NH·∫§T M·ªòT OBJECT JSON theo schema d∆∞·ªõi ƒë√¢y (kh√¥ng markdown, kh√¥ng gi·∫£i th√≠ch).

        {{
        "Ti√™u ƒë·ªÅ": "Ti√™u ƒë·ªÅ ƒë·∫ßy ƒë·ªß c·ªßa b√†i vi·∫øt",
        "Ng√†y ph√°t h√†nh": "DD-MM-YYYY",
        "Ngu·ªìn tr√≠ch ng√†y": "Chu·ªói ng√†y/gi·ªù ƒë√∫ng NGUY√äN VƒÇN b·∫°n th·∫•y (v√≠ d·ª•: '31/07/2025 11:00 (GMT+7)')",
        "Ng√†y c·∫≠p nh·∫≠t": "DD-MM-YYYY ho·∫∑c null n·∫øu kh√¥ng c√≥",
        "T√≥m t·∫Øt": "‚â§ 100 t·ª´, n√™u s·ª± ki·ªán ch√≠nh, c√°c b√™n li√™n quan, k·∫øt qu·∫£/t√°c ƒë·ªông",
        "Link": "{link}"
        }}

        QUY T·∫ÆC L·∫§Y NG√ÄY:
        1) ƒê∆Ø·ª¢C PH√âP d√πng metadata: JSON-LD Article.datePublished, meta[article:published_time], <time datetime>.
        2) KH√îNG d√πng ng√†y giao di·ªán (top bar, sidebar, footer).
        3) N·∫øu c√≥ nhi·ªÅu m·ªëc (ƒëƒÉng/c·∫≠p nh·∫≠t), ∆∞u ti√™n NG√ÄY ƒêƒÇNG G·ªêC (published). Ch·ªâ ƒëi·ªÅn "Ng√†y c·∫≠p nh·∫≠t" n·∫øu t√¨m th·∫•y m·ªëc c·∫≠p nh·∫≠t.
        4) Ch·ªâ nh·∫≠n b√†i trong kho·∫£ng {date_filter}. N·∫øu ng√†y ph√°t h√†nh ngo√†i kho·∫£ng, tr·∫£ JSON nh∆∞ng ng√†y ph√°t h√†nh ph·∫£i l√† ng√†y ƒë√∫ng b·∫°n t√¨m th·∫•y (ƒë·ª´ng t·ª± ƒë·ªïi).
        5) ƒê·ªãnh d·∫°ng ng√†y b·∫Øt bu·ªôc: YYYY-MM-DD.
        """


# <--- Agent Class -->
class CrawlerAgent(LLMUserFallbackMixin):
    """Agent chuy√™n crawl web, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát."""

    def __init__(
        self,
        model: Any,
        config: CrawlConfig,
        parser: ArticleParser,
        session_id: Optional[str] = None,
        check_cancelled: Optional[callable] = None,
        check_paused: Optional[callable] = None,
        check_pause_or_cancel: Optional[callable] = None,
        user_email: Optional[str] = None,
        status: Optional[BotStatus] = None,
        on_progress_update: Optional[callable] = None,
    ):
        self.parser = parser
        self.config = config
        self.session_id = session_id
        self.search_tools = [
            ddgs_search_text,
            # ArxivTools(),
            # BaiduSearchTools(),
            # HackerNewsTools(),
            # PubmedTools(),
            # WikipediaTools(),
            # GoogleSearchWithDelay(
            #     fixed_language="vi", timeout=60, fixed_max_results=50
            # ),
        ]
        self.search_tool_index = 0
        self.model = model
        self.agent = None
        self.cache_manager = SafeCacheManager(
            cache_dir="cache/crawl_results",
            ttl_hours=self.config.cache_duration_hours,
            version="1.1",
        )
        self.check_cancelled = check_cancelled or (lambda: False)
        self.check_paused = check_paused or (lambda: False)
        self.check_pause_or_cancel = check_pause_or_cancel
        self.user_email = user_email
        self.rotate_index = 0
        self.status = status
        self.on_progress_update = on_progress_update
        self.hub_tool = HubCrawlTool(
            self.model, self.parser, self.session_id, self.config, self.check_pause_or_cancel
        )

    def return_partial_result(self, media_source: MediaSource) -> CrawlResult:
        unique_articles = {
            article.link_bai_bao: article
            for article in getattr(self, "current_articles", [])
        }
        articles = list(unique_articles.values())

        return CrawlResult(
            source_name=media_source.name,
            source_type=media_source.type,
            url=media_source.domain,
            articles_found=articles,
            crawl_status="partial" if articles else "timeout",
            error_message="Tr·∫£ v·ªÅ c√°c b√†i ƒë√£ crawl ƒë∆∞·ª£c tr∆∞·ªõc khi timeout.",
            crawl_duration=self.config.crawl_timeout,
        )

    def _create_agent(self):
        if self.agent:
            del self.agent
        current_tool = self.search_tools[self.search_tool_index]

        self.agent = Agent(
            name="MediaCrawler",
            role="Web Crawler v√† Content Extractor",
            model=self.model,
            tools=[Crawl4aiTools(max_length=2000), current_tool],
            instructions=[
                "B·∫°n l√† m·ªôt chuy√™n gia crawl web ƒë·ªÉ theo d√µi truy·ªÅn th√¥ng t·∫°i Vi·ªát Nam.",
                "Nhi·ªám v·ª•: Crawl c√°c website b√°o ch√≠ ƒë·ªÉ t√¨m b√†i vi·∫øt v·ªÅ c√°c ƒë·ªëi th·ªß c·∫°nh tranh d·ª±a tr√™n keywords, nh√£n h√†ng v√† ng√†nh h√†ng.",
                "∆Øu ti√™n tin t·ª©c m·ªõi nh·∫•t trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.",
                "Ch·ªâ l·∫•y c√°c b√†i vi·∫øt ƒë∆∞·ª£c ƒëƒÉng trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c y√™u c·∫ßu.",
                "N·∫øu kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ b√†i vi·∫øt n√†o, KH√îNG t·ª± t·∫°o n·ªôi dung, KH√îNG tr·∫£ v·ªÅ k·∫øt qu·∫£ gi·∫£, v√† ƒë·ªÉ ph·∫£n h·ªìi tr·ªëng.",
                "Kh√¥ng l·∫•y c√°c b√†i vi·∫øt ƒëƒÉng tr∆∞·ªõc ho·∫∑c sau kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh.",
                "Tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON h·ª£p l·ªá ch·ª©a danh s√°ch b√†i b√°o v·ªõi c√°c tr∆∞·ªùng: ti√™u ƒë·ªÅ, ng√†y ph√°t h√†nh (DD-MM-YYYY), t√≥m t·∫Øt n·ªôi dung, link b√†i b√°o.",
            ],
            show_tool_calls=True,
            markdown=True,
            add_datetime_to_instructions=True,
        )

    # def _rotate_tool(self):
    #     self.search_tool_index = (self.search_tool_index + 1) % len(self.search_tools)
    #     self._create_agent()

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type(
            (APITimeoutError, httpx.ConnectTimeout, httpx.ReadTimeout)
        ),
        reraise=True,
    )
    async def crawl_media_source(
        self,
        media_source: MediaSource,
        industry_name: str,
        keywords: List[str],
        start_date: datetime,
        end_date: datetime,
    ) -> CrawlResult:
        start_time = datetime.now()
        date_filter = f"t·ª´ ng√†y {start_date.strftime('%Y-%m-%d')} ƒë·∫øn ng√†y {end_date.strftime('%Y-%m-%d')}"
        domain_url = media_source.domain
        if domain_url and not domain_url.startswith("http"):
            domain_url = f"https://{domain_url}"

        def to_date(dt):
            return dt.date() if isinstance(dt, datetime) else dt

        keyword_groups = [[kw] for kw in keywords]
        articles: List[Article] = []
        self.current_articles = []

        # Kh√¥i ph·ª•c checkpoint n·∫øu c√≥
        task = next(
            (
                t
                for t in task_manager.get_tasks(self.user_email)
                if t["session_id"] == self.session_id
            ),
            None,
        )
        checkpoint = (
            task.get("crawl_checkpoint", {}).get(media_source.name, {}) if task else {}
        )
        tools_to_try = checkpoint.get("tool_order")
        if tools_to_try is None:
            base_order = list(range(len(self.search_tools)))
            start_tool_index = self.rotate_index
            self.rotate_index = (self.rotate_index + 1) % len(self.search_tools)
            tools_to_try = base_order[start_tool_index:] + base_order[:start_tool_index]

        start_group_index = checkpoint.get("group_index", 0)
        start_tool_index = checkpoint.get("tool_index", 0)

        try:
            # L·∫∂P T·ª™NG KEYWORD
            for g_idx in range(start_group_index, len(keyword_groups)):
                await _maybe_await(self.check_pause_or_cancel)
                group = keyword_groups[g_idx]  # v√≠ d·ª• ["T∆∞·ªùng An"]
                kw_str = ", ".join(group)

                # progress text r√µ keyword
                if self.on_progress_update:
                    self.on_progress_update(
                        source_name=media_source.name,
                        completed=self.status.completed_sources,
                        failed=self.status.failed_sources,
                        progress=(
                            (self.status.completed_sources + self.status.failed_sources)
                            / max(1, self.status.total_sources)
                        )
                        * 50.0,
                        current_task=f"Crawling {media_source.name} ({industry_name}) ‚Äì keyword: {kw_str}",
                    )
                found_for_this_keyword = False

                if self.config.use_hub_page:
                    try:
                        result = await self.hub_tool.run(
                            media_source, group, start_date, end_date
                        )
                        if result and result.articles_found:
                            self.current_articles = result.articles_found.copy()
                            self.articles_so_far = result.articles_found.copy()
                            found_for_this_keyword = True
                            return result
                    except HubPageNotFound as e:
                        logger.warning(f"[{media_source.name}] ‚ö†Ô∏è {str(e)}")
                        return CrawlResult(
                            source_name=media_source.name,
                            source_type=media_source.type,
                            url=media_source.domain,
                            articles_found=[],
                            crawl_status="failed",
                            error_message=str(e),
                            crawl_duration=0.0,
                        )
                    except Exception as e:
                        logger.warning(
                            f"[{media_source.name}] ‚ö†Ô∏è L·ªói hub cho keyword '{keywords_str}': {e}"
                        )
                # Use cache if true
                # if self.config.use_cache:
                #     cache_key = self.cache_manager.make_cache_key(
                #         media_source.name,
                #         industry_name,
                #         keywords,
                #         start_date,
                #         end_date,
                #     )
                #     cached_data = self.cache_manager.load_cache(cache_key)

                #     if cached_data:
                #         logger.info(f"[{media_source.name}] ‚úÖ Loaded from cache.")
                #         return CrawlResult(**cached_data)

                # max_keywords_per_query = 3
                # keyword_groups = [
                #     keywords[i : i + max_keywords_per_query]
                #     for i in range(0, len(keywords), max_keywords_per_query)
                # ]

                if not found_for_this_keyword:
                    for i in range(start_tool_index, len(tools_to_try)):
                        await _maybe_await(self.check_pause_or_cancel)
                        tool_index = tools_to_try[i]
                        tool_name = type(self.search_tools[tool_index]).__name__

                        self.search_tool_index = tool_index
                        self._create_agent()

                        new_articles_this_tool = 0
                        self.current_articles = []

                        for group_index in range(
                            start_group_index, len(keyword_groups)
                        ):
                            await _maybe_await(self.check_pause_or_cancel)
                            group = keyword_groups[group_index]
                            keywords_str = ", ".join(group)

                            # Update progress
                            if self.on_progress_update:
                                flat_keywords = keywords_str
                                self.on_progress_update(
                                    source_name=media_source.name,
                                    completed=self.status.completed_sources,
                                    failed=self.status.failed_sources,
                                    progress=(
                                        (
                                            self.status.completed_sources
                                            + self.status.failed_sources
                                        )
                                        / self.status.total_sources
                                    )
                                    * 50.0,
                                    current_task=f"Crawling {media_source.name} ({industry_name}) ‚Äì t·ª´ kh√≥a: {flat_keywords}",
                                )

                            query_variants = [
                                # f"C√¥ng ty {industry_name} {keywords_str} site:{media_source.domain} th√°ng {start_date.month} {start_date.year}",
                                f"{keywords_str} {industry_name} tin t·ª©c m·ªõi nh·∫•t th√°ng {start_date.month} {start_date.year} site:{media_source.domain}",
                                f"site:{media_source.domain} {keywords_str} th√°ng {start_date.month} {start_date.year}",
                            ]
                            query_lines = "\n".join([f"- {q}" for q in query_variants])

                            crawl_query = f"""
                            Crawl website: {domain_url or media_source.name}
                            T√¨m c√°c b√†i b√°o tr√™n {domain_url or media_source.name} c√≥ ch·ª©a c√°c t·ª´ kh√≥a: {keywords_str} trong ti√™u ƒë·ªÅ ho·∫∑c trong b√†i vi·∫øt v√† PH·∫¢I li√™n quan ƒë·∫øn ng√†nh h√†ng: {industry_name}
                            Th·ªùi gian: {date_filter}
                            B·∫°n n√™n th·ª≠ t·∫•t c·∫£ c√°c c√¢u truy v·∫•n sau:
                            {query_lines}
                            Y√™u c·∫ßu:
                            - Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ, t√≥m t·∫Øt, ng√†y ph√°t h√†nh, link g·ªëc.
                            - Ng√†y ph√°t h√†nh (ngay_phat_hanh) ph·∫£i l√† ng√†y ƒë∆∞·ª£c ghi trong n·ªôi dung b√†i vi·∫øt.
                            - Tuy·ªát ƒë·ªëi KH√îNG ƒë∆∞·ª£c l·∫•y c√°c ng√†y n·∫±m ·ªü ph·∫ßn **header**, **menu**, **sidebar**, hay **g√≥c tr√™n c√πng c·ªßa trang** (v√¨ ƒë√≥ l√† ng√†y hi·ªán t·∫°i hi·ªÉn th·ªã giao di·ªán, KH√îNG ph·∫£i ng√†y ƒëƒÉng b√†i vi·∫øt).
                            - Ng√†y ph√°t h√†nh ph·∫£i:
                                + Xu·∫•t hi·ªán b√™n trong n·ªôi dung b√†i vi·∫øt.
                                + C√≥ th·ªÉ n·∫±m d∆∞·ªõi ti√™u ƒë·ªÅ, g·∫ßn t√™n t√°c gi·∫£, ho·∫∑c cu·ªëi b√†i vi·∫øt.
                                + N·∫øu c√≥ nhi·ªÅu ng√†y trong n·ªôi dung, b·∫°n PH·∫¢I ch·ªçn ng√†y:
                                    - C√≥ ƒë·ªãnh d·∫°ng h·ª£p l·ªá (v√≠ d·ª•: 31/07/2025, 2025-07-31, ho·∫∑c c√≥ th√™m gi·ªù)
                                    - Kh√°c v·ªõi ng√†y ƒë·∫ßu trang
                                    - L√† ng√†y nh·ªè h∆°n (s·ªõm h∆°n)
                            - C√°c v√≠ d·ª•:
                                ‚úÖ ƒê√∫ng: "31/07/2025 17:25" n·∫±m g·∫ßn t√°c gi·∫£ ho·∫∑c cu·ªëi b√†i vi·∫øt.
                                ‚ùå Sai: "Th·ª© Ba, ng√†y 05/08/2025" n·∫±m ·ªü ƒë·∫ßu trang, trong header.
                            - Ch·ªâ l·∫•y b√†i vi·∫øt li√™n quan ƒë·∫øn ng√†nh h√†ng v√† t·ª´ kh√≥a.
                            - QUY T·∫ÆC NO-RESULT:
                                + N·∫øu danh s√°ch b√†i === r·ªóng ‚Üí tr·∫£ v·ªÅ JSON m·∫£ng r·ªóng: []
                                + N·∫øu b√†i n√†o c√≥ "text" < 500 k√Ω t·ª± ‚Üí b·ªè b√†i ƒë√≥ (kh√¥ng t√≥m t·∫Øt)
                                + Tuy·ªát ƒë·ªëi KH√îNG t·∫°o b√†i khi kh√¥ng c√≥ d·ªØ li·ªáu.
                            - T·∫°o t√≥m t·∫Øt chi ti·∫øt (d∆∞·ªõi 100 t·ª´), n√™u b·∫≠t c√°c th√¥ng tin ch√≠nh nh∆∞: s·ª± ki·ªán ch√≠nh, c√°c b√™n li√™n quan, v√† k·∫øt qu·∫£ ho·∫∑c t√°c ƒë·ªông c·ªßa s·ª± ki·ªán. Kh√¥ng ch·ªâ l·∫∑p l·∫°i ti√™u ƒë·ªÅ.
                            - Format: Ti√™u ƒë·ªÅ | Ng√†y ph√°t h√†nh | T√≥m t·∫Øt | Link
                            """
                            logger.info(
                                f"[{media_source.name}] Using {tool_name} for group {group_index + 1}/{len(keyword_groups)}"
                            )

                            try:
                                logger.info(
                                    f"[{media_source.name}] Searching with keywords: {keywords_str}"
                                )
                                response = await self._arun_with_user_fallback(
                                    crawl_query,
                                    session_id=self.session_id,
                                    preferred_provider=getattr(
                                        self.config, "provider", None
                                    ),
                                    preferred_model=getattr(self.config, "model", None),
                                )
                                logger.info(
                                    f"[DEBUG] Raw response content:\n{response.content}"
                                )
                                await asyncio.sleep(1)

                                if response and response.content:
                                    if (
                                        "enable javascript" in response.content.lower()
                                        or "captcha" in response.content.lower()
                                    ):
                                        logger.warning(
                                            f"[{media_source.name}] Blocked or captcha required for {tool_name}"
                                        )
                                        continue

                                    parsed_articles = self.parser.parse(
                                        response.content, media_source
                                    )

                                    filtered_articles = [
                                        a
                                        for a in parsed_articles
                                        if not any(
                                            exclude in a.link_bai_bao
                                            for exclude in self.config.exclude_domains
                                        )
                                    ]

                                    valid_new_articles = []
                                    seen_links = set()
                                    for a in filtered_articles:
                                        if (
                                            not a
                                            or not a.link_bai_bao
                                            or not a.ngay_phat_hanh
                                        ):
                                            continue

                                        # Ki·ªÉm tra ng√†y
                                        pub_date = to_date(a.ngay_phat_hanh)
                                        if not (
                                            to_date(start_date)
                                            <= pub_date
                                            <= to_date(end_date)
                                        ):
                                            continue

                                        # Ki·ªÉm tra tr√πng link (sau khi ch·∫Øc ch·∫Øn l√† b√†i h·ª£p l·ªá)
                                        if a.link_bai_bao in seen_links:
                                            continue
                                        norm = await validate_and_normalize_link(
                                            a.link_bai_bao, media_source.domain
                                        )
                                        if not norm:
                                            # log l√Ω do lo·∫°i n·∫øu mu·ªën
                                            continue

                                        if norm in seen_links:
                                            continue
                                        a.link_bai_bao = norm
                                        seen_links.add(a.link_bai_bao)

                                        valid_new_articles.append(a)

                                    if valid_new_articles:
                                        articles.extend(valid_new_articles)
                                        self.current_articles.extend(valid_new_articles)
                                        new_articles_this_tool += len(
                                            valid_new_articles
                                        )

                                        logger.info(
                                            f"[{media_source.name}] Found {len(valid_new_articles)} articles with {tool_name} for keywords: {keywords_str}"
                                        )
                                        logger.debug(
                                            f"[{media_source.name}] Total articles so far: {len(articles)}"
                                        )
                                    else:
                                        logger.warning(
                                            f"[{media_source.name}] No articles parsed from {tool_name} for keywords: {keywords_str}"
                                        )
                                else:
                                    logger.warning(
                                        f"[{media_source.name}] No response from {tool_name} for keywords: {keywords_str}"
                                    )

                            except (
                                httpx.HTTPStatusError,
                                httpx.RequestError,
                                APITimeoutError,
                                ValueError,
                            ) as e:
                                logger.warning(
                                    f"[{media_source.name}] Error in {tool_name} for keywords {keywords_str}: {e}"
                                )
                                await asyncio.sleep(1)
                            except Exception as e:
                                logger.error(
                                    f"[{media_source.name}] Unexpected error in {tool_name} for keywords {keywords_str}: {e}",
                                    exc_info=True,
                                )
                                await asyncio.sleep(1)

                            task_manager.update_task(
                                self.user_email,
                                self.session_id,
                                {
                                    "articles_so_far": [
                                        a.model_dump(mode="json") for a in articles
                                    ],
                                    "crawl_checkpoint": {
                                        media_source.name: {
                                            "tool_order": tools_to_try,
                                            "tool_index": i,
                                            "group_index": group_index + 1,
                                        }
                                    },
                                },
                            )

                            gc.collect()

                        if new_articles_this_tool > 0:
                            logger.info(
                                f"[{media_source.name}] Stopping search as articles were found by {tool_name}"
                            )
                            break

                    unique_articles = {
                        article.link_bai_bao: article
                        for article in articles
                        if article.link_bai_bao
                    }
                    articles = list(unique_articles.values())
                    logger.info(
                        f"[{media_source.name}] Crawled {len(articles)} unique articles"
                    )

                    result = CrawlResult(
                        source_name=media_source.name,
                        source_type=media_source.type,
                        url=media_source.domain,
                        articles_found=articles,
                        crawl_status="success" if articles else "failed",
                        error_message=(
                            ""
                            if articles
                            else f"Th·ª≠ h·∫øt {len(self.search_tools)} search tool nh∆∞ng kh√¥ng t√¨m th·∫•y b√†i b√°o"
                        ),
                        crawl_duration=(datetime.now() - start_time).total_seconds(),
                    )

                    # if (
                    #     self.config.use_cache
                    #     and result.crawl_status == "success"
                    #     and len(result.articles_found) > 0
                    # ):
                    #     self.cache_manager.save_cache(cache_key, result)

                    # return result

        except Exception as e:
            logger.error(f"[{media_source.name}] Crawl failed: {e}", exc_info=True)
            return CrawlResult(
                source_name=media_source.name,
                source_type=media_source.type,
                url=media_source.domain,
                articles_found=[],
                crawl_status="failed",
                error_message=f"Crawl failed: {str(e)}",
                crawl_duration=(datetime.now() - start_time).total_seconds(),
            )
        finally:
            gc.collect()

    def close_final(self):
        logger.info("üßπ ƒê√≥ng ho√†n to√†n CrawlerAgent, gi·∫£i ph√≥ng agent v√† tools.")
        self.agent = None
        for tool in self.search_tools:
            if hasattr(tool, "close"):
                try:
                    tool.close()
                except Exception as e:
                    logger.warning(f"Tool {tool} ƒë√≥ng kh√¥ng th√†nh c√¥ng: {e}")
        gc.collect()


class ProcessorAgent(LLMUserFallbackMixin):
    """Agent chuy√™n x·ª≠ l√Ω v√† ph√¢n t√≠ch n·ªôi dung, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát chi ti·∫øt."""

    def __init__(self, model: Any):
        self.agent = Agent(
            name="ContentProcessor",
            role="Chuy√™n gia Ph√¢n t√≠ch v√† Ph√¢n lo·∫°i N·ªôi dung",
            model=model,
            instructions=[
                "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch n·ªôi dung truy·ªÅn th√¥ng cho ng√†nh FMCG t·∫°i Vi·ªát Nam.",
                "Nhi·ªám v·ª• c·ªßa b·∫°n: Ph√¢n t√≠ch v√† ph√¢n lo·∫°i c√°c b√†i b√°o theo ng√†nh h√†ng v√† nh√£n hi·ªáu.",
                "B·∫°n B·∫ÆT BU·ªòC ph·∫£i tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch JSON (JSON list) h·ª£p l·ªá c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng Article ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß.",
            ],
            markdown=True,
        )

    def extract_json(self, text: str) -> str:
        """
        Tr√≠ch xu·∫•t JSON t·ª´ ph·∫£n h·ªìi LLM v·ªõi th·ª© t·ª± ∆∞u ti√™n:
        1. T√¨m ƒëo·∫°n trong ```json ... ```
        2. N·∫øu kh√¥ng c√≥, b·ªè d·∫•u '...' ngo√†i c√πng (n·∫øu c√≥)
        3. T√¨m ƒëo·∫°n JSON {...} ho·∫∑c [...]
        """

        text = text.strip()

        # 1Ô∏è. ∆Øu ti√™n t√¨m ```json ... ```
        matches = re.findall(r"```json(.*?)```", text, re.DOTALL)
        if matches:
            for match in matches:
                match = match.strip()
                try:
                    json.loads(match)
                    return match  # ‚úÖ Tr·∫£ v·ªÅ JSON ƒë√∫ng lu√¥n
                except json.JSONDecodeError:
                    continue

        # 2. N·∫øu kh√¥ng c√≥, x·ª≠ l√Ω d·∫•u '...' ngo√†i c√πng
        if text.startswith("'") and text.endswith("'"):
            text = text[1:-1].strip()

        # 3Ô∏è. T√¨m ƒëo·∫°n JSON {...} ho·∫∑c [...]
        candidates = re.findall(r"(\{.*?\}|\[.*?\])", text, re.DOTALL)
        for candidate in candidates:
            candidate = candidate.strip()
            try:
                json.loads(candidate)
                return candidate  # ‚úÖ Tr·∫£ v·ªÅ JSON ƒë√∫ng
            except json.JSONDecodeError:
                continue

        # N·∫øu kh√¥ng t√¨m th·∫•y
        raise ValueError("Kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá trong ph·∫£n h·ªìi.")

    async def process_articles(
        self, raw_articles: List[Article], keywords_config: Dict[str, List[str]]
    ) -> List[Article]:
        """
        Processes a list of raw articles to classify industries, brands, content clusters,
        and extract summaries and keywords. Optimizes memory usage by processing articles in batches.

        Args:
            raw_articles: List of raw Article objects to process.
            keywords_config: Dictionary mapping industries to lists of keywords.

        Returns:
            List of processed Article objects.
        """
        if not raw_articles:
            logger.info("No articles to process.")
            return []

        processed_articles = []
        batch_size = 10  # Process 20 articles per batch to reduce memory usage

        try:
            # Extract brand list (uppercase keywords) for competitor identification
            brand_list = list(
                set(
                    kw
                    for kws in keywords_config.values()
                    for kw in kws
                    if kw[0].isupper()
                )
            )

            # Define content clusters with associated keywords
            content_clusters = {
                ContentCluster.HOAT_DONG_DOANH_NGHIEP: [
                    "s·∫£n xu·∫•t",
                    "nh√† m√°y",
                    "tuy·ªÉn d·ª•ng",
                    "doanh nghi·ªáp",
                    "ho·∫°t ƒë·ªông",
                    "ƒë·∫ßu t∆∞",
                ],
                ContentCluster.CHUONG_TRINH_CSR: [
                    "t√†i tr·ª£",
                    "m√¥i tr∆∞·ªùng",
                    "c·ªông ƒë·ªìng",
                    "CSR",
                    "t·ª´ thi·ªán",
                ],
                ContentCluster.MARKETING_CAMPAIGN: [
                    "truy·ªÅn th√¥ng",
                    "KOL",
                    "khuy·∫øn m√£i",
                    "qu·∫£ng c√°o",
                    "chi·∫øn d·ªãch",
                ],
                ContentCluster.PRODUCT_LAUNCH: [
                    "s·∫£n ph·∫©m m·ªõi",
                    "bao b√¨",
                    "c√¥ng th·ª©c",
                    "ra m·∫Øt",
                    "ph√°t h√†nh",
                ],
                ContentCluster.PARTNERSHIP: [
                    "MOU",
                    "li√™n doanh",
                    "k√Ω k·∫øt",
                    "h·ª£p t√°c",
                    "ƒë·ªëi t√°c",
                ],
                ContentCluster.FINANCIAL_REPORT: [
                    "l·ª£i nhu·∫≠n",
                    "doanh thu",
                    "tƒÉng tr∆∞·ªüng",
                    "b√°o c√°o t√†i ch√≠nh",
                    "k·∫øt qu·∫£ kinh doanh",
                ],
                ContentCluster.FOOD_SAFETY: [
                    "an to√†n th·ª±c ph·∫©m",
                    "ATTP",
                    "ng·ªô ƒë·ªôc",
                    "nhi·ªÖm khu·∫©n",
                    "thu h·ªìi s·∫£n ph·∫©m",
                    "ch·∫•t c·∫•m",
                    "ki·ªÉm tra ATTP",
                    "thanh tra an to√†n th·ª±c ph·∫©m",
                    "truy xu·∫•t ngu·ªìn g·ªëc",
                    "blockchain th·ª±c ph·∫©m",
                    "tem QR",
                    "chu·ªói cung ·ª©ng s·∫°ch",
                    "cam k·∫øt ch·∫•t l∆∞·ª£ng th·ª±c ph·∫©m",
                    "quy ƒë·ªãnh an to√†n th·ª±c ph·∫©m",
                    "x·ª≠ ph·∫°t vi ph·∫°m ATTP",
                    "s·ª©c kh·ªèe",
                    "thu h·ªìi s·∫£n ph·∫©m",
                ],
                ContentCluster.OTHER: [],
            }

            # Process articles in batches
            for i in range(0, len(raw_articles), batch_size):
                batch = raw_articles[i : i + batch_size]
                logger.info(
                    f"Processing batch {i // batch_size + 1} with {len(batch)} articles"
                )

                # Create analysis prompt for the batch
                analysis_prompt = f"""
                Ph√¢n t√≠ch v√† ph√¢n lo·∫°i {len(batch)} b√†i b√°o sau ƒë√¢y.

                Danh s√°ch c√°c nh√£n h√†ng ƒë·ªëi th·ªß c·∫ßn x√°c ƒë·ªãnh:
                {json.dumps(brand_list, ensure_ascii=False, indent=2)}
                
                Keywords config:
                {json.dumps(keywords_config, ensure_ascii=False, indent=2)}
                
                Raw articles:
                {json.dumps([a.model_dump(mode='json') for a in batch], ensure_ascii=False, indent=2)}
                
                Y√™u c·∫ßu:
                1. Ph√¢n lo·∫°i ch√≠nh x√°c ng√†nh h√†ng cho t·ª´ng b√†i (d·ª±a theo b·ªëi c·∫£nh b√†i v√† danh s√°ch nh√£n h√†ng ng√†nh h√†ng t∆∞∆°ng ·ª©ng).
                2. Tr√≠ch xu·∫•t `nhan_hang`:
                    - ƒê·ªçc n·ªôi dung b√†i vi·∫øt v√† ki·ªÉm tra xem c√≥ nh√£n h√†ng n√†o trong danh s√°ch sau xu·∫•t hi·ªán hay kh√¥ng: 
                    {json.dumps(brand_list, ensure_ascii=False, indent=2)}
                    - Ch·ªâ ghi nh·∫≠n nh·ªØng nh√£n h√†ng th·ª±c s·ª± xu·∫•t hi·ªán trong b√†i vi·∫øt (b·∫•t k·ªÉ vi·∫øt hoa hay vi·∫øt th∆∞·ªùng).
                    - N·∫øu kh√¥ng th·∫•y nh√£n h√†ng n√†o th√¨ ƒë·ªÉ `nhan_hang` l√† `[]`. Kh√¥ng t·ª± b·ªãa ho·∫∑c t·ª± suy ƒëo√°n th√™m.
                3. Ph√¢n lo·∫°i l·∫°i c·ª•m n·ªôi dung (`cum_noi_dung`). N·∫øu b√†i vi·∫øt c√≥ n·ªôi dung t∆∞∆°ng ƒë∆∞∆°ng, ƒë·ªìng nghƒ©a ho·∫∑c g·∫ßn gi·ªëng v·ªõi c√°c c·ª•m t·ª´ kh√≥a: {json.dumps({k.value: v for k, v in content_clusters.items()}, ensure_ascii=False, indent=2)}, h√£y ph√¢n lo·∫°i v√†o c·ª•m ƒë√≥.
                4. N·∫øu kh√¥ng t√¨m th·∫•y c·ª•m n·ªôi dung n√†o kh·ªõp v·ªõi danh s√°ch t·ª´ kh√≥a c·ª•m n·ªôi dung, B·∫ÆT BU·ªòC g√°n tr∆∞·ªùng (`cum_noi_dung`) l√† '{ContentCluster.OTHER.value}', KH√îNG ƒê∆Ø·ª¢C ƒë·ªÉ tr·ªëng ho·∫∑c tr·∫£ v·ªÅ none hay null.
                5. Vi·∫øt l·∫°i n·ªôi dung chi ti·∫øt, ng·∫Øn g·ªçn v√† mang t√≠nh m√¥ t·∫£ kh√°i qu√°t cho tr∆∞·ªùng `cum_noi_dung_chi_tiet` d·ª±a tr√™n n·ªôi dung ƒë√£ c√≥ s·∫µn:
                    - L√† 1 d√≤ng m√¥ t·∫£ ng·∫Øn (~10‚Äì20 t·ª´) cho b√†i b√°o, c√≥ c·∫•u tr√∫c:  
                    `[Lo·∫°i th√¥ng tin]: [T√≥m t·∫Øt n·ªôi dung n·ªïi b·∫≠t]`
                    - V√≠ d·ª•: "Th√¥ng tin doanh nghi·ªáp: T∆∞·ªùng An kh·∫≥ng ƒë·ªãnh v·ªã th·∫ø d·ªãp T·∫øt 2025"
                6. Tr√≠ch xu·∫•t v√† ghi v√†o `keywords_found`:
                    - L√† t·∫•t c·∫£ c√°c t·ª´ kh√≥a ng√†nh li√™n quan th·ª±c s·ª± xu·∫•t hi·ªán trong b√†i vi·∫øt.
                    - Ch·ªâ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ c√°c t·ª´ kh√≥a ƒë√£ cung c·∫•p trong `keywords_config`.
                    - N·∫øu kh√¥ng t√¨m th·∫•y t·ª´ kh√≥a n√†o, ƒë·ªÉ `keywords_found` l√† []
                7. Ch·ªâ gi·ªØ b√†i vi·∫øt li√™n quan ƒë·∫øn ng√†nh FMCG (D·∫ßu ƒÉn, Gia v·ªã, S·ªØa, v.v.) d·ª±a tr√™n t·ª´ kh√≥a trong `keywords_config`. Lo·∫°i b·ªè b√†i kh√¥ng li√™n quan (e.g., ch√≠nh tr·ªã, s·ª©c kh·ªèe kh√¥ng li√™n quan).
                8. ƒê·ªãnh d·∫°ng ng√†y ph√°t h√†nh b·∫Øt bu·ªôc: dd/mm/yyyy (VD: 01/07/2025)"
                9. N·∫øu m·ªôt b√†i b√°o ƒë·ªÅ c·∫≠p nhi·ªÅu nh√£n h√†ng th√¨ ghi t·∫•t c·∫£ nh√£n h√†ng trong danh s√°ch `nhan_hang`.
                10. N·∫øu b√†i li√™n quan nhi·ªÅu ng√†nh (v√≠ d·ª• s·∫£n ph·∫©m ƒëa d·ª•ng), h√£y ch·ªçn ng√†nh ch√≠nh nh·∫•t li√™n quan ƒë·∫øn b·ªëi c·∫£nh.
                11. Gi·ªØ nguy√™n `tom_tat_noi_dung`, kh√¥ng c·∫Øt b·ªõt, sinh ra hay thay ƒë·ªïi n·ªôi dung.

                ƒê·ªãnh d·∫°ng ƒë·∫ßu ra:
                Tr·∫£ v·ªÅ m·ªôt danh s√°ch JSON h·ª£p l·ªá ch·ª©a c√°c ƒë·ªëi t∆∞·ª£ng Article ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω. C·∫•u tr√∫c JSON c·ªßa m·ªói ƒë·ªëi t∆∞·ª£ng ph·∫£i kh·ªõp v·ªõi Pydantic model. ƒê√¢y l√† 1 v√≠ d·ª• cho b·∫°n l√†m m·∫´u:
                [
                    {{
                        "stt": 1,
                        "ngay_phat_hanh": "01/07/2025",
                        "dau_bao": "VNEXPRESS",
                        "cum_noi_dung": "Chi·∫øn d·ªãch Marketing",
                        "cum_noi_dung_chi_tiet": "Chi·∫øn d·ªãch T·∫øt 2025 c·ªßa Vinamilk chinh ph·ª•c ng∆∞·ªùi ti√™u d√πng tr·∫ª",
                        "tom_tat_noi_dung": "Vinamilk tung chi·∫øn d·ªãch T·∫øt 2025...",
                        "link_bai_bao": "https://vnexpress.net/...",
                        "nganh_hang": "S·ªØa (UHT)",
                        "nhan_hang": ["Vinamilk"],
                        "keywords_found": ["T·∫øt", "TV qu·∫£ng c√°o", "Vinamilk"]
                    }}
                ]
                [
                    {{
                        "stt": 2,
                        "ngay_phat_hanh": "06/07/2025",
                        "dau_bao": "Thanh Nien",
                        "cum_noi_dung": "Ho·∫°t ƒë·ªông doanh nghi·ªáp v√† th√¥ng tin s·∫£n ph·∫©m",
                        "cum_noi_dung_chi_tiet": "Th√¥ng tin doanh nghi·ªáp: T∆∞·ªùng An v√† Coba m·ªü r·ªông th·ªã ph·∫ßn d·∫ßu ƒÉn mi·ªÅn T√¢y",
                        "tom_tat_noi_dung": "T∆∞·ªùng An v√† Coba tƒÉng c∆∞·ªùng ƒë·∫ßu t∆∞ v√† ph√¢n ph·ªëi s·∫£n ph·∫©m d·∫ßu ƒÉn t·∫°i khu v·ª±c mi·ªÅn T√¢y Nam B·ªô.",
                        "link_bai_bao": "https://thanhnien.vn/tuong-an-coba-dau-an",
                        "nganh_hang": "D·∫ßu ƒÉn",
                        "nhan_hang": ["T∆∞·ªùng An", "Coba"],
                        "keywords_found": ["T∆∞·ªùng An", "Coba", "d·∫ßu ƒÉn", "th·ªã ph·∫ßn"]
                    }}
                ]
                [
                    {{
                        "stt": 3,
                        "ngay_phat_hanh": "07/07/2025",
                        "dau_bao": "Tuoi Tre",
                        "cum_noi_dung": "Ch∆∞∆°ng tr√¨nh CSR",
                        "cum_noi_dung_chi_tiet": "CSR: Doanh nghi·ªáp ƒë·ªãa ph∆∞∆°ng h·ªó tr·ª£ c·ªông ƒë·ªìng mi·ªÅn n√∫i",
                        "tom_tat_noi_dung": "M·ªôt s·ªë doanh nghi·ªáp ƒë·ªãa ph∆∞∆°ng ph·ªëi h·ª£p t·ªï ch·ª©c ch∆∞∆°ng tr√¨nh h·ªó tr·ª£ b√† con mi·ªÅn n√∫i m√πa m∆∞a l≈©.",
                        "link_bai_bao": "https://tuoitre.vn/csr-mien-nui",
                        "nganh_hang": "D·∫ßu ƒÉn",
                        "nhan_hang": [],
                        "keywords_found": ["h·ªó tr·ª£ c·ªông ƒë·ªìng", "CSR", "mi·ªÅn n√∫i"]
                    }}
                ]
                """

                try:
                    response = await self._arun_with_user_fallback(
                        analysis_prompt,
                        session_id=self.session_id,
                        preferred_provider=getattr(self.config, "provider", None),
                        preferred_model=getattr(self.config, "model", None),
                    )
                    logger.debug(f"LLM raw output:\n{response.content}")

                    if response and response.content:
                        try:
                            raw_json = self.extract_json(response.content)
                            articles_data = json.loads(raw_json)

                            km = KeywordManager(
                                CONFIG_DIR / "content_cluster_keywords.json"
                            )
                            valid_clusters = [c.value for c in ContentCluster]

                            for item in articles_data:
                                text_to_check = (
                                    item.get("tom_tat_noi_dung", "")
                                    + " "
                                    + (item.get("cum_noi_dung_chi_tiet", "") or "")
                                ).lower()

                                # Fallback c·ª•m n·ªôi dung n·∫øu c·∫ßn
                                if (
                                    item.get("cum_noi_dung")
                                    in [None, "null", "", "Kh√°c"]
                                    or item.get("cum_noi_dung") not in valid_clusters
                                ):
                                    text_to_check = (
                                        item.get("tom_tat_noi_dung", "")
                                        + " "
                                        + (item.get("cum_noi_dung_chi_tiet", "") or "")
                                    ).strip()
                                    fallback_cluster = km.map_to_cluster(text_to_check)
                                    item["cum_noi_dung"] = fallback_cluster

                                # Fallback keywords_found
                                if (
                                    "keywords_found" not in item
                                    or not item["keywords_found"]
                                ):
                                    item["keywords_found"] = []
                                    for industry, keywords in keywords_config.items():
                                        for kw in keywords:
                                            if kw.lower() in text_to_check:
                                                item["keywords_found"].append(kw)
                                    # Lo·∫°i tr√πng
                                    item["keywords_found"] = list(
                                        set(item["keywords_found"])
                                    )

                                # Fallback nh√£n h√†ng
                                if "nhan_hang" not in item or not item["nhan_hang"]:
                                    item["nhan_hang"] = []
                                    for brand in brand_list:
                                        if brand.lower() in text_to_check:
                                            item["nhan_hang"].append(brand)
                                    # Lo·∫°i tr√πng
                                    item["nhan_hang"] = list(set(item["nhan_hang"]))

                            processed_articles.extend(
                                [Article(**item) for item in articles_data]
                            )
                            logger.info(
                                f"Batch {i // batch_size + 1} processed: {len(articles_data)} articles"
                            )

                        except (json.JSONDecodeError, TypeError) as e:
                            logger.error(
                                f"Failed to parse JSON response for batch {i // batch_size + 1}: {e}"
                            )
                            # Fallback: g√°n c·ª•m n·ªôi dung b·∫±ng KeywordManager khi l·ªói
                            km = KeywordManager(
                                CONFIG_DIR / "content_cluster_keywords.json"
                            )
                            for article in batch:
                                text = (
                                    article.tom_tat_noi_dung
                                    + " "
                                    + (article.cum_noi_dung_chi_tiet or "")
                                )
                                article.cum_noi_dung = km.map_to_cluster(text)
                            processed_articles.extend(batch)

                    else:
                        logger.warning(
                            f"No valid response for batch {i // batch_size + 1}"
                        )
                        # Fallback t∆∞∆°ng t·ª± khi kh√¥ng c√≥ response
                        km = KeywordManager(
                            CONFIG_DIR / "content_cluster_keywords.json"
                        )
                        for article in batch:
                            text = (
                                article.tom_tat_noi_dung
                                + " "
                                + (article.cum_noi_dung_chi_tiet or "")
                            )
                            article.cum_noi_dung = km.map_to_cluster(text)
                        processed_articles.extend(batch)

                except Exception as e:
                    logger.error(
                        f"Error processing batch {i // batch_size + 1}: {e}",
                        exc_info=True,
                    )
                    processed_articles.extend(
                        batch
                    )  # Keep raw articles if processing fails

                # Free memory after each batch
                gc.collect()

            logger.info(
                f"Processing completed. Total processed articles: {len(processed_articles)}"
            )
            return processed_articles

        except Exception as e:
            logger.error(f"Article processing failed: {e}", exc_info=True)
            return raw_articles  # Return raw articles if the entire process fails
        finally:
            gc.collect()  # Final memory cleanup

    def close(self):
        del self.agent
        gc.collect()


class ReportAgent(LLMUserFallbackMixin):
    """Agent chuy√™n t·∫°o b√°o c√°o, s·ª≠ d·ª•ng prompt ti·∫øng Vi·ªát."""

    def __init__(self, model: Any):
        self.agent = Agent(
            name="ReportGenerator",
            role="Chuy√™n gia T·∫°o B√°o c√°o v√† Ph√¢n t√≠ch D·ªØ li·ªáu",
            model=model,
            instructions=[
                "B·∫°n l√† chuy√™n gia t·∫°o b√°o c√°o ph√¢n t√≠ch truy·ªÅn th√¥ng cho ng√†nh FMCG.",
                "Nhi·ªám v·ª•: T·∫°o m·ªôt b√°o c√°o ph√¢n t√≠ch ƒë·ªëi th·ªß c·∫°nh tranh t·ª´ d·ªØ li·ªáu c√°c b√†i b√°o ƒë√£ ƒë∆∞·ª£c cung c·∫•p.",
                "B·∫°n B·∫ÆT BU·ªòC ph·∫£i tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON (JSON object) duy nh·∫•t, h·ª£p l·ªá v√† tu√¢n th·ªß nghi√™m ng·∫∑t theo c·∫•u tr√∫c c·ªßa Pydantic model 'CompetitorReport'.",
                "N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ho·∫∑c c√≥ l·ªói, tr·∫£ v·ªÅ CompetitorReport r·ªóng h·ª£p l·ªá v·ªõi c√°c tr∆∞·ªùng l√† [] ho·∫∑c 0.",
            ],
            markdown=False,
        )

    def extract_json(self, text: str) -> str:
        text = text.strip()

        # ∆Øu ti√™n t√¨m ƒëo·∫°n ```json ... ```
        matches = re.findall(r"```json(.*?)```", text, re.DOTALL)
        if matches:
            for match in matches:
                match = match.strip()
                try:
                    obj = json.loads(match)
                    if isinstance(obj, dict):
                        return match
                except:
                    continue

        # N·∫øu kh√¥ng c√≥, t√¨m JSON object ngo√†i c√πng
        try:
            # Th·ª≠ parse to√†n b·ªô text lu√¥n n·∫øu l√† dict
            obj = json.loads(text)
            if isinstance(obj, dict):
                return text
        except:
            pass

        # Fallback t√¨m c√°c c·∫∑p { } l·ªõn nh·∫•t
        dict_candidates = re.findall(r"(\{.*\})", text, re.DOTALL)
        for candidate in dict_candidates:
            try:
                obj = json.loads(candidate.strip())
                if isinstance(obj, dict):
                    return candidate.strip()
            except:
                continue

        raise ValueError("Kh√¥ng t√¨m th·∫•y JSON dict h·ª£p l·ªá trong ph·∫£n h·ªìi.")

    async def generate_report(
        self, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        """
        Generates a competitor analysis report from a list of articles for a given date range.
        """

        if not articles:
            logger.info(
                "No articles provided for report generation. Returning basic report."
            )
            return self._create_basic_report([], date_range)

        try:
            logger.info(f"Generating full report for {len(articles)} articles...")

            report_prompt = f"""
            T·∫°o m·ªôt b√°o c√°o ph√¢n t√≠ch ƒë·ªëi th·ªß c·∫°nh tranh t·ª´ {len(articles)} b√†i b√°o sau ƒë√¢y cho kho·∫£ng th·ªùi gian: {date_range}.
            
            D·ªØ li·ªáu ƒë·∫ßu v√†o:
            - Input: M·ªôt danh s√°ch c√°c b√†i b√°o ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß, kh√¥ng c·∫ßn s·ª≠a ƒë·ªïi g√¨ th√™m: {json.dumps([a.model_dump(mode='json') for a in articles], ensure_ascii=False, indent=2)}

            Y√™u c·∫ßu nhi·ªám v·ª•:
            1. D√πng danh s√°ch articles tr√™n ƒë·ªÉ t·∫°o `overall_summary` v√† `industry_summaries`.
            2. T·∫°o 'overall_summary' (t√≥m t·∫Øt t·ªïng quan), bao g·ªìm: thoi_gian_trich_xuat, industries (nganh_hang, nhan_hang, cum_noi_dung, so_luong_bai, cac_dau_bao), cac_dau_bao v√† tong_so_bai.
            3. T·∫°o danh s√°ch 'industry_summaries' (t√≥m t·∫Øt theo ng√†nh), m·ªói ng√†nh l√† 1 m·ª•c (nganh_hang), bao g·ªìm: nhan_hang, cum_noi_dung (tr∆∞·ªùng cum_noi_dung s·∫Ω l√† bao g·ªìm h·∫øt t·∫•t c·∫£ c√°c c·ª•m n·ªôi dung c·ªßa t·∫•t c·∫£ c√°c b√†i trong c√πng 1 ng√†nh), cac_dau_bao, so_luong_bai.
            4. Quy t·∫Øc khi t·∫°o tr∆∞·ªùng `cum_noi_dung` trong `industry_summaries`:
                - `cum_noi_dung` ch·ªâ ƒë∆∞·ª£c ch·ªçn trong danh s√°ch sau (kh√¥ng th√™m m√¥ t·∫£ chi ti·∫øt):
                - "Ho·∫°t ƒë·ªông doanh nghi·ªáp v√† th√¥ng tin s·∫£n ph·∫©m"
                - "Ch∆∞∆°ng tr√¨nh CSR"
                - "Chi·∫øn d·ªãch Marketing"
                - "Ra m·∫Øt s·∫£n ph·∫©m"
                - "H·ª£p t√°c ƒë·ªëi t√°c"
                - "B√°o c√°o t√†i ch√≠nh"
                - "An to√†n th·ª±c ph·∫©m"
                - "Kh√°c"
            5. N·∫øu c·∫ßn m√¥ t·∫£ chi ti·∫øt, h√£y ghi v√†o `cum_noi_dung_chi_tiet`, kh√¥ng ƒë∆∞·ª£c ghi v√†o `cum_noi_dung`.
            6. ƒê·∫£m b·∫£o tr·∫£ v·ªÅ ƒë√∫ng 1 ƒë·ªëi t∆∞·ª£ng JSON duy nh·∫•t, kh√¥ng c√≥ markdown, kh√¥ng c√≥ gi·∫£i th√≠ch ngo√†i l·ªÅ.

            Tr·∫£ v·ªÅ ƒë√∫ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON duy nh·∫•t v·ªõi c·∫•u tr√∫c sau:
            {{
                overall_summary: { ... },
                industry_summaries: [ ... ],
                total_articles={len(articles)},
                date_range={date_range}
            }}
            
            Quy t·∫Øc b·∫Øt bu·ªôc:
            - B·∫Øt ƒë·∫ßu output b·∫±ng '{' v√† k·∫øt th√∫c b·∫±ng '}' duy nh·∫•t.
            """
            response = await self._arun_with_user_fallback(
                report_prompt,
                session_id=self.session_id,
                preferred_provider=getattr(self.config, "provider", None),
                preferred_model=getattr(self.config, "model", None),
            )
            logger.error(f"Raw LLM response: {response.content}")
            if response and response.content:
                try:
                    try:
                        # ∆Øu ti√™n parse b·∫±ng json.loads
                        summary_data = json.loads(response.content)
                    except json.JSONDecodeError:
                        try:
                            # N·∫øu LLM tr·∫£ v·ªÅ d·∫°ng {'key': 'value'}, d√πng ast.literal_eval
                            summary_data = ast.literal_eval(response.content)
                        except Exception:
                            # Fallback d√πng extract_json ƒë·ªÉ t√¨m ƒë√∫ng ƒëo·∫°n JSON
                            raw_json = self.extract_json(response.content)
                            summary_data = json.loads(raw_json)

                    # Check c√≥ ph·∫£i dict kh√¥ng
                    if not isinstance(summary_data, dict):
                        logger.error("LLM tr·∫£ v·ªÅ list ho·∫∑c sai schema. Fallback.")
                        return self._create_basic_report(articles, date_range)

                    # G·ªôp l·∫°i articles t·ª´ input, kh√¥ng cho LLM sinh
                    summary_data["articles"] = [
                        a.model_dump(mode="json") for a in articles
                    ]

                    # Truy·ªÅn v√†o CompetitorReport
                    return CompetitorReport(**summary_data)

                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(
                        f"Kh√¥ng th·ªÉ ph√¢n t√≠ch ph·∫£n h·ªìi t·ª´ ReportAgent d∆∞·ªõi d·∫°ng JSON: {e}. ƒêang t·∫°o b√°o c√°o c∆° b·∫£n."
                    )
                    return self._create_basic_report(articles, date_range)
            return self._create_basic_report(articles, date_range)
        except Exception as e:
            logger.error(f"T·∫°o b√°o c√°o th·∫•t b·∫°i: {e}", exc_info=True)
            return self._create_basic_report(articles, date_range)
        finally:
            gc.collect()

    def _create_basic_report(
        self, articles: List[Article], date_range: str
    ) -> CompetitorReport:
        industry_groups = {}
        for article in articles:
            industry = article.nganh_hang
            if industry not in industry_groups:
                industry_groups[industry] = []
            industry_groups[industry].append(article)

        industry_summaries = [
            IndustrySummary(
                nganh_hang=industry,
                nhan_hang=list(
                    set(
                        brand
                        for article in industry_articles
                        for brand in article.nhan_hang
                    )
                ),
                cum_noi_dung=list(
                    set(
                        c
                        for article in industry_articles
                        if (c := article.cum_noi_dung) is not None
                    )
                )
                or [ContentCluster.OTHER.value],
                so_luong_bai=len(industry_articles),
                cac_dau_bao=list(set(article.dau_bao for article in industry_articles)),
            )
            for industry, industry_articles in industry_groups.items()
        ]

        overall_summary = OverallSummary(
            thoi_gian_trich_xuat=date_range,
            industries=industry_summaries,
            tong_so_bai=len(articles),
        )

        return CompetitorReport(
            overall_summary=overall_summary,
            industry_summaries=industry_summaries,
            articles=articles,
            total_articles=len(articles),
            date_range=date_range,
        )

    def close(self):
        del self.agent
        gc.collect()


class MediaTrackerTeam:
    """ƒê·ªôi ƒëi·ªÅu ph·ªëi ch√≠nh cho to√†n b·ªô quy tr√¨nh."""

    def __init__(
        self,
        config: CrawlConfig,
        start_date: datetime,
        end_date: datetime,
        user_email: Optional[str] = None,
        session_id: Optional[str] = None,
        on_progress_update: Optional[callable] = None,
        check_cancelled: Optional[callable] = None,
        check_paused: Optional[callable] = None,
        articles_so_far=None,
        source_status_list=None,
        provider: Optional[str] = None,
        model: Optional[str] = None,
    ):
        model_runtime = get_llm_model(provider, model)
        self.config = config

        def _source_key_of(s):
            return getattr(s, "reference_name", None) or f"{s.type}|{s.domain}"

        sel = set(getattr(self.config, "selected_sources", []) or [])
        if sel:
            self.config.media_sources = [
                s for s in self.config.media_sources if _source_key_of(s) in sel
            ]

        self.session_id = session_id
        self.user_email = user_email
        self.status = BotStatus()
        self.check_cancelled = check_cancelled or (lambda: False)
        self.check_paused = check_paused or (lambda: False)
        self.articles_so_far = articles_so_far or []
        self.on_progress_update = on_progress_update
        parser = ArticleParser()
        self.crawler = CrawlerAgent(
            model_runtime,
            config,
            parser,
            session_id,
            check_cancelled,
            check_paused,
            check_pause_or_cancel=self.check_pause_or_cancel,
            user_email=user_email,
            status=self.status,
            on_progress_update=self.on_progress_update,
        )
        # self.processor = ProcessorAgent(get_llm_model("openai", "gpt-4o"))
        # self.reporter = ReportAgent(get_llm_model("openai", "gpt-4o"))
        self.processor = ProcessorAgent(model_runtime)
        self.reporter = ReportAgent(model_runtime)
        self.start_date = start_date
        self.end_date = end_date
        self.source_status_list = source_status_list or []
        self.config.use_hub_page = True

    async def check_pause_or_cancel(self):
        if self.check_cancelled():
            logger.info("‚õî Cancelled. Stop pipeline.")
            raise asyncio.CancelledError("Pipeline is cancelled.")

        while self.check_paused():
            logger.info("‚è∏Ô∏è  Pipeline paused. Waiting to resume...")
            await asyncio.sleep(2)

            if self.check_cancelled():
                logger.info(f"[{self.session_id}] Task was cancelled during pause.")
                raise asyncio.CancelledError("Cancelled during pause")

    async def run_full_pipeline(self) -> Optional[CompetitorReport]:
        """
        Executes the full media tracking pipeline: crawling, processing, and report generation.
        Optimizes memory usage by limiting concurrent tasks and processing articles in batches.
        """
        self.status.is_running = True
        self.status.current_task = "Initializing pipeline"
        self.status.total_sources = len(self.config.media_sources)
        self.status.progress = 0.0
        # all_articles = self.articles_so_far.copy()

        logger.info(
            f"üìÖ Kho·∫£ng th·ªùi gian crawl d·ªØ li·ªáu: t·ª´ ng√†y {self.start_date.strftime('%d/%m/%Y')} ƒë·∫øn ng√†y {self.end_date.strftime('%d/%m/%Y')}"
        )

        try:
            # Step 1: Crawl data from media sources
            self.status.current_task = "Crawling data from media sources"
            logger.info(f"Starting crawl for {self.status.total_sources} sources.")

            # Limit concurrent crawl tasks using Semaphore
            self.semaphore = asyncio.Semaphore(self.config.max_concurrent_sources)

            async def wrapped_crawl(media_source, industry_name, keywords):
                # N·∫øu ƒë√£ completed r·ªìi th√¨ skip
                if any(
                    s["source_name"] == media_source.name and s["status"] == "completed"
                    for s in self.source_status_list
                ):
                    logger.info(
                        f"[{media_source.name}] ‚úÖ ƒê√£ ho√†n th√†nh t·ª´ tr∆∞·ªõc, b·ªè qua."
                    )
                    return media_source, None

                async with self.semaphore:
                    try:
                        await _maybe_await(self.check_pause_or_cancel)
                        result = await self.crawler.crawl_media_source(
                            media_source=media_source,
                            industry_name=industry_name,
                            keywords=keywords,
                            start_date=self.start_date,
                            end_date=self.end_date,
                        )

                        if result and result.articles_found:
                            self.articles_so_far.extend(result.articles_found)

                        task_manager.update_task(
                            self.user_email,
                            self.session_id,
                            {
                                "articles_so_far": [
                                    a.model_dump(mode="json")
                                    for a in self.articles_so_far
                                ],
                                "source_status_list": self.source_status_list,
                            },
                        )

                        return media_source, result

                    except asyncio.TimeoutError:
                        logger.warning(
                            f"[{media_source.name}] ‚è∞ Timeout sau {self.config.crawl_timeout} gi√¢y."
                        )
                        partial_result = self.crawler.return_partial_result(
                            media_source
                        )

                        if partial_result.articles_found:
                            self.articles_so_far.extend(partial_result.articles_found)

                        self.source_status_list.append(
                            {"source_name": media_source.name, "status": "failed"}
                        )

                        task_manager.update_task(
                            self.user_email,
                            self.session_id,
                            {
                                "articles_so_far": [
                                    a.model_dump(mode="json")
                                    for a in self.articles_so_far
                                ],
                                "source_status_list": self.source_status_list,
                            },
                        )

                        return media_source, partial_result

            # Prepare all keywords
            # all_keywords = list(
            #     set(kw for kws in self.config.keywords.values() for kw in kws)
            # )
            tasks = []
            for industry_name, keywords in self.config.keywords.items():
                for media_source in self.config.media_sources:
                    already_done = next(
                        (
                            s
                            for s in self.source_status_list
                            if s["source_name"] == media_source.name
                        ),
                        None,
                    )
                    if already_done and already_done["status"] == "completed":
                        continue

                    task = asyncio.create_task(
                        wrapped_crawl(media_source, industry_name, keywords)
                    )
                    tasks.append((media_source, task))

            # Execute crawl tasks
            tasks_only = [t[1] for t in tasks]
            results = await asyncio.gather(*tasks_only, return_exceptions=True)

            for (media_source, _), result in zip(tasks, results):
                await _maybe_await(self.check_pause_or_cancel)

                try:
                    if isinstance(result, Exception):
                        logger.error(
                            f"[{media_source.name}] ‚ùå L·ªói: {result}", exc_info=True
                        )
                        self.source_status_list.append(
                            {"source_name": media_source.name, "status": "failed"}
                        )
                        continue

                    media_source, crawl_result = result
                    self.status.completed_sources += 1

                    crawl_status = (
                        getattr(crawl_result, "crawl_status", "failed")
                        if crawl_result
                        else "failed"
                    )
                    status = "completed" if crawl_status == "success" else "failed"

                    self.source_status_list.append(
                        {"source_name": media_source.name, "status": status}
                    )

                    completed = len(
                        [
                            s
                            for s in self.source_status_list
                            if s["status"] == "completed"
                        ]
                    )
                    failed = len(
                        [s for s in self.source_status_list if s["status"] == "failed"]
                    )

                    if self.on_progress_update:
                        self.on_progress_update(
                            source_name=media_source.name,
                            completed=completed,
                            failed=failed,
                            progress=((completed + failed) / self.status.total_sources)
                            * 50.0,
                            current_task=f"Crawling {media_source.name}",
                        )

                    gc.collect()

                except asyncio.CancelledError:
                    logger.warning(f"Crawl task cancelled.")
                    for _, t in tasks:
                        t.cancel()
                    self.status.failed_sources += 1
                    raise
                except Exception as e:
                    self.status.failed_sources += 1
                    logger.error(
                        f"Unexpected error crawling source {media_source.name}: {e}",
                        exc_info=True,
                    )

            if not self.articles_so_far:
                logger.warning("‚ö†Ô∏è No articles found from crawling or cache.")
                return None

            all_articles = self.articles_so_far.copy()
            logger.info(f"Crawling completed. Found {len(all_articles)} raw articles.")

            # Step 2: Process articles in batches
            self.status.current_task = "Processing and analyzing articles"
            self.status.progress = 60.0
            batch_size = 10  # Process 20 articles per batch
            processed_articles = []
            for i in range(0, len(all_articles), batch_size):
                await _maybe_await(self.check_pause_or_cancel)

                batch = all_articles[i : i + batch_size]
                batch_processed = await self.processor.process_articles(
                    batch, self.config.keywords
                )
                processed_articles.extend(batch_processed)
                logger.info(
                    f"Processed batch {i // batch_size + 1}: {len(batch_processed)} articles"
                )
                gc.collect()  # Free memory after each batch

            logger.info(
                f"Processing completed. Retained {len(processed_articles)} articles."
            )
            self.status.progress = 80.0

            # Step 3: Generate report in batches
            self.status.current_task = "Generating final report"
            date_range_str = f"T·ª´ ng√†y {self.start_date.strftime('%d/%m/%Y')} ƒë·∫øn ng√†y {self.end_date.strftime('%d/%m/%Y')}"

            report = None
            if not processed_articles:
                logger.warning("No valid articles to generate report.")
                return None

            await _maybe_await(self.check_pause_or_cancel)

            report = await self.reporter.generate_report(
                processed_articles, date_range_str
            )

            if report is None:
                logger.warning("No valid articles to generate report.")
                return None

            # Update overall summary
            report.overall_summary.tong_so_bai = len(report.articles)
            report.total_articles = len(report.articles)
            report.date_range = date_range_str

            self.status.progress = 100.0
            self.status.current_task = "Pipeline completed"
            logger.info("Pipeline completed successfully.")
            await asyncio.sleep(0.5)
            return report

        except (InterruptedError, asyncio.CancelledError) as e:
            self.status.current_task = f"Stopped: {str(e)}"
            for _, task in tasks:
                task.cancel()
            results = await asyncio.gather(
                *[t[1] for t in tasks], return_exceptions=True
            )
            for (media_source, _), result in zip(tasks, results):
                if isinstance(result, Exception):
                    logger.error(f"[{media_source.name}] ‚ùå L·ªói: {result}")
                    self.source_status_list.append(
                        {"source_name": media_source.name, "status": "failed"}
                    )
                else:
                    logger.info(f"[{media_source.name}] ‚úÖ Crawl th√†nh c√¥ng")
                    self.source_status_list.append(
                        {"source_name": media_source.name, "status": "completed"}
                    )
            self.status.current_task = "ƒê√£ h·ªßy"
            raise
        except Exception as e:
            self.status.current_task = f"Failed: {str(e)}"
            logger.error(f"Pipeline failed: {e}", exc_info=True)
            return None
        finally:
            self.status.is_running = False
            self.status.last_run = datetime.now()
            if self.status.progress < 100.0:
                self.status.progress = 100.0
            if "completed" not in self.status.current_task.lower():
                self.status.current_task = "Ready"
            self.cleanup()
            gc.collect()  # Final memory cleanup

    def get_status(self) -> BotStatus:
        return self.status

    def cleanup(self):
        logger.info("üîß ƒêang gi·∫£i ph√≥ng t√†i nguy√™n pipeline...")
        self.crawler.close_final()
        self.processor.close()
        self.reporter.close()
        gc.collect()
