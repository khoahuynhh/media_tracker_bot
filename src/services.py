# src/services.py
"""
Service Layer for the Media Tracker Bot.
This layer contains the core business logic and orchestrates the pipeline.
It decouples the API (main.py) from the agents.
"""

import asyncio
import shutil
import logging
import re
import pandas as pd
import traceback
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, DefaultDict
from collections import defaultdict
from jose import jwt, JWTError
from fastapi.security import OAuth2PasswordBearer
from fastapi import HTTPException, Depends
from openpyxl.utils import get_column_letter
from openpyxl.styles import Alignment


from .models import CompetitorReport, BotStatus, MediaType, Article
from .agents import MediaTrackerTeam
from .configs import AppSettings
from .task_state import task_manager

logger = logging.getLogger(__name__)

SECRET_KEY = "your_secret_key"
ALGORITHM = "HS256"
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/login")


def create_access_token(data: dict, expires_delta: timedelta = timedelta(hours=12)):
    to_encode = data.copy()
    expire = datetime.now(timezone.utc) + expires_delta
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)


def get_current_user(token: str = Depends(oauth2_scheme)) -> str:
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload.get("sub")
    except JWTError:
        raise HTTPException(status_code=401, detail="Token is invalid or expired")


class PipelineService:
    """
    Core pipeline logic for media tracking. This function handles crawling and report generation.
    It is intended to be called by background task workers.
    """

    def __init__(self, app_settings: AppSettings, user_email: str):
        self.settings = app_settings
        self.team: Optional[MediaTrackerTeam] = None
        self.status_by_source = {}
        self.user_email = user_email

    async def run_pipeline_logic(
        self,
        session_id: str,
        user_email: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        custom_keywords: Optional[Dict[str, List[str]]] = None,
    ) -> Optional[CompetitorReport]:
        logger.info(f"[{session_id}] Starting media tracking pipeline...")
        self.current_session_id = session_id

        session_config = self.settings.crawl_config.model_copy(deep=True)
        session_config.media_sources = self.settings.crawl_config.media_sources.copy()

        if custom_keywords:
            session_config.keywords = custom_keywords

        if start_date and end_date:
            try:
                start_dt = datetime.strptime(start_date, "%Y-%m-%d")
                end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                session_config.date_range_days = (end_dt - start_dt).days
            except ValueError:
                logger.error(
                    f"[{session_id}] Invalid date format. Using default date range."
                )
                start_dt = None
                end_dt = None
        else:
            start_dt = None
            end_dt = None

        # L·∫•y l·∫°i task hi·ªán t·∫°i
        tasks = task_manager.get_tasks(user_email)
        task = next((t for t in tasks if t["session_id"] == session_id), None)
        articles_so_far = [Article(**a) for a in task.get("articles_so_far", [])]

        self.team = MediaTrackerTeam(
            config=session_config,
            start_date=start_dt,
            end_date=end_dt,
            session_id=session_id,
            user_email=user_email,
            on_progress_update=self._update_task_progress,
            check_cancelled=self.is_cancelled,
            check_paused=self.should_pause,
            articles_so_far=articles_so_far,
            source_status_list=task.get("source_status_list", []),
        )

        report = await asyncio.wait_for(
            self.team.run_full_pipeline(),
            timeout=self.settings.crawl_config.total_pipeline_timeout,
        )

        if report:
            await self._save_report(report, user_email)
            logger.info(f"[{session_id}] Pipeline completed successfully.")
        else:
            logger.warning(
                f"[{session_id}] Pipeline finished without generating a report."
            )
        return report

    def is_cancelled(self):
        tasks = task_manager.get_tasks(self.user_email)
        task = next(
            (t for t in tasks if t["session_id"] == self.current_session_id), None
        )
        return task and task["status"] == "cancelled"

    def should_pause(self):
        tasks = task_manager.get_tasks(self.user_email)
        task = next(
            (t for t in tasks if t["session_id"] == self.current_session_id), None
        )
        return task and task["status"] == "paused"

    def resume_task_worker(self, session_id):
        logger.info(f"[{session_id}] Resuming pipeline execution (resume_task_worker)")
        asyncio.create_task(self._task_worker(session_id))

    def _update_task_progress(
        self, source_name, completed, failed, progress, current_task
    ):
        all_sources = [s.name for s in self.settings.crawl_config.media_sources]

        current_tasks = task_manager.get_tasks(self.user_email)
        task = next(
            (t for t in current_tasks if t["session_id"] == self.current_session_id),
            None,
        )

        if task.get("status") not in ["pending", "running"]:
            return

        if task:
            # L·∫•y danh s√°ch source ƒë√£ completed t·ª´ team

            completed_or_failed = [
                s["source_name"]
                for s in self.team.source_status_list
                if s["status"] in ("completed", "failed")
            ]

            remaining = [s for s in all_sources if s not in completed_or_failed]

            task_manager.update_task(
                self.user_email,
                self.current_session_id,
                {
                    "current_source": source_name,
                    "completed_sources": completed,
                    "failed_sources": failed,
                    "progress": progress,
                    "current_task": current_task,
                    "status": "running",
                    "remaining_sources": remaining,
                    "source_status_list": self.team.source_status_list,  # L∆∞u lu√¥n v√†o task
                },
            )

    async def retry_task_worker(app_settings, user_email, session_id):
        new_service = PipelineService(app_settings, user_email)
        await new_service._task_worker(session_id)

    async def _task_worker(self, session_id):
        tasks = task_manager.get_tasks(self.user_email)
        task = next((t for t in tasks if t["session_id"] == session_id), None)

        if not task:
            logger.info(f"[{session_id}] Task not found, stopping worker.")
            return

        status = task["status"]

        if status == "paused":
            logger.info(f"[{session_id}] Task is paused. Waiting to resume...")
            # ƒê·ª£i cho ƒë·∫øn khi kh√¥ng c√≤n paused
            while True:
                current_status = task_manager.get_task_status(
                    self.user_email, session_id
                )
                if current_status == "paused":
                    await asyncio.sleep(2)
                elif current_status == "cancelled":
                    logger.info(
                        f"[{session_id}] Task was cancelled while paused. Exiting."
                    )
                    return  # üëâ D·ª´ng ngay, kh√¥ng ti·∫øp t·ª•c
                else:
                    break  # Resume h·ª£p l·ªá
            logger.info(
                f"[{session_id}] Resumed. Checking if cancelled before proceeding..."
            )

        # Ki·ªÉm tra l·∫°i tr·∫°ng th√°i tr∆∞·ªõc khi ti·∫øp t·ª•c
        current_status = task_manager.get_task_status(self.user_email, session_id)
        if current_status == "cancelled":
            logger.info(f"[{session_id}] Task is cancelled. Exiting worker.")
            return

        if current_status == "pending":
            logger.info(f"[{session_id}] Task is pending. Preparing pipeline setup...")
            task_manager.update_task(
                self.user_email,
                session_id,
                {
                    "status": "pending",
                    "current_task": "Preparing tools and resources...",
                    "progress": 0.0,
                },
            )
            await asyncio.sleep(5)  # Cho FE th·∫•y tr·∫°ng th√°i pending

            # Ki·ªÉm tra l·∫°i tr·∫°ng th√°i tr∆∞·ªõc khi chuy·ªÉn sang running
            current_status = task_manager.get_task_status(self.user_email, session_id)
            if current_status == "cancelled":
                logger.info(
                    f"[{session_id}] Task was cancelled during pending. Exiting."
                )
                return

            task_manager.update_task(
                self.user_email,
                session_id,
                {
                    "status": "running",
                    "current_task": "Crawling... please wait.",
                    "progress": 0.0,
                },
            )

        # B·∫Øt ƒë·∫ßu pipeline th·ª±c s·ª±
        try:
            logger.info(f"[{session_id}] Starting pipeline execution.")
            params = task["params"]

            await self.run_pipeline_logic(
                session_id,
                self.user_email,
                start_date=params["start_date"],
                end_date=params["end_date"],
                custom_keywords=params["custom_keywords"],
            )
            task_manager.update_task(
                self.user_email, session_id, {"status": "completed", "progress": 100.0}
            )

        except asyncio.CancelledError:
            logger.warning(f"[{session_id}] ‚ö†Ô∏è Task forcefully cancelled.")
            task_manager.update_task(
                self.user_email,
                session_id,
                {
                    "status": "cancelled",
                    "current_task": "Pipeline was cancelled",
                },
            )

        except Exception as e:
            logger.error(
                f"[{session_id}] Pipeline failed: {e}\n{traceback.format_exc()}"
            )
            task["retry_count"] += 1
            if task["retry_count"] < task["max_retries"]:
                logger.info(
                    f"[{session_id}] Retry {task['retry_count']} / {task['max_retries']}"
                )
                task_manager.update_task(
                    self.user_email,
                    session_id,
                    {
                        "status": "pending",
                        "current_source": None,
                        "progress": 0.0,
                        "current_task": "Chu·∫©n b·ªã retry sau l·ªói",
                    },
                )
                await asyncio.sleep(5)
                asyncio.create_task(
                    self._task_worker(session_id)
                )  # T·ª± g·ªçi l·∫°i ƒë·ªÉ retry
            else:
                task_manager.update_task(
                    self.user_email, session_id, {"status": "failed", "error": str(e)}
                )

    def run_background_task(
        self, session_id, start_date=None, end_date=None, custom_keywords=None
    ):
        task_data = {
            "session_id": session_id,
            "status": "pending",
            "start_time": datetime.now().isoformat(),
            "progress": 0.0,
            "retry_count": 0,
            "max_retries": 2,
            # Th√¥ng tin c·∫ßn cho dashboard
            "params": {
                "start_date": start_date,
                "end_date": end_date,
                "custom_keywords": custom_keywords,
            },
            "current_source": None,
            "total_sources": len(self.settings.crawl_config.media_sources),
            "completed_sources": 0,
            "failed_sources": 0,
            "current_task": "Kh·ªüi ƒë·ªông pipeline",
            "source_status_list": [],
        }

        task_manager.add_task(self.user_email, task_data)

        # Kh·ªüi ch·∫°y task async
        asyncio.create_task(self._task_worker(session_id))

        return session_id

    def get_status(self) -> Dict:
        """Gets the current status of the service and the running team."""
        return {"tasks": task_manager.get_tasks(self.user_email)}

    def _sanitize_user_name(self, username: str) -> str:
        """Tr·∫£ v·ªÅ t√™n th∆∞ m·ª•c an to√†n t·ª´ t√™n user."""
        username = username.strip().lower()
        sanitized = re.sub(r"[^a-zA-Z0-9]+", "_", username)
        return sanitized.strip("_") or "unknown_user"

    async def _save_report(self, report: CompetitorReport, user_email: str):
        """Saves the report to JSON and Excel files."""

        folder_name = self._sanitize_user_name(user_email)
        reports_dir = self.settings.reports_dir / folder_name
        reports_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        try:
            json_file = reports_dir / f"report_{timestamp}.json"
            with open(json_file, "w", encoding="utf-8") as f:
                f.write(report.model_dump_json(indent=2))

            excel_file = reports_dir / f"report_{timestamp}.xlsx"
            # articles_df = pd.DataFrame([a.model_dump() for a in report.articles])
            # summary_df = pd.DataFrame(
            #     [s.model_dump() for s in report.industry_summaries]
            # )

            # Format Summary - Overall
            overall_df = pd.DataFrame(
                [
                    {
                        "Ng√†nh h√†ng": s.nganh_hang,
                        "Nh√£n h√†ng": ", ".join(s.nhan_hang),
                        "C√°c ƒë·∫ßu b√°o": ", ".join(s.cac_dau_bao),
                        "S·ªë l∆∞·ª£ng b√†i": s.so_luong_bai,
                    }
                    for s in report.overall_summary.industries
                ]
            )

            with pd.ExcelWriter(excel_file, engine="openpyxl") as writer:
                # 1. Summary t·ªïng quan
                overall_df.to_excel(writer, sheet_name="Summary - Overall", index=False)

                # 2. Summary theo ng√†nh (1 sheet m·ªói ng√†nh)
                for s in report.industry_summaries:
                    sheet_name = (
                        f"Summary - Ng√†nh {s.nganh_hang[:25]}"  # Gi·ªõi h·∫°n 31 k√Ω t·ª±
                    )

                    rows = []

                    for brand in s.nhan_hang:
                        # L·ªçc c√°c b√†i b√°o c√≥ ch·ª©a nh√£n h√†ng n√†y
                        related_articles = [
                            article
                            for article in report.articles
                            if brand in article.nhan_hang
                            and article.nganh_hang == s.nganh_hang
                        ]

                        # L·∫•y c√°c c·ª•m n·ªôi dung xu·∫•t hi·ªán trong c√°c b√†i b√°o ƒë√≥ (kh√¥ng tr√πng)
                        clusters = set(
                            article.cum_noi_dung
                            for article in related_articles
                            if article.cum_noi_dung
                        )

                        # G·ªôp c·ª•m n·ªôi dung th√†nh chu·ªói c√≥ ƒë√°nh s·ªë
                        if clusters:
                            cluster_text = "\n".join(
                                [f"{idx+1}. {c}" for idx, c in enumerate(clusters)]
                            )
                        else:
                            cluster_text = ""

                        # S·ªë l∆∞·ª£ng b√†i l√† t·ªïng s·ªë b√†i c√≥ brand n√†y (kh√¥ng ph√¢n c·ª•m)
                        count = len(related_articles)

                        rows.append(
                            {
                                "Ng√†nh h√†ng": s.nganh_hang,
                                "Nh√£n h√†ng": brand,
                                "C·ª•m n·ªôi dung": cluster_text,
                                "S·ªë l∆∞·ª£ng b√†i": count,
                            }
                        )

                    # B·ªï sung d√≤ng cho c√°c b√†i kh√¥ng c√≥ nh√£n h√†ng
                    no_brand_articles = [
                        article
                        for article in report.articles
                        if not article.nhan_hang and article.nganh_hang == s.nganh_hang
                    ]

                    if no_brand_articles:
                        # L·∫•y c√°c c·ª•m n·ªôi dung c·ªßa b√†i kh√¥ng c√≥ nh√£n
                        clusters_no_brand = set(
                            article.cum_noi_dung
                            for article in no_brand_articles
                            if article.cum_noi_dung
                        )

                        if clusters_no_brand:
                            cluster_text = "\n".join(
                                [
                                    f"{idx+1}. {c}"
                                    for idx, c in enumerate(clusters_no_brand)
                                ]
                            )
                        else:
                            cluster_text = ""

                        count = len(no_brand_articles)

                        rows.append(
                            {
                                "Ng√†nh h√†ng": s.nganh_hang,
                                "Nh√£n h√†ng": "Kh√¥ng nh√£n h√†ng",
                                "C·ª•m n·ªôi dung": cluster_text,
                                "S·ªë l∆∞·ª£ng b√†i": count,
                            }
                        )

                    # T·∫°o DataFrame v√† ghi v√†o Excel
                    df = pd.DataFrame(rows)
                    df.to_excel(writer, index=False, sheet_name=sheet_name)

                    worksheet = writer.sheets[sheet_name]

                    for idx, col_name in enumerate(df.columns):
                        col_letter = get_column_letter(idx + 1)
                        for row in range(2, len(df) + 2):  # B·ªè header
                            cell = worksheet[f"{col_letter}{row}"]
                            if col_name == "C·ª•m n·ªôi dung":
                                cell.alignment = Alignment(
                                    wrap_text=True, vertical="top"
                                )
                            else:
                                # C√°c c·ªôt kh√°c: ch·ªâ middle align
                                cell.alignment = Alignment(vertical="center")

                # 3. M·ªói nh√£n h√†ng m·ªôt sheet b√†i b√°o

                # Gom b√†i theo nh√£n h√†ng
                articles_by_brand: DefaultDict[str, List[Article]] = defaultdict(list)
                articles_no_brand: List[Article] = []
                for article in report.articles:
                    if article.nhan_hang:
                        for brand in article.nhan_hang:
                            articles_by_brand[brand].append(article)
                    else:
                        articles_no_brand.append(article)

                # Ghi m·ªói nh√£n h√†ng ra 1 sheet
                for brand, articles in articles_by_brand.items():
                    first_article = articles[0]
                    sheet_name = f"Ng√†nh {first_article.nganh_hang[:31]} - {brand[:25]}"  # Gi·ªõi h·∫°n t√™n sheet
                    df = pd.DataFrame(
                        [
                            {
                                "STT": a.stt,
                                "Ng√†y ph√°t h√†nh": a.ngay_phat_hanh.strftime("%d/%m/%Y"),
                                "ƒê·∫ßu b√°o": a.dau_bao,
                                "C·ª•m n·ªôi dung": a.cum_noi_dung_chi_tiet
                                or a.cum_noi_dung,
                                "T√≥m t·∫Øt n·ªôi dung": a.tom_tat_noi_dung,
                                "Link b√†i b√°o": a.link_bai_bao,
                                # "Ng√†nh h√†ng": a.nganh_hang,
                                # "Nh√£n h√†ng": ", ".join(a.nhan_hang),
                                "Keywords": ", ".join(a.keywords_found),
                            }
                            for a in articles
                        ]
                    )
                    # Sequence STT
                    df["STT"] = range(1, len(df) + 1)
                    # N·∫øu mu·ªën n√≥ ·ªü ƒë·∫ßu ti√™n:
                    df = df[["STT"] + [col for col in df.columns if col != "STT"]]
                    df.to_excel(writer, sheet_name=sheet_name, index=False)

                # 4. Ghi sheet cho c√°c b√†i kh√¥ng c√≥ nh√£n h√†ng
                if articles_no_brand:
                    first_article = articles_no_brand[0]
                    industry = first_article.nganh_hang
                    sheet_name = f"General - {industry}"
                    sheet_name = sheet_name[:31]  # ƒê·∫£m b·∫£o kh√¥ng qu√° 31 k√Ω t·ª±

                    df = pd.DataFrame(
                        [
                            {
                                "STT": a.stt,
                                "Ng√†y ph√°t h√†nh": a.ngay_phat_hanh.strftime("%d/%m/%Y"),
                                "ƒê·∫ßu b√°o": a.dau_bao,
                                "C·ª•m n·ªôi dung": a.cum_noi_dung_chi_tiet
                                or a.cum_noi_dung,
                                "T√≥m t·∫Øt n·ªôi dung": a.tom_tat_noi_dung,
                                "Link b√†i b√°o": a.link_bai_bao,
                            }
                            for a in articles_no_brand
                        ]
                    )
                    df["STT"] = range(1, len(df) + 1)
                    df = df[["STT"] + [col for col in df.columns if col != "STT"]]
                    df.to_excel(writer, sheet_name=sheet_name, index=False)

            for ext in ["json", "xlsx"]:
                latest_path = reports_dir / f"latest_report.{ext}"
                target_file = reports_dir / f"report_{timestamp}.{ext}"
                if latest_path.exists() or latest_path.is_symlink():
                    latest_path.unlink()
                shutil.copy(target_file, latest_path)

            logger.info(f"Report saved to {json_file} and {excel_file}")
        except Exception as e:
            logger.error(f"Failed to save report: {e}", exc_info=True)
